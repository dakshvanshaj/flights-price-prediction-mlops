# params.yaml

# This list is used by DVC to loop through data splits.
data_splits:
  - train
  - validation
  - test

# =================================================================
# --- MODELING STRATEGY ---
# =================================================================
# Set to true if using a tree-based model (like LGBM or XGB) to skip
# unnecessary preprocessing steps (scaling, power transforms, etc.)
is_tree_model: true

# =================================================================
# --- HYPERPARAMETERS FOR GOLD PIPELINE STAGES ---
# =================================================================
gold_pipeline:
  imputation:
    median: ["price", "time", "distance"]
    mode: ["agency", "flight_type"]
    constant:
      from_location: "Unknown"
      to_location: "Unknown"
  rare_category_grouping:
  #   cardinality_threshold: 0.01
  outlier_handling:
    detection_strategy: "iqr"
    handling_strategy: "trim"
  power_transformer:
  # strategy: "yeo-johnson"
  scaler:
    strategy: ""

# =================================================================
# --- PARAMETERS FOR MLFLOW ---
# =================================================================
mlflow_params:
  experiment_name: "Challengers"

# =================================================================
# --- PARAMETERS FOR MODEL TRAINING ---
# =================================================================
training_pipeline:
  # -----------------------------------------------------------------
  # >> The ONLY line you need to change to run a different model <<
  # This key points to one of the configurations in the 'models' dictionary below.
  model_config_to_run: "LGBMRegressor"

  # -----------------------------------------------------------------

  # --- Dictionary of all available model configurations ---
  models:
    # =================================================================
    # 1. Linear Models
    # =================================================================
    LinearRegression:
      model_class: "LinearRegression"
      name: "LinearRegression_Base"
      run_name: "LR_Base"
      log_model_artifact: false
      register_model: false
      log_predictions: false
      log_interpretability_artifacts: true
      log_plots: true
      evaluate_on_test_set: false
      drop_multicollinear_cols: ["time"]
      training_params:
        n_jobs: -1
        fit_intercept: true
      train_model_only: false
      cross_validation:
        enabled: true
        n_splits: 5

    Ridge:
      model_class: "Ridge"
      name: "Ridge_Tuned"
      run_name: "Ridge_Base"
      log_model_artifact: true
      register_model: false
      log_predictions: true
      log_interpretability_artifacts: true
      log_plots: true
      evaluate_on_test_set: false
      drop_multicollinear_cols: ["time"]
      training_params:
        alpha: 1.0
        fit_intercept: true
        solver: "auto"
        random_state: 42
      train_model_only: false
      cross_validation:
        enabled: true
        n_splits: 5

    Lasso:
      model_class: "Lasso"
      name: "Lasso_Tuned"
      run_name: "Lasso_Base"
      log_model_artifact: true
      register_model: false
      log_predictions: true
      log_interpretability_artifacts: true
      log_plots: true
      evaluate_on_test_set: false
      drop_multicollinear_cols: ["time"]
      training_params:
        alpha: 1.0
        fit_intercept: true
        selection: "cyclic"
        random_state: 42
      train_model_only: false
      cross_validation:
        enabled: true
        n_splits: 5

    ElasticNet:
      model_class: "ElasticNet"
      name: "ElasticNet_Tuned"
      run_name: "ElasticNet_Base"
      log_model_artifact: true
      register_model: false
      log_predictions: true
      log_interpretability_artifacts: true
      log_plots: true
      evaluate_on_test_set: false
      drop_multicollinear_cols: ["time"]
      training_params:
        alpha: 1.0
        l1_ratio: 0.5
        fit_intercept: true
        selection: "cyclic"
        random_state: 42
      train_model_only: false
      cross_validation:
        enabled: true
        n_splits: 5

    # =================================================================
    # 2. Tree-Based Ensemble Models
    # =================================================================
    RandomForestRegressor:
      model_class: "RandomForestRegressor"
      name: "RFR_Best"
      run_name: "RFR_Test"
      log_model_artifact: false
      register_model: false
      log_predictions: true
      log_interpretability_artifacts: true
      log_plots: true
      evaluate_on_test_set: true
      drop_multicollinear_cols: []
      training_params:
        n_estimators: 600
        max_depth: 14
        min_samples_split: 16
        min_samples_leaf: 4
        max_features: 0.5062662102018453
        random_state: 42
        n_jobs: -1
      train_model_only: true
      cross_validation:
        enabled: false
        n_splits: 5

    XGBRegressor:
      model_class: "XGBRegressor"
      name: "XGBR_Best"
      run_name: "XGBR_Challenger"
      log_model_artifact: true
      register_model: false
      log_predictions: false
      log_interpretability_artifacts: true
      log_plots: true
      log_shap_plots: true
      evaluate_on_test_set: false
      drop_multicollinear_cols: []
      training_params:
        n_estimators: 1100
        learning_rate: 0.064
        max_depth: 12
        subsample: 0.8484233884504717
        colsample_bytree: 0.6535905760097056
        gamma: 0.5111157004979503
        reg_alpha: 8.325147849527994e-06
        reg_lambda: 1.035392577097126e-08
        random_state: 42
        n_jobs: -1
      train_model_only: true
      cross_validation:
        enabled: false
        n_splits: 5

    LGBMRegressor:
      model_class: "LGBMRegressor"
      name: "LGBMR"
      run_name: "LGBMR_Champion"
      log_model_artifact: true
      register_model: true
      log_predictions: false
      log_interpretability_artifacts: true
      log_plots: true
      drop_multicollinear_cols: []
      training_params:
        n_estimators: 700
        learning_rate: 0.16627974464599812
        num_leaves: 15
        max_depth: 5
        subsample: 0.8337243366457066
        colsample_bytree: 0.6144913771873575
        reg_alpha: 4.999937468073592e-05
        reg_lambda: 0.33053572287139826
        random_state: 42
        n_jobs: -1
        verbose: -1
      cross_validation:
        enabled: false
        n_splits: 5
      train_model_only: true # ! for final model training wont log or calculate any metrics or cross validation (trainset + valset + best hyperparams)
      evaluate_on_test_set: false

    # =================================================================
    # 3. Other Models
    # =================================================================
    SGDRegressor:
      model_class: "SGDRegressor"
      name: "SGD_Tuned"
      run_name: "SGDRegressor_Tuned"
      log_model_artifact: true
      register_model: false
      log_predictions: true
      log_interpretability_artifacts: true
      log_plots: true
      evaluate_on_test_set: false
      drop_multicollinear_cols: ["time"]
      training_params:
        loss: "squared_error"
        penalty: "l2"
        alpha: 0.0001
        max_iter: 1000
        random_state: 42
      train_model_only: false
      cross_validation:
        enabled: true
        n_splits: 5

    SVR:
      model_class: "SVR"
      name: "SVR_Tuned"
      run_name: "SVR_Tuned"
      log_model_artifact: true
      register_model: false
      log_predictions: true
      log_interpretability_artifacts: true
      log_plots: true
      evaluate_on_test_set: false
      drop_multicollinear_cols: []
      training_params:
        kernel: "rbf"
        C: 1.0
        gamma: "scale"
        epsilon: 0.1
      train_model_only: false
      cross_validation:
        enabled: true
        n_splits: 5
