{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\u2708\ufe0f Flight Price Prediction: An MLOps Project","text":"<p>Welcome to the official documentation for the Flight Price Prediction project. This project is a comprehensive, end-to-end MLOps pipeline for predicting flight prices, designed to be reproducible, automated, and robust.</p> <p>This documentation serves as a central hub for all project artifacts, analysis, and reports, guiding you through the entire lifecycle from data to deployment.</p>"},{"location":"#project-architecture","title":"\ud83c\udfdb\ufe0f Project Architecture","text":"<p>The architecture is designed to be modular and scalable. The primary method of orchestration is DVC pipelines, which define and connect all stages of data processing and model training.</p> <pre><code>graph TD\n    subgraph \"Code &amp; Data Versioning\"\n        Dev[Developer] -- \"git push\" --&gt; GitHub[GitHub Repo];\n        Dev -- \"dvc push\" --&gt; S3[S3 Storage];\n        GitHub --&gt; DVC[DVC];\n        DVC --&gt; S3;\n    end\n\n    subgraph \"Automated Data Pipelines\"\n        DVC_Pipeline[DVC Pipeline: `dvc repro`] --&gt; Bronze[Bronze Pipeline];\n        Bronze -- \"Uses\" --&gt; GE[Great Expectations];\n        Bronze --&gt; Silver[Silver Pipeline];\n        Silver -- \"Uses\" --&gt; GE;\n        Silver --&gt; Gold[Gold Pipeline];\n        Gold -- \"Uses\" --&gt; GE;\n    end\n\n    subgraph \"Modeling &amp; Tracking\"\n        Gold --&gt; Training[Training/Tuning&lt;br/&gt;Pipelines];\n        Training -- \"Logs to\" --&gt; MLflow[MLflow Tracking&lt;br/&gt;&amp; Registry];\n    end\n\n    subgraph \"Deployment &amp; Serving\"\n        GitHub -- \"CI/CD Trigger\" --&gt; GHA[GitHub Actions];\n        GHA --&gt; Build[Build &amp; Test];\n        Build --&gt; Deploy[Deploy to Google Cloud Run];\n        Deploy -- \"Loads Model from\" --&gt; MLflow;\n    end\n</code></pre> <p>For a deeper dive, see the full Architecture Documentation.</p>"},{"location":"#navigating-the-documentation","title":"\ud83d\uddfa\ufe0f Navigating the Documentation","text":"<p>This project is documented across several key areas. Here\u2019s a recommended reading path to understand the project from the ground up:</p> <ol> <li> <p>The \"Why\": Business Objective &amp; Data Insights</p> <ul> <li>Exploratory Data Analysis (EDA): Understand the business case and discover the key patterns and insights that drive our modeling strategy.</li> </ul> </li> <li> <p>The \"How\": System Design &amp; Pipelines</p> <ul> <li>MLOps Architecture: A high-level look at the tools and workflows that power this project.</li> <li>Data Pipelines: Learn about the Medallion architecture (Bronze, Silver, Gold) used to process and validate the data.</li> <li>Modeling Pipelines: See how models are trained, tuned, and evaluated systematically.</li> <li>Continuous Integration (CI): See how the project is automatically tested and validated.</li> <li>Continuous Deployment (CD): Learn how the model is automatically deployed to production.</li> </ul> </li> <li> <p>The \"What\": Model &amp; API</p> <ul> <li>Model Selection Report: Follow the journey of how the champion model was chosen, including the investigation into an over-performing initial model.</li> <li>Champion Model Deep Dive: An in-depth analysis of the final LightGBM model's behavior using SHAP.</li> <li>API Reference: Detailed documentation for the production-ready FastAPI prediction server, including deployment to Google Cloud.</li> </ul> </li> <li> <p>The \"Tools\": MLOps Stack</p> <ul> <li>Tooling Overview: A summary of all the MLOps tools used and their roles in the project.</li> </ul> </li> </ol>"},{"location":"#roadmap-future-work","title":"\ud83d\udee3\ufe0f Roadmap &amp; Future Work","text":"<p>This project provides a solid foundation for a production-ready MLOps workflow. The following steps outline a roadmap for enhancing its capabilities to a more advanced, enterprise-grade level:</p> <ol> <li> <p>Advanced Orchestration with Apache Airflow</p> <ul> <li>Current State: Pipelines are orchestrated manually via <code>dvc repro</code>.</li> <li>Next Step: Implement the existing Airflow DAGs (<code>airflow/dags</code>) to run the DVC pipelines on a schedule. This enables automated, time-based retraining and data processing, moving from a manual trigger to a true production orchestrator.</li> </ul> </li> <li> <p>Advanced CI/CD Workflows</p> <ul> <li>Current State: CI validates the pipeline and CD deploys the API on new tags.</li> <li>Next Step: Enhance the CD workflow to include automated integration tests against the deployed API. Implement a \"staging\" environment to automatically deploy PRs for review before merging to <code>main</code>.</li> </ul> </li> <li> <p>Comprehensive Monitoring</p> <ul> <li>Data &amp; Model Drift: Integrate tools like Evidently AI or NannyML to monitor for statistical drift in input data and degradation in model performance over time.</li> <li>Prediction Monitoring: Implement logging and alerting for the prediction API to track request latency, error rates, and the distribution of incoming prediction data.</li> <li>Dashboarding: Create dashboards (e.g., using Grafana or a BI tool) to visualize model performance metrics, data drift reports, and API health.</li> </ul> </li> <li> <p>Advanced Model Deployment &amp; Management</p> <ul> <li>A/B Testing &amp; Shadow Deployment: Enhance the deployment strategy to allow for A/B testing new models against the production champion or deploying models in a \"shadow\" mode to monitor their predictions without affecting users.</li> <li>Automated Model Promotion: Create a workflow (potentially in MLflow or via CI/CD) to automatically promote a model from \"Staging\" to \"Production\" in the MLflow Model Registry if it passes all evaluation and validation criteria.</li> </ul> </li> </ol>"},{"location":"quickstart/","title":"\u2708\ufe0f Flight Price Prediction MLOps Project","text":"<p>This project is a comprehensive, end-to-end MLOps pipeline for predicting flight prices. It leverages a modern stack of data and machine learning tools to build a reproducible, automated, and robust system that covers the entire lifecycle, from data ingestion and validation to model training, explainability, and serving.</p>"},{"location":"quickstart/#features","title":"\u2728 Features","text":"<ul> <li>Data &amp; Pipeline Versioning: Uses DVC to version control data, models, and intermediate artifacts, ensuring full reproducibility.</li> <li>Declarative Pipeline Orchestration: The entire ML pipeline is defined as code in <code>dvc.yaml</code>, allowing for robust, dependency-aware execution.</li> <li>Automated Data Validation: Integrates Great Expectations at each pipeline stage to enforce data quality.</li> <li>Experiment Tracking &amp; Model Management: Integrates MLflow for comprehensive experiment tracking, parameter logging, and model registration.</li> <li>Automated CI/CD: Implements GitHub Actions for automated linting, testing, pipeline validation, and deployment to Google Cloud.</li> <li>Production-Ready API: Includes a high-performance FastAPI server to serve the champion model, containerized with Docker.</li> <li>Reproducible Environment: Project dependencies are managed with <code>uv</code> and locked in <code>requirements.lock</code> for fast, deterministic setups.</li> </ul>"},{"location":"quickstart/#quickstart-local-setup","title":"\ud83d\ude80 Quickstart: Local Setup","text":"<p>Follow these steps to get the project running on your local machine.</p>"},{"location":"quickstart/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.12+</li> <li>uv: An extremely fast Python package installer and resolver.</li> <li>Git</li> <li>DVC</li> <li>act (Optional, for local CI/CD testing)</li> </ul>"},{"location":"quickstart/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone https://github.com/dakshvanshaj/flights-price-prediction-mlops.git\ncd flights-price-prediction-mlops\n</code></pre>"},{"location":"quickstart/#2-create-virtual-environment-install-dependencies","title":"2. Create Virtual Environment &amp; Install Dependencies","text":"<pre><code># Create and activate a virtual environment using uv\nuv venv\nsource .venv/bin/activate\n# On Windows: .\\.venv\\Scripts\\activate\n\n# Sync the environment with the lock file for a reproducible setup\nuv pip sync requirements.lock\n\n# Install the project in editable mode\nuv pip install -e .\n</code></pre>"},{"location":"quickstart/#3-get-the-project-data","title":"3. Get the Project Data","text":"<p>You have two options to get the data needed to run the pipelines.</p>"},{"location":"quickstart/#option-a-quick-local-start-no-credentials-needed","title":"Option A: Quick Local Start (No Credentials Needed)","text":"<p>This is the fastest way to get started. This project includes a Git-tracked archive with the initial raw data.</p> <pre><code># Unzip the archive to get the initial flights.csv\nunzip data/archieve-git-tracked/raw.zip -d data/raw/\n</code></pre>"},{"location":"quickstart/#option-b-full-dvc-setup-recommended","title":"Option B: Full DVC Setup (Recommended)","text":"<p>To get all versioned data, models, and artifacts, you must configure DVC to connect to the remote S3-compatible storage. See the DVC Integration Guide \u00bb for more details.</p> <pre><code># Configure the DVC remote endpoint URL and credentials.\ndvc remote add -d myremote s3://your-bucket-name\ndvc remote modify --local myremote endpointurl &lt;YOUR_S3_ENDPOINT_URL&gt;\ndvc remote modify --local myremote access_key_id &lt;YOUR_ACCESS_KEY_ID&gt;\ndvc remote modify --local myremote secret_access_key &lt;YOUR_SECRET_ACCESS_KEY&gt;\n\n# Pull all DVC-tracked data and model artifacts\ndvc pull -v\n</code></pre>"},{"location":"quickstart/#4-set-up-mlflow-tracking-server-optional","title":"4. Set Up MLflow Tracking Server (Optional)","text":"<p>By default, MLflow will log experiments locally. To use a remote, centralized server, create a <code>.env</code> file in the project root and populate it with your server's credentials. The application will automatically load these using <code>dotenv</code>.</p> <pre><code># .env file\nMLFLOW_TRACKING_URI=http://your-remote-mlflow-server-ip:5000\nMLFLOW_AWS_ACCESS_KEY_ID=your_mlflow_s3_access_key\nMLFLOW_AWS_SECRET_ACCESS_KEY=your_mlflow_s3_secret_key\nMLFLOW_AWS_DEFAULT_REGION=your_s3_bucket_region\n</code></pre> <p>For a complete guide on deploying a production-grade MLflow server, see the MLflow Deployment Documentation \u00bb.</p>"},{"location":"quickstart/#5-running-the-pipelines","title":"5. Running the Pipelines","text":"<p>You can run the project's pipelines in several ways. See the DVC Pipeline Documentation \u00bb for a full breakdown.</p>"},{"location":"quickstart/#method-1-automated-dag-execution-with-dvc-recommended","title":"Method 1: Automated DAG Execution with DVC (Recommended)","text":"<pre><code># Run the entire pipeline from start to finish\ndvc repro\n\n# Force if it shows no change in pipeline\ndvc repro -f\n\n# Or, run the pipeline up to a specific stage\ndvc repro gold_pipeline\n</code></pre>"},{"location":"quickstart/#method-2-manual-script-execution-for-debugging","title":"Method 2: Manual Script Execution (For Debugging)","text":"<p>Use the CLI shortcuts defined in <code>pyproject.toml</code>:</p> <pre><code>run-bronze-pipeline train.csv\nrun-silver-pipeline train.csv\nrun-gold-pipeline\nrun-training-pipeline\n</code></pre>"},{"location":"quickstart/#configuring-the-pipelines","title":"\ud83d\udd27 Configuring the Pipelines","text":"<p>The behavior of the pipelines can be customized without changing the source code.</p> <ul> <li>High-Level Parameters (<code>params.yaml</code>): Control the overall strategy, such as which model to run (<code>model_config_to_run</code>) or whether to use the tree-based preprocessing path (<code>is_tree_model</code>).</li> <li>Low-Level Configuration (<code>src/shared/config/</code>): Contains static configurations like file paths and column lists for transformations.</li> </ul>"},{"location":"quickstart/#local-cicd-testing-with-act","title":"\ud83e\udd16 Local CI/CD Testing with <code>act</code>","text":"<p>You can run the GitHub Actions workflows locally using act. This is incredibly useful for testing changes to your CI/CD pipeline without pushing to GitHub. See the CI and CD docs for more details.</p>"},{"location":"quickstart/#setup","title":"Setup","text":"<p>Create a <code>.secrets</code> file in the project root and populate it with the necessary credentials for local testing.</p> <p>Warning: The <code>.secrets</code> file contains sensitive information. It is already listed in <code>.gitignore</code> and should never be committed to version control.</p>"},{"location":"quickstart/#usage","title":"Usage","text":"<pre><code># Run the default `on: push` workflow\nact\n\n# Run a specific job from a workflow\nact -j test_and_lint\n\n# Run the CD workflow by simulating a tag push\nact push -W .github/workflows/cd.yml -e tag_push_event.json\n</code></pre>"},{"location":"quickstart/#full-project-documentation","title":"\ud83d\udcda Full Project Documentation","text":"<p>This project is documented using MkDocs. To view the full, searchable documentation site locally, run:</p> <pre><code>mkdocs serve\n</code></pre> <p>Navigate to <code>http://127.0.0.1:8000</code> in your browser.</p>"},{"location":"API/api_reference/","title":"\ud83d\ude80 API Reference &amp; Deployment Guide","text":"<p>This document provides a comprehensive technical reference for the FastAPI prediction server, including an in-depth analysis of its architecture, containerization, and cloud deployment workflow on Google Cloud.</p> <ul> <li>Source Code: <code>src/prediction_server/</code></li> </ul>"},{"location":"API/api_reference/#overview","title":"\ud83c\udfaf Overview","text":"<p>The prediction server is a high-performance API built with FastAPI that serves the champion LightGBM model. It is designed with production best practices to provide low-latency, reliable flight price predictions.</p> <p>Key architectural features include:</p> <ul> <li>Asynchronous by Default: Built on FastAPI for high throughput.</li> <li>Robust Data Validation: Uses Pydantic schemas to define a strict, self-documenting contract for all API requests and responses, preventing invalid data from ever reaching the model.</li> <li>Efficient Startup/Shutdown: Loads the MLflow model and all DVC-tracked preprocessing artifacts into memory on startup to minimize prediction latency. These resources are gracefully released on shutdown.</li> <li>Decoupled Configuration: The specific model to be served is managed in a dedicated configuration file, allowing for model updates without changing the application code.</li> </ul>"},{"location":"API/api_reference/#core-components-analysis","title":"\u2699\ufe0f Core Components Analysis","text":"<p>The server's logic is modularized across several key files.</p>"},{"location":"API/api_reference/#mainpy-the-fastapi-application","title":"<code>main.py</code>: The FastAPI Application","text":"<p>This is the core of the server. It uses a <code>lifespan</code> context manager to load all necessary ML artifacts into a global <code>ml_artifacts</code> dictionary upon startup. This is a crucial performance optimization, as it means the model and transformers are loaded only once, not on every prediction request.</p>"},{"location":"API/api_reference/#schemaspy-the-data-contract","title":"<code>schemas.py</code>: The Data Contract","text":"<p>This file defines the API's data contract using Pydantic. The <code>InputSchema</code> is particularly important, as it uses Python's <code>Enum</code> types for categorical features like <code>from_location</code> and <code>flight_type</code>. This provides automatic, built-in validation, ensuring that only valid categories are accepted by the API.</p>"},{"location":"API/api_reference/#predictpy-the-transformation-engine","title":"<code>predict.py</code>: The Transformation Engine","text":"<p>This module is the bridge between raw API input and the model's required feature format.</p> <ol> <li><code>preprocessing_for_prediction</code>: This function meticulously replicates the entire training pipeline. It takes the raw DataFrame from the API request and applies the exact same sequence of transformations (Silver and Gold steps) using the pre-loaded preprocessor objects. This guarantees that the data fed to the model at inference time has the exact same structure, encoding, and scale as the data it was trained on.</li> <li><code>postprocessing_for_target</code>: Since the model predicts a transformed target (due to scaling and power transformations), this function reverses those steps. It takes the model's raw output and applies the inverse transformations from the <code>Scaler</code> and <code>PowerTransformer</code> objects to return a final price in its original, interpretable currency format.</li> </ol>"},{"location":"API/api_reference/#model_loaderpy-artifact-loading","title":"<code>model_loader.py</code>: Artifact Loading","text":"<p>This module is responsible for loading all ML assets.</p> <ul> <li><code>load_production_model()</code>: Connects to the MLflow Tracking Server using environment variables, and pulls the model specified by <code>MODEL_NAME</code> and <code>MODEL_VERSION_ALIAS</code> from the Model Registry.</li> <li><code>load_preprocessing_artifacts()</code>: Loads all the data transformers (imputer, encoder, scaler, etc.) that were fitted during the <code>gold_pipeline</code> run and saved by DVC.</li> </ul>"},{"location":"API/api_reference/#api-endpoints","title":"\ud83d\udce1 API Endpoints","text":""},{"location":"API/api_reference/#health-check","title":"Health Check","text":"<ul> <li>Endpoint: <code>GET /</code></li> <li>Description: A simple health check to confirm that the API is running and responsive.</li> <li> <p>Success Response (200):</p> <p><code>json {   \"status\": \"ok\",   \"message\": \"Welcome to the Flight Price Prediction API\" }</code></p> </li> </ul>"},{"location":"API/api_reference/#prediction","title":"Prediction","text":"<ul> <li>Endpoint: <code>POST /prediction</code></li> <li>Description: The core endpoint that takes flight details, preprocesses them, runs the prediction, and returns the final estimated price.</li> </ul>"},{"location":"API/api_reference/#request-body-inputschema","title":"Request Body (<code>InputSchema</code>)","text":"<p>The request body must be a JSON object conforming to the <code>InputSchema</code>.</p> <pre><code>{\n  \"from_location\": \"Recife (PE)\",\n  \"to_location\": \"Florianopolis (SC)\",\n  \"flight_type\": \"firstClass\",\n  \"time\": 1.76,\n  \"distance\": 676.53,\n  \"agency\": \"FlyingDrops\",\n  \"date\": \"2019-09-26\"\n}\n</code></pre>"},{"location":"API/api_reference/#success-response-200-outputschema","title":"Success Response (200 - <code>OutputSchema</code>)","text":"<pre><code>{\n  \"predicted_price\": 1434.35\n}\n</code></pre>"},{"location":"API/api_reference/#containerization-execution","title":"\ud83d\udc33 Containerization &amp; Execution","text":"<p>The server is containerized using a multi-stage <code>Dockerfile</code> for a lean and secure production image.</p>"},{"location":"API/api_reference/#the-docker-entrypointsh-startup-logic","title":"The <code>docker-entrypoint.sh</code> Startup Logic","text":"<p>The container's startup process is managed by the entrypoint script, which performs critical setup before the FastAPI application starts. This two-step process is key to the server's architecture.</p> <pre><code>graph TD\n    A[Container Starts] --&gt; B{Run `docker-entrypoint.sh`};\n    B --&gt; C{Step 1: Configure DVC &amp; Pull Artifacts};\n    C --&gt; D{Step 2: Export MLflow Credentials};\n    D --&gt; E{Step 3: `exec uvicorn`};\n    E --&gt; F[FastAPI App Runs with All Artifacts &amp; Credentials];\n</code></pre> <ol> <li>DVC Configuration &amp; Pull: The script first uses <code>DVC_*</code> and <code>B2_*</code> environment variables to configure DVC to connect to the S3-compatible remote storage. It then runs <code>dvc pull models</code> to download the fitted data transformers.</li> <li>MLflow Credential Export: Next, the script takes the <code>MLFLOW_*</code> environment variables and exports them as standard <code>AWS_*</code> credentials. The <code>exec \"$@\"</code> command then replaces the script process with the Uvicorn server process, which inherits these exported variables, allowing it to connect to the MLflow Tracking Server.</li> </ol>"},{"location":"API/api_reference/#building-and-running-the-container","title":"Building and Running the Container","text":"<ol> <li>Build the Image:     From the project root, run:</li> </ol> <pre><code>docker build -t prediction-server:latest -f src/prediction_server/Dockerfile .\n</code></pre> <ol> <li>Run the Container:     Use an <code>.env</code> file (e.g., <code>prediction_app.env</code>) to securely manage your credentials.</li> </ol> <pre><code>docker run --env-file ./src/prediction_server/prediction_app.env -p 9000:9000 prediction-server:latest\n</code></pre> <pre><code>The API will be available at [http://localhost:9000/docs](http://localhost:9000/docs).\n</code></pre>"},{"location":"API/api_reference/#cloud-deployment-google-cloud-run","title":"\u2601\ufe0f Cloud Deployment: Google Cloud Run","text":"<p>Deploying to Google Cloud Run provides a secure, scalable, and cost-effective serverless environment. The primary deployment is automated via the CD workflow.</p>"},{"location":"API/api_reference/#step-1-secure-credentials-with-google-secret-manager","title":"\ud83d\udd11 Step 1: Secure Credentials with Google Secret Manager","text":"<p>Before deployment, all secrets (from your <code>.env</code> file) must be stored securely in Google Secret Manager. Each variable (<code>MLFLOW_TRACKING_URI</code>, <code>AWS_ACCESS_KEY_ID</code>, <code>B2_ACCESS_KEY_ID</code>, etc.) should be created as a separate secret.</p>"},{"location":"API/api_reference/#step-2-create-a-dedicated-service-account","title":"\ud83d\udc64 Step 2: Create a Dedicated Service Account","text":"<p>A best practice is to create a dedicated service account for the Cloud Run service with minimal permissions:</p> <ul> <li>Secret Manager Secret Accessor: Allows it to read the secrets.</li> <li>Artifact Registry Reader: Allows it to pull the container image.</li> </ul>"},{"location":"API/api_reference/#step-3-deploy-the-service","title":"\ud83d\ude80 Step 3: Deploy the Service","text":"<p>While this is automated in CI/CD, here is how to do it manually via the Google Cloud Console:</p> <ol> <li>Navigate to Cloud Run and click \"Create service\".</li> <li>Container Image: Select the image you pushed to Artifact Registry.</li> <li>Authentication: Choose \"Require authentication\" for a private API.</li> <li>Container Port: Set to <code>9000</code>.</li> <li>Autoscaling: Set Min instances to <code>0</code> (to scale to zero) and Max instances to a reasonable number (e.g., <code>3</code>) to control costs.</li> <li>Security Tab: Attach the dedicated service account you created.</li> <li>Variables &amp; Secrets Tab: This is the most critical step.<ul> <li>For every environment variable your application needs, add a new variable.</li> <li>Choose \"Reference a secret\", select the corresponding secret from Secret Manager, and use the <code>latest</code> version.</li> <li>Do not enter any secrets in plain text.</li> </ul> </li> <li>Click \"Create\" to deploy.</li> </ol> <p>Once deployed, Cloud Run provides a secure, public URL for your service. In a full production setup, this URL would be fronted by a Google Cloud API Gateway to handle API key authentication and rate limiting.</p>"},{"location":"CD/cd/","title":"\ud83d\ude80 Continuous Deployment (CD) with GitHub Actions","text":"<p>This project uses GitHub Actions for Continuous Deployment to automate the release and deployment of the prediction server to Google Cloud Run. The workflow is defined in <code>.github/workflows/cd.yml</code>.</p>"},{"location":"CD/cd/#trigger","title":"\ud83c\udfaf Trigger","text":"<p>The CD workflow is triggered automatically whenever a new tag matching the pattern <code>v*</code> (e.g., <code>v1.0</code>, <code>v1.1.0</code>) is pushed to the repository. This practice ensures that only specific, versioned releases are deployed to production.</p> <pre><code>graph TD\n    A[Push tag `v*`] --&gt; B{CD Workflow};\n    B --&gt; C[Authenticate to GCP];\n    C --&gt; D[Build &amp; Push Docker Image];\n    D --&gt; E[Deploy to Cloud Run];\n</code></pre>"},{"location":"CD/cd/#deployment-steps","title":"\ud83e\ude9c Deployment Steps","text":"<ol> <li> <p>Authenticate: Authenticates to Google Cloud using a service account key stored in GitHub Secrets.</p> </li> <li> <p>Configure Docker: Configures the Docker client to push images to Google Artifact Registry.</p> </li> <li> <p>Build and Push Image: Builds the prediction server Docker image from <code>src/prediction_server/Dockerfile</code> and pushes it to the project's private Artifact Registry. The image is tagged with the Git tag from the release (e.g., <code>prediction-server:v1.0</code>).</p> </li> <li> <p>Deploy to Cloud Run: Deploys the newly pushed image to the specified Cloud Run service, which automatically creates a new, serving revision. The workflow also securely injects all necessary secrets (for DVC, MLflow, etc.) from Google Secret Manager into the running service environment.</p> </li> </ol>"},{"location":"CI/ci/","title":"\ud83d\udee1\ufe0f Continuous Integration (CI) with GitHub Actions","text":"<p>This project uses GitHub Actions for Continuous Integration to automate quality checks and validate the integrity of the entire project on every code change.</p> <p>The CI pipeline runs automatically on every push and pull request to the <code>main</code> branch. Its purpose is to provide rapid feedback and prevent errors from being merged into the main codebase. The workflow is defined in <code>.github/workflows/ci.yml</code>.</p> <p>It consists of three main jobs that run in parallel after an initial setup:</p> <pre><code>graph TD\n    A[Push or PR to `main`] --&gt; B{CI Workflow};\n    B --&gt; C[test_and_lint];\n    C -- Success --&gt; D[validate_dvc_pipeline];\n    C -- Success --&gt; E[build_prediction_server];\n</code></pre>"},{"location":"CI/ci/#1-test_and_lint","title":"\ud83e\uddea 1. <code>test_and_lint</code>","text":"<p>This is the first gate of quality control. It performs three critical checks:</p> <ul> <li>Linting: Uses <code>ruff check .</code> to enforce code style and detect common errors.</li> <li>Formatting: Uses <code>ruff format --check .</code> to ensure consistent code formatting.</li> <li>Unit Testing: Executes the entire test suite with <code>pytest</code> to verify that individual components work as expected.</li> </ul>"},{"location":"CI/ci/#2-validate_dvc_pipeline","title":"\u26d3\ufe0f 2. <code>validate_dvc_pipeline</code>","text":"<p>This job runs only after <code>test_and_lint</code> succeeds. It ensures the end-to-end data pipeline is reproducible and functional:</p> <ol> <li>Configure Credentials: Securely configures credentials for MLflow (AWS) and DVC (Backblaze B2) using GitHub Secrets.</li> <li>Pull DVC Data: Runs <code>dvc pull</code> to download the data, models, and artifacts required for the pipeline.</li> <li>Reproduce Pipeline: Executes <code>dvc repro</code> to run the entire DVC pipeline from start to finish. This is a crucial integration test that validates the data processing and model training stages defined in <code>dvc.yaml</code>.</li> </ol>"},{"location":"CI/ci/#3-build_prediction_server","title":"\ud83d\udc33 3. <code>build_prediction_server</code>","text":"<p>This job also runs after <code>test_and_lint</code> and verifies that the prediction server's Docker image can be built successfully, catching any potential dependency or configuration issues early.</p>"},{"location":"Data%20Pipelines/bronze_pipeline/","title":"\ud83e\udd49 Bronze Pipeline","text":"<p>The Bronze pipeline is the first entry point for raw data into the system. Its primary responsibility is to act as an initial quality gate, ensuring that incoming data conforms to a basic, expected schema and structure.</p> <ul> <li>Source Code: <code>src/pipelines/bronze_pipeline.py</code></li> </ul>"},{"location":"Data%20Pipelines/bronze_pipeline/#purpose","title":"\ud83c\udfaf Purpose","text":"<ul> <li>To validate the structure and basic quality of raw data files using Great Expectations.</li> <li>To separate valid data from invalid data, preventing \"garbage in, garbage out.\"</li> </ul>"},{"location":"Data%20Pipelines/bronze_pipeline/#pipeline-workflow","title":"\ud83d\udd04 Pipeline Workflow","text":"<pre><code>%%{init: {'theme': 'dark'}}%%\ngraph TD\n    A[Start] --&gt; B{Input: Raw CSV File};\n    B --&gt; C{Initialize GE Context};\n    C --&gt; D{Get Datasource &amp; Asset};\n    D --&gt; E{Build Expectation Suite};\n    E --&gt; F{Run GE Checkpoint};\n    F --&gt; G{Validation Result?};\n    G -- Success --&gt; H[Move to Bronze Processed];\n    G -- Failure --&gt; I[Move to Bronze Quarantined];\n    H --&gt; J[End];\n    I --&gt; J;\n</code></pre>"},{"location":"Data%20Pipelines/bronze_pipeline/#key-steps","title":"\ud83d\udd11 Key Steps","text":"<ol> <li>Initialize Great Expectations (GE) Context: Sets up the GE environment.</li> <li>Define Data Source and Asset: Points GE to the raw data directory and specifies how to read the CSV files.</li> <li>Build and Apply Expectation Suite: Uses the <code>build_bronze_expectations</code> suite to check for things like column presence, non-nullness, and basic type adherence.</li> <li>Run Checkpoint: Executes the validation.</li> <li>Move File Based on Result:<ul> <li>On Success: Moves the raw file to the <code>data/bronze_data/processed/</code> directory.</li> <li>On Failure: Moves the raw file to the <code>data/bronze_data/quarantined/</code> directory.</li> </ul> </li> </ol>"},{"location":"Data%20Pipelines/bronze_pipeline/#how-to-run","title":"\u25b6\ufe0f How to Run","text":"<p>After installing the project in editable mode (<code>pip install -e .</code>), you can use the CLI shortcut defined in <code>pyproject.toml</code>.</p> <p>Using CLI Shortcut:</p> <pre><code>run-bronze-pipeline &lt;file_name.csv&gt;\n</code></pre> <p>Example:</p> <pre><code>run-bronze-pipeline train.csv\n</code></pre> <p>Direct Execution:</p> <pre><code>python src/pipelines/bronze_pipeline.py &lt;file_name.csv&gt;\n</code></pre>"},{"location":"Data%20Pipelines/bronze_pipeline/#configuration","title":"\u2699\ufe0f Configuration","text":"<p>The Bronze pipeline's behavior is configured through constants defined within the project's Python modules, primarily in <code>src/shared/config/config_bronze.py</code>. Key configurable parameters include:</p> <ul> <li>Paths: Input directory for raw data (<code>RAW_DATA_SOURCE</code>), and output directories for processed (<code>BRONZE_PROCESSED_DIR</code>) and quarantined files (<code>BRONZE_QUARANTINE_DIR</code>).</li> <li>Great Expectations: Names for the data source, asset, batch definition, expectation suite, and checkpoint.</li> <li>Logging: The output path for log files (<code>BRONZE_PIPELINE_LOGS_PATH</code>) and the path to the <code>logging.yaml</code> configuration.</li> </ul> <p>These are generally static configurations managed by developers within the codebase.</p>"},{"location":"Data%20Pipelines/bronze_pipeline/#dependencies-and-environment","title":"\ud83d\udce6 Dependencies and Environment","text":"<ul> <li>Key Libraries: <code>great-expectations</code>, <code>pandas</code>, <code>pyyaml</code>, <code>python-json-logger</code>.</li> <li>Input Schema: The pipeline expects raw CSV files. The exact schema is defined in the <code>BronzeExpectations</code> suite. A typical file is expected to contain columns related to flight details.</li> </ul>"},{"location":"Data%20Pipelines/bronze_pipeline/#error-handling-and-quarantining","title":"\ud83d\udc1b Error Handling and Quarantining","text":"<ul> <li>Process: If the input file fails the Great Expectations validation checkpoint, the pipeline's execution is marked as failed. The <code>handle_file_based_on_validation</code> utility function moves the entire source file from the raw data directory to the quarantine directory (<code>data/bronze_data/quarantined/</code>).</li> <li>Debugging: To debug a quarantined file, a developer should:<ol> <li>Check the pipeline's log file for details on the failure.</li> <li>Inspect the Great Expectations Data Docs, which provide a detailed report on which specific expectation failed and why.</li> <li>Manually examine the quarantined CSV file to find the offending data.</li> </ol> </li> <li>Common Failure Reasons:<ul> <li>A column is missing, has been renamed, or is in the wrong order.</li> <li>Null values are present in a column that is expected to be non-null.</li> <li>The data type of a column does not match the expectation (e.g., a string appears in a numeric column).</li> </ul> </li> </ul>"},{"location":"Data%20Pipelines/bronze_pipeline/#next-steps","title":"\u27a1\ufe0f Next Steps","text":"<ul> <li>Silver Pipeline - Data Preprocessing &amp; Validation \u00bb</li> <li>Gold Pipeline - Feature Engineering, Preprocessing &amp; Validation \u00bb</li> </ul>"},{"location":"Data%20Pipelines/data_pipelines/","title":"\ud83c\udf0a Data Pipelines","text":"<p>This document outlines the architecture of the data pipelines, which are designed to progressively process, validate, and enrich data. The project follows the Medallion Architecture (Bronze, Silver, Gold) to ensure data quality and traceability from raw source to model-ready features.</p>"},{"location":"Data%20Pipelines/data_pipelines/#the-medallion-architecture","title":"\ud83c\udfdb\ufe0f The Medallion Architecture","text":"<p>This project adopts a layered approach to data processing, which provides a robust and maintainable workflow.</p> <p>Key Benefits of this Architecture: -   Data Quality &amp; Governance: Each layer enforces increasingly strict quality standards, ensuring that business-critical data is reliable and well-documented. -   Traceability &amp; Debugging: When an issue arises, it's easy to trace it back through the layers to pinpoint exactly where an error was introduced. -   Idempotency &amp; Reprocessing: The separation of layers allows for efficient reprocessing. If a bug is found in the Gold pipeline, it can be fixed and re-run on the trusted Silver data without needing to re-ingest the raw source data.</p>"},{"location":"Data%20Pipelines/data_pipelines/#core-technologies","title":"Core Technologies","text":"<ul> <li>Great Expectations: Acts as the primary data quality framework. It is used to define \"expectation suites\" at the end of each pipeline stage, acting as a quality gate that prevents bad data from moving to the next layer.</li> <li>Parquet: The chosen storage format for the Silver and Gold data layers. As a columnar format, Parquet offers significant performance and storage efficiency advantages over row-based formats like CSV.</li> </ul>"},{"location":"Data%20Pipelines/data_pipelines/#end-to-end-pipeline-flow","title":"\ud83d\uddfa\ufe0f End-to-End Pipeline Flow","text":"<p>The data flows through a series of DVC-orchestrated pipelines, with Great Expectations validating the output of each stage.</p> <pre><code>%%{init: {'theme': 'dark'}}%%\ngraph TD\n    A[Raw Data csv] --&gt; B{Bronze Pipeline&lt;br/&gt;Validation};\n    B -- Validation Pass --&gt; C[Bronze Data csv];\n    B -- Validation Fail --&gt; D[Quarantined Raw Data];\n    C --&gt; E{Silver Pipeline&lt;br/&gt;Transformation + Validation};\n    E -- Validation Pass --&gt; F[Silver Data .parquet];\n    E -- Validation Fail --&gt; G[Quarantined Bronze Data];\n    F --&gt; H{Gold Pipeline&lt;br/&gt;Feature Engineering +&lt;br/&gt;Transformation + Validation};\n    H -- Validation Pass --&gt; I[Gold Data .parquet];\n    H -- Validation Fail --&gt; J[Quarantined Silver Data];\n    I --&gt; K{Training &amp; Tuning Pipelines};\n    K -- Logs &amp; Models --&gt; M[MLflow Tracking Server];\n</code></pre>"},{"location":"Data%20Pipelines/data_pipelines/#pipeline-stages-in-detail","title":"\ud83e\udde9 Pipeline Stages in Detail","text":""},{"location":"Data%20Pipelines/data_pipelines/#bronze-stage","title":"\ud83e\udd49 Bronze Stage","text":"<p>The Bronze pipeline is the first quality gate for raw data. It validates the basic structure and schema of incoming files using Great Expectations, separating valid data from invalid data to ensure a reliable foundation for subsequent processing.</p> <p>Learn more about the Bronze Pipeline \u00bb</p>"},{"location":"Data%20Pipelines/data_pipelines/#silver-stage","title":"\ud83e\udd48 Silver Stage","text":"<p>The Silver pipeline focuses on cleaning and conforming the data. It takes validated Bronze data and applies transformations like standardizing formats, enriching features (e.g., from dates), and handling duplicates to create a clean, consistent, and queryable dataset.</p> <p>Learn more about the Silver Pipeline \u00bb</p>"},{"location":"Data%20Pipelines/data_pipelines/#gold-stage","title":"\ud83e\udd47 Gold Stage","text":"<p>The Gold pipeline prepares the data for its final use case: machine learning. It applies complex feature engineering, imputation, encoding, and scaling transformations to the Silver data, producing a feature-rich, model-ready dataset.</p> <p>Learn more about the Gold Pipeline \u00bb</p>"},{"location":"Data%20Pipelines/gold_pipeline/","title":"\ud83e\udd47 Gold Pipeline","text":"<p>The Gold pipeline is the final and most intensive transformation stage. It prepares the data specifically for machine learning by applying complex feature engineering and preprocessing steps.</p> <ul> <li>Source Code: <code>src/pipelines/gold_pipeline.py</code></li> </ul>"},{"location":"Data%20Pipelines/gold_pipeline/#purpose","title":"\ud83c\udfaf Purpose","text":"<ul> <li>To create a feature-rich, analysis-ready dataset for modeling.</li> <li>To handle missing values, encode categorical variables, and apply advanced transformations.</li> <li>To provide a flexible workflow that can be optimized for different model architectures (e.g., linear models vs. tree-based models).</li> <li>To save the fitted preprocessing objects (like scalers and encoders) from the training run so they can be applied consistently across all data splits.</li> </ul>"},{"location":"Data%20Pipelines/gold_pipeline/#pipeline-workflow","title":"\ud83d\udd04 Pipeline Workflow","text":"<p>The pipeline's workflow is dynamically adjusted based on the <code>is_tree_model</code> parameter in <code>params.yaml</code>. This allows for an optimized path for tree-based models like LightGBM.</p> <pre><code>%%{init: {'theme': 'dark'}}%%\ngraph TD\n    A[Load Silver Data] --&gt; B{Clean &amp; Impute Data}\n    B --&gt; C{Feature Engineering}\n    C --&gt; D{is_tree_model: true?}\n    D -- No --&gt; E{Full Preprocessing&lt;br/&gt;Grouping, Outliers, Power Transforms, Scaling}\n    D -- Yes --&gt; F{Optimized Preprocessing&lt;br/&gt;Integer Encoding}\n    E --&gt; G{Run GE Validation}\n    F --&gt; G\n    G --&gt; H{Save Gold Data &amp; Transformers}\n</code></pre>"},{"location":"Data%20Pipelines/gold_pipeline/#key-steps","title":"\ud83d\udd11 Key Steps","text":"<p>The pipeline has two main execution paths, controlled by the <code>is_tree_model</code> parameter.</p>"},{"location":"Data%20Pipelines/gold_pipeline/#default-path-is_tree_model-false","title":"Default Path (<code>is_tree_model: false</code>)","text":"<p>This path is designed for models that are sensitive to feature scale and distribution, such as linear models or SVMs.</p> <ol> <li>Data Ingestion &amp; Cleaning: Loads and cleans data from the Silver layer.</li> <li>Imputation: Fills missing values based on the configured strategies.</li> <li>Feature Engineering: Creates cyclical and interaction features.</li> <li>Rare Category Grouping: Groups infrequent categorical values.</li> <li>Categorical Encoding: Transforms categorical columns into a numerical format (often One-Hot Encoding).</li> <li>Outlier Handling: Detects and mitigates the effect of outliers.</li> <li>Power Transformations: Applies transformations (e.g., Yeo-Johnson) to make data distributions more Gaussian-like.</li> <li>Scaling: Fits a scaler (e.g., StandardScaler) and scales numerical features.</li> <li>Final Validation &amp; Saving: Runs a Great Expectations checkpoint and saves the data and fitted transformers.</li> </ol>"},{"location":"Data%20Pipelines/gold_pipeline/#optimized-tree-model-path-is_tree_model-true","title":"Optimized Tree Model Path (<code>is_tree_model: true</code>)","text":"<p>This streamlined path is used for tree-based models like LightGBM and XGBoost, which do not require extensive preprocessing.</p> <ol> <li>Data Ingestion &amp; Cleaning: Same as the default path.</li> <li>Imputation: Same as the default path.</li> <li>Feature Engineering: Same as the default path.</li> <li>Categorical Encoding: Uses efficient integer-based encoding (e.g., <code>OrdinalEncoder</code>), which is handled natively by tree-based models.</li> <li>Final Validation &amp; Saving: Runs a Great Expectations checkpoint and saves the data and fitted transformers.</li> </ol> <p>Skipped Steps: In this path, Rare Category Grouping, Outlier Handling, Power Transformations, and Scaling are all bypassed, leading to a much faster and more efficient pipeline.</p>"},{"location":"Data%20Pipelines/gold_pipeline/#how-to-run","title":"\u25b6\ufe0f How to Run","text":"<p>The main function in the script orchestrates the processing for the <code>train</code>, <code>validation</code>, and <code>test</code> splits automatically.</p> <p>Using CLI Shortcut:</p> <pre><code>run-gold-pipeline\n</code></pre> <p>Direct Execution:</p> <pre><code>python src/pipelines/gold_pipeline.py\n</code></pre>"},{"location":"Data%20Pipelines/gold_pipeline/#configuration","title":"\u2699\ufe0f Configuration","text":"<p>The Gold pipeline uses a hybrid configuration approach:</p> <ul> <li><code>params.yaml</code>: This file stores parameters that are treated like hyperparameters for the pipeline itself. This includes strategies for imputation, outlier handling, scaling, and thresholds for grouping rare categories. A key parameter here is <code>is_tree_model</code>, which, when set to <code>true</code>, bypasses unnecessary steps like scaling and power transformations that are not required for tree-based models like LightGBM.</li> <li><code>src/shared/config/config_gold.py</code>: This stores more static, developer-managed configuration, such as lists of columns to be dropped, lists of columns to undergo specific transformations (e.g., <code>POWER_TRANSFORMER_COLUMNS</code>), and paths for saving processed data and fitted transformer objects.</li> </ul>"},{"location":"Data%20Pipelines/gold_pipeline/#dependencies-and-environment","title":"\ud83d\udce6 Dependencies and Environment","text":"<ul> <li>Key Libraries: <code>pandas</code>, <code>scikit-learn</code>, <code>great-expectations</code>.</li> <li>Input Schema: The pipeline expects Parquet files from the <code>data/silver_data/processed/</code> directory, conforming to the schema produced by the Silver pipeline.</li> </ul>"},{"location":"Data%20Pipelines/gold_pipeline/#error-handling-and-quarantining","title":"\ud83d\udc1b Error Handling and Quarantining","text":"<ul> <li>Process: Similar to the Silver pipeline, if the final DataFrame fails the Gold validation checkpoint, it is saved to the quarantine directory (<code>data/gold_data/quarantined/</code>). The pipeline logs are particularly helpful here, as they print a summary of the failing expectations directly to the console.</li> <li>Debugging:<ol> <li>Check the console output and log files for the summary of failing expectations.</li> <li>For a deeper dive, consult the Great Expectations Data Docs.</li> <li>Analyze the quarantined Parquet file to understand the data state that led to the failure.</li> </ol> </li> <li>Common Failure Reasons:<ul> <li>A feature engineering step produced an unexpected result (e.g., <code>NaN</code> or <code>inf</code>).</li> <li>The final number of columns is incorrect, often due to an issue with categorical encoding.</li> <li>The distribution of a scaled feature is outside the expected range, indicating a potential problem with the source data or the scaling logic.</li> </ul> </li> </ul>"},{"location":"Data%20Pipelines/silver_pipeline/","title":"\ud83e\udd48 Silver Pipeline","text":"<p>The Silver pipeline takes the validated data from the Bronze layer and begins the process of cleaning, standardizing, and enriching it.</p> <ul> <li>Source Code: <code>src/pipelines/silver_pipeline.py</code></li> </ul>"},{"location":"Data%20Pipelines/silver_pipeline/#purpose","title":"\ud83c\udfaf Purpose","text":"<ul> <li>To clean and standardize data.</li> <li>To perform initial feature engineering, such as extracting features from dates.</li> <li>To enforce a consistent schema and data types.</li> </ul>"},{"location":"Data%20Pipelines/silver_pipeline/#pipeline-workflow","title":"\ud83d\udd04 Pipeline Workflow","text":"<pre><code>%%{init: {'theme': 'dark'}}%%\ngraph TD\n    A[Start] --&gt; B{Input: Bronze CSV};\n    B --&gt; C{Load Data};\n    C --&gt; D{Preprocessing &amp; Cleaning};\n    D --&gt; E{Feature Engineering};\n    E --&gt; F{Enforce Schema};\n    F --&gt; G{Run GE Checkpoint};\n    G --&gt; H{Validation Result?};\n    H -- Success --&gt; I[Save to Silver Processed Parquet];\n    H -- Failure --&gt; J[Save to Silver Quarantined Parquet];\n    I --&gt; K[End];\n    J --&gt; K;\n</code></pre>"},{"location":"Data%20Pipelines/silver_pipeline/#key-steps","title":"\ud83d\udd11 Key Steps","text":"<ol> <li>Data Ingestion: Loads a file from the Bronze processed directory.</li> <li>Preprocessing &amp; Cleaning:<ul> <li>Renames columns for clarity.</li> <li>Standardizes column names to a consistent format (e.g., snake_case).</li> <li>Optimizes data types (e.g., converting strings to numeric/datetime).</li> <li>Sorts data by date to prepare for time-series analysis.</li> <li>Handles erroneous duplicates.</li> </ul> </li> <li>Feature Engineering: Creates new features from existing ones (e.g., <code>day</code>, <code>month</code>, <code>year</code> from a <code>date</code> column).</li> <li>Enforce Schema: Reorders columns to a predefined, consistent order.</li> <li>Data Validation: Runs a <code>silver_expectations</code> suite with Great Expectations to ensure the output data meets higher quality standards.</li> <li>Save Data:<ul> <li>On Success: Saves the processed DataFrame as a Parquet file to <code>data/silver_data/processed/</code>.</li> <li>On Failure: Saves the failed DataFrame to <code>data/silver_data/quarantined/</code>.</li> </ul> </li> </ol>"},{"location":"Data%20Pipelines/silver_pipeline/#how-to-run","title":"\u25b6\ufe0f How to Run","text":"<p>Using CLI Shortcut:</p> <pre><code>run-silver-pipeline &lt;bronze_file_name.csv&gt;\n</code></pre> <p>Example:</p> <pre><code>run-silver-pipeline train.csv\n</code></pre> <p>Direct Execution:</p> <pre><code>python src/pipelines/silver_pipeline.py &lt;bronze_file_name.csv&gt;\n</code></pre>"},{"location":"Data%20Pipelines/silver_pipeline/#configuration","title":"\u2699\ufe0f Configuration","text":"<p>Similar to the Bronze pipeline, configuration is managed in <code>src/shared/config/config_silver.py</code>.</p> <ul> <li>Paths: Output directories for processed (<code>SILVER_PROCESSED_DIR</code>) and quarantined (<code>SILVER_QUARANTINE_DIR</code>) Parquet files.</li> <li>Schema: The column rename mapping (<code>COLUMN_RENAME_MAPPING</code>), the expected final column order (<code>SILVER_EXPECTED_COLS_ORDER</code>), and expected data types (<code>SILVER_EXPECTED_COLUMN_TYPES</code>).</li> <li>Validation: Column lists for non-null checks and identifying unique records.</li> </ul>"},{"location":"Data%20Pipelines/silver_pipeline/#dependencies-and-environment","title":"\ud83d\udce6 Dependencies and Environment","text":"<ul> <li>Key Libraries: <code>great-expectations</code>, <code>pandas</code>, <code>pyarrow</code>.</li> <li>Input Schema: The pipeline expects a CSV file from the <code>data/bronze_data/processed/</code> directory, conforming to the schema validated by the Bronze pipeline.</li> </ul>"},{"location":"Data%20Pipelines/silver_pipeline/#error-handling-and-quarantining","title":"\ud83d\udc1b Error Handling and Quarantining","text":"<ul> <li>Process: If the transformed DataFrame fails the Silver validation checkpoint, the <code>save_dataframe_based_on_validation</code> function saves the entire failing DataFrame as a Parquet file to the quarantine directory (<code>data/silver_data/quarantined/</code>).</li> <li>Debugging:<ol> <li>Review the pipeline logs and Great Expectations Data Docs for specific failure reasons.</li> <li>Load the quarantined Parquet file into a pandas DataFrame to analyze the data that caused the validation to fail.</li> </ol> </li> <li>Common Failure Reasons:<ul> <li>An upstream change caused a data type to be inferred incorrectly.</li> <li>The cleaning or feature engineering logic introduced null values unexpectedly.</li> <li>Duplicate records were not handled as expected, violating a uniqueness constraint.</li> </ul> </li> </ul>"},{"location":"Data%20Pipelines/silver_pipeline/#next-steps","title":"\u27a1\ufe0f Next Steps","text":"<ul> <li>Gold Pipeline - Feature Engineering, Preprocessing &amp; Validation \u00bb</li> </ul>"},{"location":"EDA/flights_eda/","title":"\ud83e\udded Exploratory Data Analysis (EDA): Flights Dataset","text":"In\u00a0[277]: Copied! <pre>import pandas as pd\n\n%matplotlib inline\n</pre> import pandas as pd  %matplotlib inline In\u00a0[\u00a0]: Copied! <pre># from utils folder import specific functions from data_utils which contains basic data operations\nfrom utils.data_utils import (\n    check_duplicates,\n    # generate_eda_report,\n    get_date_stats,\n    count_rows_between_dates,\n    check_missing,\n    optimize_dtypes,\n    skewness,\n)\n\nfrom utils.plots_utils import (\n    plot_flights_per_year,\n    plot_flights_per_month,\n    histograms,\n    boxplots,\n    barplot_univariate,\n    pairplots,\n    boxplot_bivariate,\n    barplot_bivariate,\n    correlation_heatmap,\n)\n\nfrom utils.data_splitting import chronological_split\n</pre> # from utils folder import specific functions from data_utils which contains basic data operations from utils.data_utils import (     check_duplicates,     # generate_eda_report,     get_date_stats,     count_rows_between_dates,     check_missing,     optimize_dtypes,     skewness, )  from utils.plots_utils import (     plot_flights_per_year,     plot_flights_per_month,     histograms,     boxplots,     barplot_univariate,     pairplots,     boxplot_bivariate,     barplot_bivariate,     correlation_heatmap, )  from utils.data_splitting import chronological_split In\u00a0[279]: Copied! <pre>from shared.config import core_paths\n</pre> from shared.config import core_paths In\u00a0[\u00a0]: Copied! <pre># relative path to the dataset\nflights_path = core_paths.RAW_DATA_DIR / \"flights.csv\"\n# Load the csv dataset\nflights = pd.read_csv(flights_path)\n</pre> # relative path to the dataset flights_path = core_paths.RAW_DATA_DIR / \"flights.csv\" # Load the csv dataset flights = pd.read_csv(flights_path) In\u00a0[281]: Copied! <pre># view first five rows\nflights.head()\n\n# view 5 random samples\n# flights.sample(5)\n</pre> # view first five rows flights.head()  # view 5 random samples # flights.sample(5) Out[281]: travelCode userCode from to flightType price time distance agency date 0 0 0 Recife (PE) Florianopolis (SC) firstClass 1434.38 1.76 676.53 FlyingDrops 09/26/2019 1 0 0 Florianopolis (SC) Recife (PE) firstClass 1292.29 1.76 676.53 FlyingDrops 09/30/2019 2 1 0 Brasilia (DF) Florianopolis (SC) firstClass 1487.52 1.66 637.56 CloudFy 10/03/2019 3 1 0 Florianopolis (SC) Brasilia (DF) firstClass 1127.36 1.66 637.56 CloudFy 10/04/2019 4 2 0 Aracaju (SE) Salvador (BH) firstClass 1684.05 2.16 830.86 CloudFy 10/10/2019 <ul> <li>Dataset loaded to flights variable as a pandas dataframe</li> </ul> In\u00a0[282]: Copied! <pre># check duplicates\ncheck_duplicates(flights)  # takes input dataframe\n</pre> # check duplicates check_duplicates(flights)  # takes input dataframe <pre>Percentage of rows involved in duplication: 0.00%\n</pre> In\u00a0[283]: Copied! <pre>subset_cols = [\n    \"userCode\",\n    \"from\",\n    \"to\",\n    \"flightType\",\n    \"price\",\n    \"time\",\n    \"distance\",\n    \"date\",\n]\nerroneous_duplicate_rows = flights[\n    flights.duplicated(subset=subset_cols, keep=False)\n].sort_values(by=subset_cols)\nerroneous_duplicate_rows\n</pre> subset_cols = [     \"userCode\",     \"from\",     \"to\",     \"flightType\",     \"price\",     \"time\",     \"distance\",     \"date\", ] erroneous_duplicate_rows = flights[     flights.duplicated(subset=subset_cols, keep=False) ].sort_values(by=subset_cols) erroneous_duplicate_rows Out[283]: travelCode userCode from to flightType price time distance agency date In\u00a0[284]: Copied! <pre># Drop primary and foreign keys\nflights.drop(columns=[\"travelCode\", \"userCode\"], inplace=True)\n</pre> # Drop primary and foreign keys flights.drop(columns=[\"travelCode\", \"userCode\"], inplace=True) In\u00a0[285]: Copied! <pre># view the changes\nflights.head()\n</pre> # view the changes flights.head() Out[285]: from to flightType price time distance agency date 0 Recife (PE) Florianopolis (SC) firstClass 1434.38 1.76 676.53 FlyingDrops 09/26/2019 1 Florianopolis (SC) Recife (PE) firstClass 1292.29 1.76 676.53 FlyingDrops 09/30/2019 2 Brasilia (DF) Florianopolis (SC) firstClass 1487.52 1.66 637.56 CloudFy 10/03/2019 3 Florianopolis (SC) Brasilia (DF) firstClass 1127.36 1.66 637.56 CloudFy 10/04/2019 4 Aracaju (SE) Salvador (BH) firstClass 1684.05 2.16 830.86 CloudFy 10/10/2019 In\u00a0[286]: Copied! <pre># function to check percentage of duplicate values\ncheck_duplicates(flights)\n</pre> # function to check percentage of duplicate values check_duplicates(flights) <pre>Percentage of rows involved in duplication: 45.02%\n</pre> In\u00a0[287]: Copied! <pre>subset_cols = [\"from\", \"to\", \"agency\", \"price\"]\nduplicate_rows = flights[\n    flights.duplicated(subset=subset_cols, keep=False)\n].sort_values(by=subset_cols)\n\nduplicate_rows\n</pre> subset_cols = [\"from\", \"to\", \"agency\", \"price\"] duplicate_rows = flights[     flights.duplicated(subset=subset_cols, keep=False) ].sort_values(by=subset_cols)  duplicate_rows Out[287]: from to flightType price time distance agency date 211 Aracaju (SE) Brasilia (DF) economic 472.72 1.11 425.98 CloudFy 12/09/2019 368 Aracaju (SE) Brasilia (DF) economic 472.72 1.11 425.98 CloudFy 06/10/2021 682 Aracaju (SE) Brasilia (DF) economic 472.72 1.11 425.98 CloudFy 12/09/2021 810 Aracaju (SE) Brasilia (DF) economic 472.72 1.11 425.98 CloudFy 03/02/2023 1446 Aracaju (SE) Brasilia (DF) economic 472.72 1.11 425.98 CloudFy 05/28/2020 ... ... ... ... ... ... ... ... ... 177352 Sao Paulo (SP) Salvador (BH) firstClass 1400.35 1.04 401.66 Rainbow 08/11/2022 177662 Sao Paulo (SP) Salvador (BH) firstClass 1400.35 1.04 401.66 Rainbow 12/24/2020 178230 Sao Paulo (SP) Salvador (BH) firstClass 1400.35 1.04 401.66 Rainbow 12/01/2022 179108 Sao Paulo (SP) Salvador (BH) firstClass 1400.35 1.04 401.66 Rainbow 11/14/2019 179282 Sao Paulo (SP) Salvador (BH) firstClass 1400.35 1.04 401.66 Rainbow 07/15/2021 <p>271888 rows \u00d7 8 columns</p> In\u00a0[288]: Copied! <pre>exact_duplicates = flights[flights.duplicated(keep=False)]\n\nall_columns = exact_duplicates.columns.tolist()\n# 4. Rename the new count column to 'occurrence_count'.\ngrouped_duplicates = (\n    exact_duplicates.groupby(all_columns).size().reset_index(name=\"occurrence_count\")\n)\n# Sort the results by the count in descending order.\nsorted_groups = grouped_duplicates.sort_values(by=\"occurrence_count\", ascending=False)\nsorted_groups\n</pre> exact_duplicates = flights[flights.duplicated(keep=False)]  all_columns = exact_duplicates.columns.tolist() # 4. Rename the new count column to 'occurrence_count'. grouped_duplicates = (     exact_duplicates.groupby(all_columns).size().reset_index(name=\"occurrence_count\") ) # Sort the results by the count in descending order. sorted_groups = grouped_duplicates.sort_values(by=\"occurrence_count\", ascending=False) sorted_groups Out[288]: from to flightType price time distance agency date occurrence_count 29847 Florianopolis (SC) Natal (RN) economic 726.95 1.84 709.37 CloudFy 10/03/2019 19 29682 Florianopolis (SC) Campo Grande (MS) premium 714.86 1.49 573.81 Rainbow 10/31/2019 19 32567 Florianopolis (SC) Rio de Janeiro (RJ) economic 317.08 1.21 466.30 Rainbow 10/17/2019 18 30575 Florianopolis (SC) Natal (RN) premium 1114.55 1.84 709.37 Rainbow 01/09/2020 17 26075 Florianopolis (SC) Aracaju (SE) firstClass 1582.10 2.10 808.85 Rainbow 01/23/2020 17 ... ... ... ... ... ... ... ... ... ... 25195 Florianopolis (SC) Aracaju (SE) economic 819.41 2.10 808.85 Rainbow 05/25/2020 2 25196 Florianopolis (SC) Aracaju (SE) economic 819.41 2.10 808.85 Rainbow 05/26/2022 2 25197 Florianopolis (SC) Aracaju (SE) economic 819.41 2.10 808.85 Rainbow 05/27/2021 2 25199 Florianopolis (SC) Aracaju (SE) economic 819.41 2.10 808.85 Rainbow 05/29/2020 2 60593 Sao Paulo (SP) Salvador (BH) premium 1118.61 1.04 401.66 CloudFy 12/31/2020 2 <p>60594 rows \u00d7 9 columns</p> <ul> <li>This suggest many of the duplicates were hidded due to unique identifiers</li> </ul> In\u00a0[289]: Copied! <pre># remove duplicates\nflights.drop_duplicates(inplace=True)\ncheck_duplicates(flights)\n</pre> # remove duplicates flights.drop_duplicates(inplace=True) check_duplicates(flights) <pre>Percentage of rows involved in duplication: 0.00%\n</pre> In\u00a0[290]: Copied! <pre>flights.head()\n</pre> flights.head() Out[290]: from to flightType price time distance agency date 0 Recife (PE) Florianopolis (SC) firstClass 1434.38 1.76 676.53 FlyingDrops 09/26/2019 1 Florianopolis (SC) Recife (PE) firstClass 1292.29 1.76 676.53 FlyingDrops 09/30/2019 2 Brasilia (DF) Florianopolis (SC) firstClass 1487.52 1.66 637.56 CloudFy 10/03/2019 3 Florianopolis (SC) Brasilia (DF) firstClass 1127.36 1.66 637.56 CloudFy 10/04/2019 4 Aracaju (SE) Salvador (BH) firstClass 1684.05 2.16 830.86 CloudFy 10/10/2019 In\u00a0[291]: Copied! <pre># this function generates a minimal basic stats report\n\n# generate_eda_report(\n#     flights,\n#     \"Flights EDA Report\",\n#     \"../reports/eda/flights_eda_report.html\",\n#     minimal=True,\n#     explorative=False,\n# )\n</pre> # this function generates a minimal basic stats report  # generate_eda_report( #     flights, #     \"Flights EDA Report\", #     \"../reports/eda/flights_eda_report.html\", #     minimal=True, #     explorative=False, # ) In\u00a0[292]: Copied! <pre># df info\nflights.info()\n</pre> # df info flights.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 149484 entries, 0 to 271851\nData columns (total 8 columns):\n #   Column      Non-Null Count   Dtype  \n---  ------      --------------   -----  \n 0   from        149484 non-null  object \n 1   to          149484 non-null  object \n 2   flightType  149484 non-null  object \n 3   price       149484 non-null  float64\n 4   time        149484 non-null  float64\n 5   distance    149484 non-null  float64\n 6   agency      149484 non-null  object \n 7   date        149484 non-null  object \ndtypes: float64(3), object(5)\nmemory usage: 10.3+ MB\n</pre> In\u00a0[293]: Copied! <pre># shows number of unique values for each column\nflights.nunique()\n</pre> # shows number of unique values for each column flights.nunique() Out[293]: <pre>from            9\nto              9\nflightType      3\nprice         490\ntime           33\ndistance       35\nagency          3\ndate          999\ndtype: int64</pre> In\u00a0[294]: Copied! <pre>flights[\"date\"] = pd.to_datetime(flights[\"date\"], errors=\"coerce\")\nflights[\"month\"] = flights[\"date\"].dt.month\nflights[\"year\"] = flights[\"date\"].dt.year\nflights[\"day\"] = flights[\"date\"].dt.day\nflights[\"day_name\"] = flights[\"date\"].dt.day_name()\nflights.head()\n</pre> flights[\"date\"] = pd.to_datetime(flights[\"date\"], errors=\"coerce\") flights[\"month\"] = flights[\"date\"].dt.month flights[\"year\"] = flights[\"date\"].dt.year flights[\"day\"] = flights[\"date\"].dt.day flights[\"day_name\"] = flights[\"date\"].dt.day_name() flights.head() Out[294]: from to flightType price time distance agency date month year day day_name 0 Recife (PE) Florianopolis (SC) firstClass 1434.38 1.76 676.53 FlyingDrops 2019-09-26 9 2019 26 Thursday 1 Florianopolis (SC) Recife (PE) firstClass 1292.29 1.76 676.53 FlyingDrops 2019-09-30 9 2019 30 Monday 2 Brasilia (DF) Florianopolis (SC) firstClass 1487.52 1.66 637.56 CloudFy 2019-10-03 10 2019 3 Thursday 3 Florianopolis (SC) Brasilia (DF) firstClass 1127.36 1.66 637.56 CloudFy 2019-10-04 10 2019 4 Friday 4 Aracaju (SE) Salvador (BH) firstClass 1684.05 2.16 830.86 CloudFy 2019-10-10 10 2019 10 Thursday In\u00a0[295]: Copied! <pre>flights.dtypes\n</pre> flights.dtypes Out[295]: <pre>from                  object\nto                    object\nflightType            object\nprice                float64\ntime                 float64\ndistance             float64\nagency                object\ndate          datetime64[ns]\nmonth                  int32\nyear                   int32\nday                    int32\nday_name              object\ndtype: object</pre> In\u00a0[296]: Copied! <pre># optimize_dtypes function automatically based on a criteria converts datatypes to optimal\nflights = optimize_dtypes(flights)\nflights.info()\n</pre> # optimize_dtypes function automatically based on a criteria converts datatypes to optimal flights = optimize_dtypes(flights) flights.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 149484 entries, 0 to 271851\nData columns (total 12 columns):\n #   Column      Non-Null Count   Dtype         \n---  ------      --------------   -----         \n 0   from        149484 non-null  category      \n 1   to          149484 non-null  category      \n 2   flightType  149484 non-null  category      \n 3   price       149484 non-null  float32       \n 4   time        149484 non-null  float32       \n 5   distance    149484 non-null  float32       \n 6   agency      149484 non-null  category      \n 7   date        149484 non-null  datetime64[ns]\n 8   month       149484 non-null  uint8         \n 9   year        149484 non-null  uint16        \n 10  day         149484 non-null  uint8         \n 11  day_name    149484 non-null  category      \ndtypes: category(5), datetime64[ns](1), float32(3), uint16(1), uint8(2)\nmemory usage: 5.3 MB\n</pre> In\u00a0[297]: Copied! <pre># year to category type\nflights[\"year\"] = flights[\"year\"].astype(\"category\")\nflights.info()\n</pre> # year to category type flights[\"year\"] = flights[\"year\"].astype(\"category\") flights.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 149484 entries, 0 to 271851\nData columns (total 12 columns):\n #   Column      Non-Null Count   Dtype         \n---  ------      --------------   -----         \n 0   from        149484 non-null  category      \n 1   to          149484 non-null  category      \n 2   flightType  149484 non-null  category      \n 3   price       149484 non-null  float32       \n 4   time        149484 non-null  float32       \n 5   distance    149484 non-null  float32       \n 6   agency      149484 non-null  category      \n 7   date        149484 non-null  datetime64[ns]\n 8   month       149484 non-null  uint8         \n 9   year        149484 non-null  category      \n 10  day         149484 non-null  uint8         \n 11  day_name    149484 non-null  category      \ndtypes: category(6), datetime64[ns](1), float32(3), uint8(2)\nmemory usage: 5.1 MB\n</pre> In\u00a0[298]: Copied! <pre>flights.sample(5)\n</pre> flights.sample(5) Out[298]: from to flightType price time distance agency date month year day day_name 157809 Recife (PE) Sao Paulo (SP) firstClass 980.830017 1.26 486.519989 Rainbow 2020-09-27 9 2020 27 Sunday 63494 Recife (PE) Aracaju (SE) economic 684.880005 1.44 555.739990 CloudFy 2022-06-02 6 2022 2 Thursday 102249 Natal (RN) Campo Grande (MS) firstClass 855.820007 0.65 250.679993 Rainbow 2019-12-13 12 2019 13 Friday 70456 Aracaju (SE) Salvador (BH) premium 1289.050049 2.16 830.859985 Rainbow 2022-07-28 7 2022 28 Thursday 226585 Recife (PE) Florianopolis (SC) firstClass 1434.380005 1.76 676.530029 FlyingDrops 2019-10-26 10 2019 26 Saturday In\u00a0[299]: Copied! <pre># descibe basic stats numerical columns\nflights.describe(include=\"number\")\n</pre> # descibe basic stats numerical columns flights.describe(include=\"number\") Out[299]: price time distance month day count 149484.000000 149484.000000 149484.000000 149484.000000 149484.000000 mean 949.422363 1.363235 524.676514 6.579473 15.757258 std 358.446228 0.547743 210.998199 3.587578 8.817627 min 301.510010 0.440000 168.220001 1.000000 1.000000 25% 669.830017 0.850000 327.549988 3.000000 8.000000 50% 898.039978 1.440000 555.739990 7.000000 16.000000 75% 1214.819946 1.760000 676.530029 10.000000 23.000000 max 1754.170044 2.440000 937.770020 12.000000 31.000000 In\u00a0[300]: Copied! <pre># descibe basic stats categorical  columns\nflights.describe(include=[\"object\", \"category\"])\n</pre> # descibe basic stats categorical  columns flights.describe(include=[\"object\", \"category\"]) Out[300]: from to flightType agency year day_name count 149484 149484 149484 149484 149484 149484 unique 9 9 3 3 5 5 top Florianopolis (SC) Florianopolis (SC) firstClass Rainbow 2020 Thursday freq 18800 31438 64169 64167 55095 52115 In\u00a0[301]: Copied! <pre># check datatypes for new columns and optimize them\n# flights.info()\nflights = optimize_dtypes(flights)\nflights.info()\n</pre> # check datatypes for new columns and optimize them # flights.info() flights = optimize_dtypes(flights) flights.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 149484 entries, 0 to 271851\nData columns (total 12 columns):\n #   Column      Non-Null Count   Dtype         \n---  ------      --------------   -----         \n 0   from        149484 non-null  category      \n 1   to          149484 non-null  category      \n 2   flightType  149484 non-null  category      \n 3   price       149484 non-null  float32       \n 4   time        149484 non-null  float32       \n 5   distance    149484 non-null  float32       \n 6   agency      149484 non-null  category      \n 7   date        149484 non-null  datetime64[ns]\n 8   month       149484 non-null  uint8         \n 9   year        149484 non-null  category      \n 10  day         149484 non-null  uint8         \n 11  day_name    149484 non-null  category      \ndtypes: category(6), datetime64[ns](1), float32(3), uint8(2)\nmemory usage: 5.1 MB\n</pre> In\u00a0[302]: Copied! <pre># shows min, max, vlaue counts for year and month\nget_date_stats(flights[\"date\"])\n</pre> # shows min, max, vlaue counts for year and month get_date_stats(flights[\"date\"]) <pre>--- Date Stats for: Date Column ---\nMin date: 2019-09-26\nMax date: 2023-07-24\nTime span: 1397 days 00:00:00\nUnique days: 999\n\nYear counts:\n date\n2019    16153\n2020    55095\n2021    43792\n2022    28912\n2023     5532\nName: count, dtype: int64\n\nMonth counts:\n date\n1     13903\n2     12335\n3     12651\n4     12330\n5     11916\n6     10743\n7     11137\n8     10392\n9     10792\n10    15018\n11    13965\n12    14302\nName: count, dtype: int64\n\nUnique year-months: 47\n--- End of Stats ---\n</pre> In\u00a0[303]: Copied! <pre>flights.groupby([\"month\", \"year\"], observed=False).aggregate({\"price\": \"mean\"}).rename(\n    {\"price\": \"average price\"}\n)\n</pre> flights.groupby([\"month\", \"year\"], observed=False).aggregate({\"price\": \"mean\"}).rename(     {\"price\": \"average price\"} ) Out[303]: price month year 1 2019 NaN 2020 943.285767 2021 941.015503 2022 950.483154 2023 945.310974 2 2019 NaN 2020 943.707764 2021 943.018982 2022 956.953369 2023 977.811890 3 2019 NaN 2020 948.160339 2021 948.967957 2022 955.758118 2023 954.138245 4 2019 NaN 2020 939.292236 2021 950.007263 2022 949.714417 2023 958.361877 5 2019 NaN 2020 947.484863 2021 947.645386 2022 953.654480 2023 961.209900 6 2019 NaN 2020 941.442932 2021 948.829590 2022 969.782166 2023 964.049438 7 2019 NaN 2020 943.606262 2021 940.923401 2022 978.065735 2023 1040.593140 8 2019 NaN 2020 945.804871 2021 952.486450 2022 962.183838 2023 NaN 9 2019 939.672791 2020 945.418945 2021 952.704468 2022 956.616699 2023 NaN 10 2019 941.447571 2020 947.688049 2021 960.673828 2022 970.662170 2023 NaN 11 2019 947.524658 2020 945.929871 2021 959.591492 2022 957.162659 2023 NaN 12 2019 945.182373 2020 942.819702 2021 950.159363 2022 960.838989 2023 NaN In\u00a0[304]: Copied! <pre># bar plots\nplot_flights_per_year(flights[\"date\"])\nplot_flights_per_month(flights[\"date\"])\n</pre> # bar plots plot_flights_per_year(flights[\"date\"]) plot_flights_per_month(flights[\"date\"]) In\u00a0[305]: Copied! <pre># Min date: 2019-09-26\n# Max date: 2023-07-24\nprint(count_rows_between_dates(flights, \"date\", \"2019-09-26\", \"2023-07-24\"))\n# view split for training set keeping in mind the seasonality of the data\nprint(count_rows_between_dates(flights, \"date\", \"2019-09-26\", \"2021-05-22\"))\n# view split for validation set\nprint(count_rows_between_dates(flights, \"date\", \"2021-05-22\", \"2021-12-05\"))\n# view split for test set\nprint(count_rows_between_dates(flights, \"date\", \"2021-12-05\", \"2022-08-10\"))\n# view split for production simulation\nprint(count_rows_between_dates(flights, \"date\", \"2022-08-10\", \"2023-07-24\"))\n</pre> # Min date: 2019-09-26 # Max date: 2023-07-24 print(count_rows_between_dates(flights, \"date\", \"2019-09-26\", \"2023-07-24\")) # view split for training set keeping in mind the seasonality of the data print(count_rows_between_dates(flights, \"date\", \"2019-09-26\", \"2021-05-22\")) # view split for validation set print(count_rows_between_dates(flights, \"date\", \"2021-05-22\", \"2021-12-05\")) # view split for test set print(count_rows_between_dates(flights, \"date\", \"2021-12-05\", \"2022-08-10\")) # view split for production simulation print(count_rows_between_dates(flights, \"date\", \"2022-08-10\", \"2023-07-24\")) <pre>Rows between 2019-09-26 and 2023-07-24: 149484 (100.00% of total)\n149484\nRows between 2019-09-26 and 2021-05-22: 89935 (60.16% of total)\n89935\nRows between 2021-05-22 and 2021-12-05: 22568 (15.10% of total)\n22568\nRows between 2021-12-05 and 2022-08-10: 22556 (15.09% of total)\n22556\nRows between 2022-08-10 and 2023-07-24: 14675 (9.82% of total)\n14675\n</pre> In\u00a0[306]: Copied! <pre># prints missing values in each column and %age\ncheck_missing(flights)\n</pre> # prints missing values in each column and %age check_missing(flights) Out[306]: Missing Values Percentage from 0 0.0 to 0 0.0 flightType 0 0.0 price 0 0.0 time 0 0.0 distance 0 0.0 agency 0 0.0 date 0 0.0 month 0 0.0 year 0 0.0 day 0 0.0 day_name 0 0.0 In\u00a0[307]: Copied! <pre>all_sets_dict = chronological_split(\n    flights,\n    \"date\",  # date column\n    \"price\",  # target variable\n    \"2021-05-22\",  # train_end\n    \"2021-12-05\",  # val_end\n    \"2022-08-10\",  # test_end\n)\n\n# Now unpack each tuple from the dictionary\ntrain_x, train_y = all_sets_dict[\"train\"]\nvalidation_x, validation_y = all_sets_dict[\"validation\"]\ntest_x, test_y = all_sets_dict[\"test\"]\n\n# For the 'holdout' set:\nholdout_x, holdout_y = all_sets_dict[\"holdout\"]\n</pre> all_sets_dict = chronological_split(     flights,     \"date\",  # date column     \"price\",  # target variable     \"2021-05-22\",  # train_end     \"2021-12-05\",  # val_end     \"2022-08-10\",  # test_end )  # Now unpack each tuple from the dictionary train_x, train_y = all_sets_dict[\"train\"] validation_x, validation_y = all_sets_dict[\"validation\"] test_x, test_y = all_sets_dict[\"test\"]  # For the 'holdout' set: holdout_x, holdout_y = all_sets_dict[\"holdout\"] In\u00a0[308]: Copied! <pre># Add price column back and make a new df called flights_train\nflights_train = train_x.copy()\nflights_train[\"price\"] = train_y\nflights_train.sample(5)\n</pre> # Add price column back and make a new df called flights_train flights_train = train_x.copy() flights_train[\"price\"] = train_y flights_train.sample(5) Out[308]: from to flightType time distance agency date month year day day_name price 81429 Brasilia (DF) Campo Grande (MS) economic 0.72 277.700012 CloudFy 2021-03-18 3 2021 18 Thursday 583.599976 22365 Aracaju (SE) Campo Grande (MS) premium 1.69 650.099976 Rainbow 2020-02-08 2 2020 8 Saturday 1116.829956 79034 Rio de Janeiro (RJ) Aracaju (SE) premium 1.55 597.609985 Rainbow 2021-02-27 2 2021 27 Saturday 1098.930054 48348 Aracaju (SE) Natal (RN) firstClass 0.46 176.330002 FlyingDrops 2020-07-25 7 2020 25 Saturday 577.710022 28669 Recife (PE) Salvador (BH) firstClass 2.05 788.549988 CloudFy 2020-03-19 3 2020 19 Thursday 1484.939941 In\u00a0[309]: Copied! <pre># generate_eda_report(\n#     flights_train,\n#     \"Flights Training set EDA Report\",\n#     \"../reports/eda/flights_training_eda_report.html\",\n#     minimal=False,\n#     explorative=True,\n# )\n</pre> # generate_eda_report( #     flights_train, #     \"Flights Training set EDA Report\", #     \"../reports/eda/flights_training_eda_report.html\", #     minimal=False, #     explorative=True, # ) In\u00a0[310]: Copied! <pre>flights_train.describe(include=\"number\")\n</pre> flights_train.describe(include=\"number\") Out[310]: time distance month day price count 89935.000000 89935.000000 89935.000000 89935.000000 89935.000000 mean 1.350871 519.917725 6.426597 15.760327 944.785095 std 0.548463 211.149643 3.745957 8.800451 357.462952 min 0.440000 168.220001 1.000000 1.000000 301.510010 25% 0.850000 327.549988 3.000000 8.000000 667.000000 50% 1.440000 555.739990 6.000000 16.000000 889.070007 75% 1.760000 676.530029 10.000000 23.000000 1209.040039 max 2.440000 937.770020 12.000000 31.000000 1754.170044 In\u00a0[311]: Copied! <pre>flights_train.describe(include=\"category\")\n</pre> flights_train.describe(include=\"category\") Out[311]: from to flightType agency year day_name count 89935 89935 89935 89935 89935 89935 unique 9 9 3 3 3 5 top Campo Grande (MS) Florianopolis (SC) firstClass Rainbow 2020 Thursday freq 11020 18337 38498 38557 55095 29104 In\u00a0[312]: Copied! <pre>flights_train.describe(include=\"datetime\")\n</pre> flights_train.describe(include=\"datetime\") Out[312]: date count 89935 mean 2020-07-09 01:49:17.696113920 min 2019-09-26 00:00:00 25% 2020-02-08 00:00:00 50% 2020-07-02 00:00:00 75% 2020-12-04 00:00:00 max 2021-05-22 00:00:00 In\u00a0[313]: Copied! <pre># list of numerical columns in flights_train\nflights_train_num = flights_train.select_dtypes(include=[\"number\"]).columns.tolist()\nflights_train_num\n</pre> # list of numerical columns in flights_train flights_train_num = flights_train.select_dtypes(include=[\"number\"]).columns.tolist() flights_train_num Out[313]: <pre>['time', 'distance', 'month', 'day', 'price']</pre> In\u00a0[314]: Copied! <pre># calculates the skew for each column\nskewness(flights_train[flights_train_num])\n</pre> # calculates the skew for each column skewness(flights_train[flights_train_num]) Out[314]: Column Skew 0 time -0.055489 1 distance -0.059037 2 month 0.065595 3 day 0.007979 4 price 0.353881 In\u00a0[315]: Copied! <pre># plots histograms\nhistograms(flights_train, flights_train_num)\n</pre> # plots histograms histograms(flights_train, flights_train_num) In\u00a0[316]: Copied! <pre>boxplots(flights_train, flights_train_num)\n</pre> boxplots(flights_train, flights_train_num) In\u00a0[317]: Copied! <pre>flights_train_cat = flights_train.select_dtypes(include=\"category\").columns.to_list()\nflights_train_cat\n</pre> flights_train_cat = flights_train.select_dtypes(include=\"category\").columns.to_list() flights_train_cat Out[317]: <pre>['from', 'to', 'flightType', 'agency', 'year', 'day_name']</pre> In\u00a0[318]: Copied! <pre>barplot_univariate(flights_train, flights_train_cat)\n</pre> barplot_univariate(flights_train, flights_train_cat) In\u00a0[319]: Copied! <pre>pairplots(flights_train[flights_train_num])\n</pre> pairplots(flights_train[flights_train_num]) In\u00a0[320]: Copied! <pre>corr = flights_train[flights_train_num].corr()\ncorr\n</pre> corr = flights_train[flights_train_num].corr() corr Out[320]: time distance month day price time 1.000000 0.999991 0.000075 -0.002326 0.653463 distance 0.999991 1.000000 0.000079 -0.002339 0.653665 month 0.000075 0.000079 1.000000 0.032190 0.000804 day -0.002326 -0.002339 0.032190 1.000000 0.000453 price 0.653463 0.653665 0.000804 0.000453 1.000000 In\u00a0[321]: Copied! <pre>correlation_heatmap(flights_train, flights_train_num)\n</pre> correlation_heatmap(flights_train, flights_train_num) In\u00a0[322]: Copied! <pre>boxplot_bivariate(flights_train, flights_train_cat, [\"time\", \"distance\", \"price\"])\n</pre> boxplot_bivariate(flights_train, flights_train_cat, [\"time\", \"distance\", \"price\"]) In\u00a0[323]: Copied! <pre># can change aggregation to mean, mode ,median and sum as well\nbarplot_bivariate(\n    flights_train,\n    flights_train_cat,\n    [\"time\", \"distance\", \"price\"],\n    \"sum\",\n)\n</pre> # can change aggregation to mean, mode ,median and sum as well barplot_bivariate(     flights_train,     flights_train_cat,     [\"time\", \"distance\", \"price\"],     \"sum\", )"},{"location":"EDA/flights_eda/#exploratory-data-analysis-eda-flights-dataset","title":"\ud83e\udded Exploratory Data Analysis (EDA): Flights Dataset\u00b6","text":"<p>This notebook documents the Exploratory Data Analysis (EDA) for the Flights dataset, a cornerstone of our Flights MLOps Project. Here, we will dissect the data to uncover critical insights, identify quality issues, and lay a robust foundation for building our flight price prediction model.</p>"},{"location":"EDA/flights_eda/#business-objective","title":"\ud83c\udfaf Business Objective\u00b6","text":"<p>To develop and integrate a machine learning-powered flight price prediction tool that provides travelers with real-time, data-driven estimates for future airfare on specific routes. This initiative aims to enhance user engagement and trust by empowering them to make informed booking decisions, identify optimal travel times, and ultimately secure the best possible value. By delivering accurate and transparent fare forecasts, we will position our platform as an indispensable tool for savvy travelers, driving user loyalty and increasing booking conversions.</p>"},{"location":"EDA/flights_eda/#scope-of-this-analysis","title":"\ud83d\udd2c Scope of this Analysis\u00b6","text":"<p>The primary goal of this EDA is to deeply understand the dataset to inform the development of a high-performing Regression Model for predicting flight prices.</p> <p>It's important to note that our data processing decisions are strictly optimized for this modeling objective. For instance, handling duplicate records is tailored to build an unbiased model. Consequently, the resulting dataset may not be suitable for direct business intelligence questions (e.g., calculating total revenue), where every transaction is vital. Our priority is predictive accuracy, not transactional accounting.</p>"},{"location":"EDA/flights_eda/#dataset-at-a-glance-flights","title":"\ud83d\udce6 Dataset At a Glance: Flights\u00b6","text":"<p>The dataset contains structured records of flight bookings, encompassing travel routes, flight characteristics, pricing, and booking agencies.</p> Column Description <code>travelCode</code> Unique identifier for each travel itinerary <code>userCode</code> Identifier for the user (links to a Users table) <code>from</code> Origin city or airport of the flight <code>to</code> Destination city or airport of the flight <code>flightType</code> Service class of the flight (e.g., Economy, First Class) <code>price</code> Target Variable: Cost of the flight (USD) <code>time</code> Flight duration in hours <code>distance</code> Flight distance in kilometers <code>agency</code> Airline or travel agency that facilitated the booking <code>date</code> Departure date of the flight"},{"location":"EDA/flights_eda/#key-objectives","title":"\ud83c\udfaf Key Objectives\u00b6","text":"<p>This analysis serves several critical functions in our model development lifecycle:</p> <ul> <li>Understand Data Structure: Thoroughly examine the schema, data types, and the statistical distributions of all features.</li> <li>Identify Data Quality Issues: Pinpoint and quantify problems like missing values, outliers, anomalies, and duplicate records.</li> <li>Evaluate Feature Relationships: Analyze how each feature correlates with the target variable (<code>price</code>) and with other features to identify potential predictors.</li> <li>Inform Feature Engineering: Use the insights gained to guide data cleaning strategies, transformations, and the creation of new, impactful features for the model.</li> </ul>"},{"location":"EDA/flights_eda/#analytical-approach","title":"\ud83d\udd0d Analytical Approach\u00b6","text":"<p>Our exploration will involve a comprehensive analysis using descriptive statistics, data visualizations (histograms, scatter plots, box plots), and correlation studies. The findings are crucial as they will directly inform our feature selection, data transformation techniques, and the overall design of the model training pipeline.</p> <p>Given the manageable number of features, we will employ a thorough visualization strategy. This includes generating pair plots for numerical features, frequency plots for categorical features, and comparative plots (like box plots) to understand inter-feature relationships and patterns comprehensively.</p> <p>Let's begin!</p>"},{"location":"EDA/flights_eda/#executive-summary-eda-findings-modeling-blueprint","title":"Executive Summary: EDA Findings &amp; Modeling Blueprint \ud83d\ude80\u00b6","text":"<p>Our Exploratory Data Analysis of the Flights dataset reveals a clean, high-quality foundation for modeling after significant deduplication (45% of records removed). The data shows no missing values, and memory usage was halved through optimization.</p> <p>The strongest predictors for flight <code>price</code> are <code>flightType</code>, Route (<code>from</code>/<code>to</code>) and <code>agency</code>. Furthermore, strong temporal patterns exist, with distinct price and volume seasonality by month and day of the week. The target variable, <code>price</code>, is moderately right-skewed (skewness ~0.53), and <code>distance</code> and <code>time</code> are almost perfectly correlated.</p> <p>Core Recommendation: Develop a tree-based regression model (e.g., Gradient Boosting) using a strict time-based data split. Key preprocessing steps may include transformation like  log-transforming the <code>price</code>, using only one of <code>distance</code> or <code>time</code> to avoid multicollinearity, and engineering features to capture temporal dynamics (e.g., cyclical encoding for month).</p>"},{"location":"EDA/flights_eda/#1-data-quality-preparation","title":"1. \ud83d\uddc2\ufe0f Data Quality &amp; Preparation\u00b6","text":"<ul> <li>Integrity: The initial dataset of 271,888 records was reduced to 149,484 unique flight instances after removing duplicates based on core flight attributes.This is due to the fact that multiple people on the same date may have booked the same flight on same date. This is crucial to prevent the model from being biased toward common routes. The deduplicated dataset has zero missing values.</li> <li>Feature Set: The model will use 8 core features: <code>from</code>, <code>to</code>, <code>flightType</code>, <code>price</code>, <code>time</code>, <code>distance</code>, <code>agency</code>, and <code>date</code>.</li> <li>Memory Optimization: Strategic downcasting of data types reduced memory usage from 10.3 MB to 5.1 MB, ensuring better performance in production.</li> </ul>"},{"location":"EDA/flights_eda/#2-key-feature-insights","title":"2. \ud83c\udfaf Key Feature Insights\u00b6","text":"<ul> <li><p>Target Variable (<code>price</code>):</p> <ul> <li>Distribution: Moderately right-skewed (mean $949, median $898).</li> <li>Action: Apply a log transformation (<code>np.log1p</code>) or Other transformations like box-cox, yeo-johnson to normalize the distribution and stabilize variance, which is critical for model performance.</li> </ul> </li> <li><p>Primary Predictive Features:</p> <ul> <li><code>flightType</code>: The strongest price differentiator. Mean prices follow a clear hierarchy: <code>firstClass</code> &gt; <code>premium</code> &gt; <code>economic</code>.</li> <li><code>agency</code>: Significant price variation. 'FlyingDrops' has the highest average price but lowest market share, suggesting a niche, high-margin strategy.</li> <li>Route (<code>from</code>/<code>to</code>): Flights involving Salvador (BH) and Florianopolis (SC) are consistently the most expensive routes.</li> </ul> </li> <li><p>Temporal Trends (<code>date</code>):</p> <ul> <li>Seasonality: Prices peak during summer months (June-August) and year-end (December).</li> <li>Weekly Pattern: Prices tend to be higher on Fridays and Sundays. Flight volume is highest on Thursdays.</li> <li>Action: Engineer features like <code>month</code>, <code>day_of_week</code>, and <code>is_weekend</code>. Use cyclical encoding for <code>month</code> and <code>day</code> to help the model understand their cyclical nature.</li> </ul> </li> <li><p>Numerical Features (<code>distance</code>, <code>time</code>):</p> <ul> <li>Multicollinearity: These two features are almost perfectly correlated (~1.0).</li> <li>Action: Use only one of these features (e.g., <code>distance</code>) in the model to avoid multicollinearity.</li> </ul> </li> </ul>"},{"location":"EDA/flights_eda/#3-modeling-pipeline-recommendations","title":"3. \ud83e\udd16 Modeling Pipeline Recommendations\u00b6","text":"Stage Strategy Rationale Data Splitting Strict Chronological Split Prevents temporal data leakage, ensuring model is validated on \"future\" data. Preprocessing Log Transform <code>price</code> Normalize the skewed target variable. Cyclical Encode <code>month</code>, <code>day</code> Help the model learn seasonal and weekly patterns. One-Hot Encode low-cardinality features Handle <code>flightType</code> and <code>agency</code>. Target Encode high-cardinality features Handle <code>route</code> (with cross-validation to prevent leakage). Feature Selection Drop <code>time</code> (or <code>distance</code>) Address severe multicollinearity. Model Choice Gradient Boosting or Random Forest Robust to outliers and effective at capturing complex, non-linear relationships. Evaluation Primary: RMSE/MAE; Secondary: R\u00b2 Provide an absolute error in dollars and explain variance."},{"location":"EDA/flights_eda/#import-necessary-libraries-and-packages","title":"Import Necessary Libraries and Packages\u00b6","text":""},{"location":"EDA/flights_eda/#load-the-dataset","title":"Load The Dataset\u00b6","text":""},{"location":"EDA/flights_eda/#checking-duplicates","title":"Checking Duplicates\u00b6","text":""},{"location":"EDA/flights_eda/#check-for-erroneous-duplicates","title":"check for erroneous duplicates\u00b6","text":"<ul> <li>we need check duplicates with same userCode, date and other features which implies that a user travel entry on same date should not be present multiple times.</li> </ul>"},{"location":"EDA/flights_eda/#duplicate-values-insights","title":"Duplicate Values Insights\u00b6","text":"<ul> <li>No Duplicate values were found if we use travelCode and userCode as unique idetifiers.</li> <li>No erroneous duplicates found were same userCode would have a duplicate entry on the same date.</li> </ul>"},{"location":"EDA/flights_eda/#relevance-of-travelcode-and-usercode-in-eda","title":"Relevance of <code>travelCode</code> and <code>userCode</code> in EDA\u00b6","text":"<p>The <code>travelCode</code> and <code>userCode</code> columns primarily serve as identifiers. They are important for data analysis to gather business insights accurately, separating similar records for different user for same date.</p> <ul> <li><p><code>travelCode</code>: This links related flight segments, such as an outbound and its corresponding return trip. For example:</p> <p>| travelCode | userCode | from             | to                 | date       | |------------|----------|------------------|--------------------|------------| | 0          | 0        | Recife (PE)      | Florianopolis (SC) | 09/26/2019 | | 0          | 0        | Florianopolis (SC)| Recife (PE)        | 09/30/2019 |</p> </li> <li><p><code>userCode</code>: This links flights to specific users, enabling connections to the <code>users.csv</code> data.</p> </li> <li><p>It's possible for different <code>travelCode</code> or <code>userCode</code> entries to correspond to flights with identical details (route, price, time, distance, agency,date). This can happen if multiple users book the same flight. Including these identifiers directly in analyses focused on flight attributes without aggregation could potentially cause Bias in a model by over-representing frequently booked flight configurations.</p> </li> </ul>"},{"location":"EDA/flights_eda/#checking-and-handling-duplicate-values-after-removing-unique-identifierstravelcode-and-usercode","title":"Checking And Handling duplicate values after removing unique identifiers(<code>travelCode</code> and <code>userCode</code>)\u00b6","text":""},{"location":"EDA/flights_eda/#addressing-duplicate-flight-records","title":"\u2708\ufe0f Addressing Duplicate Flight Records\u00b6","text":"<p>Our analysis revealed a significant number of duplicate entries within the dataset after removal of unique identifiers:</p> <ul> <li><p>Substantial Duplicate Presence: Approximately 45.02% of the flight records were identified as duplicates. This high percentage is attributed to multiple unique <code>travelCode</code> and <code>userCode</code> entries corresponding to identical flight details (route, timing, price, etc.), likely representing different bookings for the same underlying flight service on same date by different users.</p> </li> <li><p>Critical Preprocessing Step: Duplicate Removal:</p> <ul> <li>Although for data analysis to accurately answer business questions like total number of sales etc its very important to not remove these duplicate entries as they are from a different user however for model building and doing eda specifically for eda we have to remove these duplicates carrying same information.</li> <li>Rows with different date even if rest of the features are same will be preserved as they shows time trend with price. \ud83d\udeae It is crucial to remove these duplicate records before proceeding with further analysis or model training.</li> <li>Impact on Analysis: Retaining duplicates would skew descriptive statistics and visualizations, leading to inaccurate interpretations of flight patterns and pricing.</li> <li>Impact on Modeling: For machine learning, these duplicates can introduce significant bias, leading to an overestimation of model performance on seen data and poor generalization to new, unseen flight queries. Removing them ensures model integrity and more reliable predictions.</li> </ul> </li> </ul>"},{"location":"EDA/flights_eda/#automated-basic-stats-report-with-ydata-profiling","title":"\ud83d\udcd1 Automated Basic Stats Report with <code>ydata-profiling</code>\u00b6","text":"<ul> <li>This will calculate basic stats about flights dataset and wont perform any extensive eda which may cause data leakage and bias.</li> </ul>"},{"location":"EDA/flights_eda/#column-information-feature-creation-and-datatype-opimization","title":"Column Information, Feature creation and DataType Opimization\u00b6","text":""},{"location":"EDA/flights_eda/#breaking-date-column","title":"Breaking Date Column\u00b6","text":"<ul> <li>Extracting Year, Month , Day and Day name from Date column</li> </ul>"},{"location":"EDA/flights_eda/#optimize-dataypes","title":"Optimize Dataypes\u00b6","text":""},{"location":"EDA/flights_eda/#column-information-and-data-type-optimization","title":"Column Information and Data Type Optimization\u00b6","text":"<p>Effective data management is crucial for efficient analysis and model training. This section details the steps taken to inspect the dataset's structure, understand its column types, and optimize these types for better memory usage and computational performance.</p>"},{"location":"EDA/flights_eda/#initial-data-structure","title":"Initial Data Structure\u00b6","text":"<p>Upon loading and initial cleaning (including duplicate removal), the dataset comprised 149,484 flight records. A preliminary check using <code>flights.info()</code> revealed the following data types:</p> <ul> <li>Object Types: <code>from</code>, <code>to</code>, <code>flightType</code>, <code>agency</code>, <code>date</code>. These are typically strings and require careful handling.</li> <li>Float64 Types: <code>price</code>, <code>time</code>, <code>distance</code>. These are standard 64-bit floating-point numbers.</li> </ul> <p>The <code>flights.nunique()</code> command was used to assess the number of unique values in each column, providing an early indication of which columns might be suitable for conversion to more memory-efficient types like <code>category</code>.</p>"},{"location":"EDA/flights_eda/#date-feature-engineering-and-standardization","title":"Date Feature Engineering and Standardization\u00b6","text":"<p>The <code>date</code> column, initially an object type, is vital for time-series analysis and understanding temporal patterns. The following transformations were applied:</p> <ul> <li>Conversion to Datetime: The <code>date</code> column was converted to Pandas' <code>datetime64[ns]</code> format using <code>pd.to_datetime(flights[\"date\"], errors=\"coerce\")</code>. This standardization is essential for performing date-specific operations.</li> <li>Feature Extraction: To leverage temporal information more directly, new columns were extracted from the <code>date</code> column:<ul> <li><code>month</code>: The month of the flight (e.g., 1 for January, 12 for December).</li> <li><code>year</code>: The year of the flight.</li> <li><code>day</code>: The day of the month.</li> <li><code>day_name</code>: The name of the day of the week (e.g., Monday, Tuesday).</li> </ul> </li> </ul> <p>These new features allow for more granular analysis of trends related to specific times of the year or days of the week.</p>"},{"location":"EDA/flights_eda/#data-type-optimization-strategy","title":"Data Type Optimization Strategy\u00b6","text":"<p>To enhance performance and reduce the memory footprint of the dataset, the following data type optimizations were implemented using a custom <code>optimize_dtypes</code> function and specific column conversions:</p> <ul> <li><p>Categorical Conversion for Low Cardinality Features:</p> <ul> <li>Columns such as <code>from</code>, <code>to</code>, <code>flightType</code>, <code>agency</code>, and the newly created <code>year</code> were identified as having a relatively small number of unique values (low cardinality).</li> <li>These were converted from <code>object</code> or numerical types to the <code>category</code> dtype. This is highly beneficial as Pandas stores <code>category</code> data more efficiently by using integer codes internally, significantly reducing memory usage for repetitive string data.</li> </ul> </li> <li><p>Downcasting Numerical Features:</p> <ul> <li>The numerical columns <code>price</code>, <code>time</code>, and <code>distance</code>, initially <code>float64</code>, were downcast to <code>float32</code>. This conversion reduces the memory required for each floating-point number by half, often without a practical loss of precision for the type of data represented (e.g., flight prices, durations, distances).</li> </ul> </li> <li><p>Optimizing Extracted Date Components:</p> <ul> <li>The newly created <code>month</code> and <code>day</code> columns, representing integer values within a small range, were optimized to <code>uint8</code> (unsigned 8-bit integer). However for Modeling month and day can be converted to  <code>cyclic</code>.</li> <li>The <code>year</code> column, before its conversion to <code>category</code>, was suitable for <code>uint16</code> (unsigned 16-bit integer).</li> </ul> </li> </ul>"},{"location":"EDA/flights_eda/#impact-of-optimization","title":"Impact of Optimization\u00b6","text":"<p>The combined effect of these data type optimizations was substantial:</p> <ul> <li>Memory Reduction: The dataset's memory usage was significantly reduced. Initially, after basic loading and cleaning, the memory footprint was over 10.3 MB. After the optimization process, this was brought down to approximately 5.1 MB.</li> <li>Performance Gains: Smaller data types lead to faster processing for many Pandas operations and can accelerate the training of machine learning models.</li> </ul> <p>By thoughtfully optimizing data types, we ensure that the dataset is not only memory-efficient but also primed for faster and more effective subsequent analytical and modeling tasks. This step is a best practice in any data science workflow, particularly when dealing with larger datasets.</p>"},{"location":"EDA/flights_eda/#5-number-summary","title":"5 Number Summary\u00b6","text":""},{"location":"EDA/flights_eda/#descriptive-statistics-the-five-number-summary-and-categorical-overview","title":"Descriptive Statistics: The Five-Number Summary and Categorical Overview\u00b6","text":"<p>To gain a foundational understanding of the dataset's characteristics, descriptive statistics were computed for both numerical and categorical features. This includes the five-number summary (minimum, first quartile (Q1), median (Q2), third quartile (Q3), and maximum) for numerical variables, and frequency distributions for categorical variables.</p>"},{"location":"EDA/flights_eda/#numerical-feature-summary","title":"Numerical Feature Summary\u00b6","text":"<p>The <code>flights.describe(include=\"number\")</code> method provided key statistical measures for the numerical columns: <code>price</code>, <code>time</code>, <code>distance</code>, <code>month</code>, and <code>day</code>.</p> <ul> <li><p><code>price</code> (Target Variable):</p> <ul> <li>Mean: Approximately 949.42 currency units.</li> <li>Standard Deviation: Approximately 358.45, indicating a moderate spread in prices.</li> <li>Minimum: 301.51 currency units.</li> <li>25th Percentile (Q1): 669.83 currency units.</li> <li>Median (50th Percentile): 898.04 currency units. The median being slightly lower than the mean suggests a mild positive (right) skew in the price distribution, implying a larger number of flights at lower price points with a tail of higher-priced flights.</li> <li>75th Percentile (Q3): 1214.82 currency units. The interquartile range (IQR = Q3 - Q1) is approximately 544.99.</li> <li>Maximum: 1754.17 currency units.</li> <li>Interpretation: Flight prices exhibit a considerable range, with most falling within the IQR. The skewness indicates that while the bulk of prices are below 1000 units, there are instances of significantly higher prices.</li> </ul> </li> <li><p><code>time</code> (Flight Duration in Hours):</p> <ul> <li>Mean: Approximately 1.36 hours.</li> <li>Standard Deviation: Approximately 0.55 hours, reflecting moderate variability in flight durations.</li> <li>Minimum: 0.44 hours (approximately 26 minutes).</li> <li>Median: 1.44 hours. The median is very close to the mean, suggesting a relatively symmetric distribution of flight times.</li> <li>Maximum: 2.44 hours.</li> <li>Interpretation: Flight durations are generally clustered around the mean, with a limited range from short hops to medium-length flights.</li> </ul> </li> <li><p><code>distance</code> (Flight Distance):</p> <ul> <li>Mean: Approximately 524.68 units (e.g., kilometers).</li> <li>Standard Deviation: Approximately 211.00 units, indicating a fairly wide spread in flight distances.</li> <li>Minimum: 168.22 units.</li> <li>Median: 555.74 units. The median being slightly higher than the mean suggests a slight negative (left) skew, though the distribution is close to symmetrical.</li> <li>Maximum: 937.77 units.</li> <li>Interpretation: The dataset covers a diverse range of flight distances, from short regional flights to longer domestic routes.</li> </ul> </li> <li><p><code>month</code> and <code>day</code>:</p> <ul> <li>These features, derived from the <code>date</code>, represent the month of the year (1-12) and the day of the month (1-31). Their distributions are as expected, covering the full range of possible values. The mean for <code>month</code> is around 6.58, and for <code>day</code> is around 15.76, indicating data is spread across the calendar.</li> </ul> </li> </ul>"},{"location":"EDA/flights_eda/#categorical-feature-summary","title":"Categorical Feature Summary\u00b6","text":"<p>The <code>flights.describe(include=[\"object\", \"category\"])</code> method provided insights into categorical columns:</p> <ul> <li><p><code>from</code> (Origin City/Airport):</p> <ul> <li>Unique Values: 9 distinct origin locations.</li> <li>Top (Most Frequent): \"Florianopolis (SC)\" with 18,800 occurrences.</li> <li>Interpretation: Indicates a limited set of departure points, with Florianopolis (SC) being a primary hub.</li> </ul> </li> <li><p><code>to</code> (Destination City/Airport):</p> <ul> <li>Unique Values: 9 distinct destination locations.</li> <li>Top (Most Frequent): \"Florianopolis (SC)\" with 31,438 occurrences.</li> <li>Interpretation: Similar to origins, destinations are concentrated, with Florianopolis (SC) also being a major arrival hub. The higher frequency as a destination suggests it might be a more common endpoint or a central connecting point.</li> </ul> </li> <li><p><code>flightType</code> (Service Class):</p> <ul> <li>Unique Values: 3 distinct flight types.</li> <li>Top (Most Frequent): \"firstClass\" with 64,169 occurrences.</li> <li>Interpretation: First class is the most represented service type in this dataset, followed by other classes like economic and premium.</li> </ul> </li> <li><p><code>agency</code> (Booking Agency):</p> <ul> <li>Unique Values: 3 distinct agencies.</li> <li>Top (Most Frequent): \"Rainbow\" with 64,167 occurrences.</li> <li>Interpretation: The \"Rainbow\" agency handles the largest volume of bookings in this dataset.</li> </ul> </li> <li><p><code>year</code>:</p> <ul> <li>Unique Values: 5 distinct years.</li> <li>Top (Most Frequent): 2020 with 55,095 occurrences.</li> <li>Interpretation: The data spans five years, with 2020 having the highest representation of flight records.</li> </ul> </li> <li><p><code>day_name</code> (Day of the Week):</p> <ul> <li>Unique Values: 5 distinct day names (e.g., Monday to Friday, as weekends were noted to be excluded).</li> <li>Top (Most Frequent): \"Thursday\" with 52,115 occurrences.</li> <li>Interpretation: Flights are distributed across the weekdays, with Thursday showing the highest frequency in this particular dataset.</li> </ul> </li> </ul>"},{"location":"EDA/flights_eda/#overall-summary","title":"Overall Summary\u00b6","text":"<p>The descriptive statistics reveal a dataset characterized by flights primarily within a domestic or regional scope, given the distance and time ranges. Prices show variability and a slight positive skew. Key hubs, flight classes, and agencies are evident from the categorical data. The temporal distribution indicates that the data is spread across several years, with specific years and days of the week showing higher flight volumes. These initial statistics are crucial for understanding data distributions, identifying potential outliers, and informing subsequent feature engineering and modeling decisions.</p>"},{"location":"EDA/flights_eda/#date-analysis-for-splitting-criteria","title":"Date Analysis for Splitting Criteria\u00b6","text":"<ul> <li>To prevent data leakage and ensure robust model performance, it is essential to use only historical (past) data during exploratory data analysis (EDA) and model training phases. Future data should be strictly reserved for validation and testing.</li> <li>Implementing date-based splits simulates real-world scenarios by maintaining temporal order, enabling realistic and unbiased model evaluation while preserving the integrity and generalizability of your results.</li> </ul>"},{"location":"EDA/flights_eda/#flights-data-split-strategy","title":"\u2708\ufe0f Flights Data Split Strategy\u00b6","text":"<p>Note!: Holdout set was not created in production project instead train:70%, validation:15%, test:15% configuration was used.</p> Split Date Range Rows % of Total Train 2019-09-26 to 2021-05-22 89,935 60.16% Validation 2021-05-22 to 2021-12-05 22,568 15.10% Test 2021-12-05 to 2022-08-10 22,556 15.09% Holdout(Production Simulation) \ud83d\udea6 2022-08-10 to 2023-07-24 14,675 9.82% Total 2019-09-26 to 2023-07-24 149,484 100.00%"},{"location":"EDA/flights_eda/#initial-split-insights","title":"\ud83d\udcdd Initial Split Insights\u00b6","text":"<ul> <li>Chronological splits are fundamental to prevent data leakage and accurately reflect real-world model deployment scenarios where predictions are made on future, unseen data.</li> <li>Seasonality is preserved: Each split is designed to cover multiple months, aiming to capture various holiday and travel patterns.</li> <li>Balanced validation and test sets (approx. 15% each) ensure reliable model tuning and an unbiased final evaluation.</li> <li>Production simulation (\ud83d\udea6) utilizes the most recent data, offering a realistic preview of model performance during retraining cycles and aiding in drift monitoring.</li> </ul>"},{"location":"EDA/flights_eda/#why-this-chronological-split-is-crucial-evidence-based-justification","title":"\ud83d\udca1 Why This Chronological Split is Crucial (Evidence-Based Justification)\u00b6","text":"<p>A date-based (chronological) split is not just a best practice; it's essential for this dataset due to observable temporal patterns:</p> <ul> <li>Mimics Real-World Deployment \ud83c\udf0e: The core principle is to train the model on past data and evaluate its ability to predict future outcomes. This directly mirrors how the model would be used in a production environment.</li> <li>Captures Evolving Trends &amp; Seasonality \ud83d\udcc8:<ul> <li>The \"Flights per Month\" bar chart clearly shows significant seasonality in flight volume. For instance, October, December, and January exhibit peak flight numbers, while months like June, July, August, and September show relative dips. A random split could create imbalanced representations of these crucial periods across train, validation, and test sets, leading to a model that performs poorly on unseen seasonal data.</li> <li>The average price table (month/year) reveals that prices are not static. There are noticeable year-over-year price trends (e.g., prices in 2022 and 2023 are often higher than in 2020/2021 for the same month) and distinct monthly price fluctuations within each year (e.g., July 2023 shows a significantly higher average price). A chronological split ensures the model learns these temporal price dynamics and how seasonality might interact with longer-term price evolution.</li> </ul> </li> <li>Prevents Lookahead Bias (Data Leakage) \ud83d\udd75\ufe0f: Randomly shuffling data that has a temporal component would allow the model to inadvertently learn from future information during training, leading to overly optimistic performance metrics that don't hold up in real-world scenarios.</li> <li>Ensures Robust Model Evaluation &amp; Lifecycle Management \ud83d\udd04: By testing on a distinct future period, we get a more reliable estimate of the model's generalization capabilities. The \"Production Sim\" split further allows for testing how the model adapts to the very latest data patterns, which is vital for ongoing monitoring and retraining decisions.</li> </ul>"},{"location":"EDA/flights_eda/#missing-values","title":"Missing Values\u00b6","text":""},{"location":"EDA/flights_eda/#missing-value-assessment","title":"\ud83d\udd75\ufe0f\u200d\u2642\ufe0f Missing Value Assessment\u00b6","text":"<ul> <li><p>\u2705 No Missing Data Found: A thorough check for missing values across all columns and rows confirmed that the current dataset is complete. There are no missing values in any cell.</p> </li> <li><p>Implication for Current Analysis: This is a positive finding for the initial EDA and model prototyping phases, as it simplifies preprocessing by eliminating the need for immediate imputation strategies for the existing data.</p> </li> <li><p>Consideration for Production Pipelines: While the current dataset is clean, it's crucial to design production MLOps pipelines to be robust against potential missing data in future incoming flight records. Therefore, strategies for handling missing values (e.g., imputation or flagging) should still be incorporated into the production data preprocessing steps.</p> </li> </ul>"},{"location":"EDA/flights_eda/","title":"\u00b6","text":""},{"location":"EDA/flights_eda/#analysis-on-training-set","title":"Analysis on Training Set\u00b6","text":""},{"location":"EDA/flights_eda/#spitting-data-into-training-valdiation-testing-and-holdoutfor-production-simulation-sets","title":"Spitting Data into Training, Valdiation, Testing and Holdout(For Production Simulation) Sets\u00b6","text":"<p>Note!: Holdout set was not created in production project instead train:70%, validation:15%, test:15% configuration was used.</p> Split Date Range Rows % of Total Train 2019-09-26 to 2021-05-22 89,935 60.16% Validation 2021-05-23 to 2021-12-05 22,568 15.10% Test 2021-12-06 to 2022-08-10 22,556 15.09% Production Sim \ud83d\udea6 2022-08-11 to 2023-07-24 14,675 9.82% Total 2019-09-26 to 2023-07-24 149,484 100.00%"},{"location":"EDA/flights_eda/#extensive-automated-eda-for-training-set","title":"\ud83d\udcd1 Extensive Automated EDA for Training Set\u00b6","text":"<ul> <li>This will generate a extensive EDA report fot the training set.</li> </ul>"},{"location":"EDA/flights_eda/#insights-from-the-report","title":"Insights from the Report\u00b6","text":"<ul> <li>Distance shows a strong positive correlation with both price and flight time.</li> <li>Month and year are highly correlated, reflecting the chronological structure of the data.</li> <li>Price is strongly correlated with both distance and flight time, indicating that longer and longer-duration flights tend to be more expensive.</li> <li>Flight time is also highly correlated with both distance and price, suggesting that longer flights cover greater distances and cost more.</li> <li>Year and month exhibit a strong correlation, as expected in time series data.</li> </ul> <p>Detailed report can be found in reports/eda folder which can be viewed as html file in browser.</p>"},{"location":"EDA/flights_eda/#5-number-summary","title":"5 Number Summary\u00b6","text":""},{"location":"EDA/flights_eda/#five-number-summary-insights","title":"\u2708\ufe0f Five Number Summary Insights\u00b6","text":"<ul> <li><p>Flight Time \u23f1\ufe0f:</p> <ul> <li>Ranges from 0.44 to 2.44 hours, with an average of 1.35 hours.</li> <li>The median is 1.44 hours, and most flights are between 0.85 and 1.76 hours, indicating a focus on short to medium-haul routes.</li> </ul> </li> <li><p>Distance \ud83d\udccf:</p> <ul> <li>Flights cover 168 km to 938 km, with an average of 520 km.</li> <li>The median distance is 556 km, and most flights fall between 328 km and 677 km, showing a concentration on regional travel.</li> </ul> </li> <li><p>Month &amp; Day \ud83d\udcc5:</p> <ul> <li>Flights are distributed across all months (1\u201312) and days (1\u201331), with the median flight in June on the 16th day.</li> <li>The average month is about June (6.4), and the average day is the 16th, suggesting relatively even distribution throughout the year and month.</li> </ul> </li> <li><p>Price \ud83d\udcb8:</p> <ul> <li>Prices range from 301.51 to 1,754.17, with an average of 944.79.</li> <li>The median price is 889.07, and most flights are priced between 667.00 and 1,209.04.</li> <li>The right-skewed distribution indicates a few high-priced flights.</li> </ul> </li> <li><p>Categorical Features \ud83c\udff7\ufe0f:</p> <ul> <li>9 unique origins and destinations, with Campo Grande (MS) as the most common origin and Florianopolis (SC) as the top destination.</li> <li>FirstClass is the most frequent flight type, and Rainbow is the leading agency.</li> <li>The year 2020 dominates the data, with 55,095 flights.</li> </ul> </li> <li><p>Date Range \ud83d\udcc6:</p> <ul> <li>Data spans from 2019-09-26 to 2021-05-22.</li> <li>The median flight date is 2020-07-02, and the average date falls in mid-2020, indicating a concentration of flights during this period.</li> </ul> </li> </ul> <p>Overall: The dataset primarily features short to medium-haul flights, with a strong presence of first-class tickets and a concentration of flights in 2020. Prices and distances show moderate variability, and the data is well-distributed across months and days, supporting robust time-based analysis.</p>"},{"location":"EDA/flights_eda/#skew-in-each-numerical-column","title":"Skew in Each Numerical Column\u00b6","text":""},{"location":"EDA/flights_eda/#skewness-in-numerical-features","title":"Skewness in Numerical Features\u00b6","text":"<p>Skewness measures the asymmetry of a feature's distribution. Understanding this is crucial as it can guide feature transformation choices and influence model performance. The following skewness values were observed for the numerical columns:</p> <ul> <li><p><code>price</code> (Target Variable) \ud83d\udcb0:</p> <ul> <li>Skewness Value: +0.353881</li> <li>Interpretation: Exhibits a mild positive (right) skew. This indicates that while many flight prices are concentrated at the lower end of the distribution, there's a tail extending towards higher prices. This suggests more flights are relatively inexpensive, with fewer, but notably more expensive, options.</li> </ul> </li> <li><p><code>time</code> (Flight Duration) \u23f1\ufe0f:</p> <ul> <li>Skewness Value: -0.055489</li> <li>Interpretation: The distribution of flight duration is very close to symmetrical. The skewness value is slightly negative but very near zero, implying that flight times are quite evenly distributed around the average, without a significant bias towards shorter or longer durations.</li> </ul> </li> <li><p><code>distance</code> (Flight Distance) \ud83d\uddfa\ufe0f:</p> <ul> <li>Skewness Value: -0.059037</li> <li>Interpretation: Similar to flight time, the distribution of flight distance is very close to symmetrical. The slightly negative skewness value is minimal, indicating a balanced distribution of flight distances around their mean.</li> </ul> </li> <li><p><code>month</code> (Month of Flight) \ud83d\uddd3\ufe0f:</p> <ul> <li>Skewness Value: +0.065595</li> <li>Interpretation: The distribution of flights across months is very close to symmetrical. The small positive skewness value suggests a fairly even spread of flight records throughout the months of the year, without a strong lean towards earlier or later months in the overall dataset distribution (though seasonal peaks exist, skewness here refers to the shape of the count distribution itself).</li> </ul> </li> <li><p><code>day</code> (Day of Month) \ud83d\udd22:</p> <ul> <li>Skewness Value: +0.007979</li> <li>Interpretation: The distribution of flights by the day of the month is highly symmetrical. The skewness value is extremely close to zero, indicating that flight occurrences are very evenly spread across the days of the month.</li> </ul> </li> </ul> <p>Summary of Skewness Insights:</p> <p>The target variable, <code>price</code>, displays a mild positive skew, a common characteristic for pricing data. The primary predictor candidates, <code>time</code> and <code>distance</code>, along with the temporal features <code>month</code> and <code>day</code>, all exhibit distributions that are very close to symmetrical. This general lack of strong skew in most predictors is favorable, as many machine learning algorithms perform optimally with or assume near-symmetrical data distributions. For the <code>price</code> variable, its mild skew might still warrant consideration for transformation (e.g., log transformation) depending on the chosen modeling approach to potentially improve model performance and linearity assumptions.</p>"},{"location":"EDA/flights_eda/#univariate-analysis","title":"Univariate Analysis\u00b6","text":""},{"location":"EDA/flights_eda/#univariate-analysis-of-numerical-features","title":"\ud83d\udcca Univariate Analysis of Numerical Features\u00b6","text":""},{"location":"EDA/flights_eda/#distribution-of-numerical-columns","title":"Distribution Of Numerical Columns\u00b6","text":""},{"location":"EDA/flights_eda/#distribution-analysis-main-insights","title":"\ud83d\udcca Distribution Analysis Main Insights\u00b6","text":""},{"location":"EDA/flights_eda/#price","title":"\ud83d\udcb0 Price\u00b6","text":"<p>The price distribution is multimodal and exhibits a slight right skew. The presence of several peaks clearly indicates multiple common price ranges. These variations can be attributed to factors such as different flight classes, varying routes, or periods of high demand. Prices are distributed from approximately 300 to 1800, without extreme outliers.</p>"},{"location":"EDA/flights_eda/#day","title":"\ud83d\uddd3\ufe0f Day\u00b6","text":"<p>The distribution of flights across days 1\u201331 is nearly uniform. This uniformity demonstrates no particular day-of-the-month bias in the flight data.</p>"},{"location":"EDA/flights_eda/#month","title":"\ud83d\udcc5 Month\u00b6","text":"<p>Flight frequencies vary significantly by month. Months such as January (\u2744\ufe0f), February, March, April, October (\ud83c\udf42), November, and December show higher flight frequencies. Conversely, June, July (\u2600\ufe0f), August, and September have lower frequencies. This pattern strongly indicates seasonality in flight operations, with more flights occurring in the winter and spring months and fewer during the summer and early fall.</p>"},{"location":"EDA/flights_eda/#distance","title":"\u2708\ufe0f Distance\u00b6","text":"<p>The distribution of flight distances is multimodal, characterized by several distinct peaks. This pattern shows that flights are predominantly clustered around certain common distances, which likely correspond to popular or standard flight routes.</p>"},{"location":"EDA/flights_eda/#time","title":"\u23f1\ufe0f Time\u00b6","text":"<p>The distribution of flight durations is also multimodal. A significant peak is observed around 1.5 (presumably hours), indicating that flight durations of this length are considerably more common. This reflects typical times for standard routes.</p>"},{"location":"EDA/flights_eda/#summary-of-univariate-insights","title":"\ud83d\udcdd Summary of Univariate Insights\u00b6","text":"<p>The data reveals clear and actionable patterns:</p> <ul> <li>Seasonality Trends \ud83d\udcc8: Flight activity varies significantly by month.</li> <li>Route Clustering \ud83d\uddfa\ufe0f: Flight distances and durations show distinct clustering, indicative of popular routes.</li> <li>Pricing Bands \ud83c\udff7\ufe0f: Multiple distinct price ranges are evident in the data.</li> </ul> <p>No major data quality issues are apparent from this initial univariate examination.</p>"},{"location":"EDA/flights_eda/#outlier-detection-using-boxplots","title":"Outlier Detection Using Boxplots\u00b6","text":""},{"location":"EDA/flights_eda/#box-plot-analysis-insights","title":"\ud83d\udce6 Box Plot Analysis Insights\u00b6","text":""},{"location":"EDA/flights_eda/#price","title":"\ud83d\udcb0 Price\u00b6","text":"<ul> <li>Median: ~889</li> <li>IQR: 542 (Q1: 667, Q3: 1209)</li> <li>Range: 301 (min) to 1754 (max) Insight: The price distribution is fairly wide, underscored by a large interquartile range. While no extreme outliers are present, this spread signifies considerable price variability. This variation likely stems from diverse factors such as different routes, flight classes, or booking times.</li> </ul>"},{"location":"EDA/flights_eda/#day","title":"\ud83d\uddd3\ufe0f Day\u00b6","text":"<ul> <li>Median: 16</li> <li>IQR: 15 (Q1: 8, Q3: 23)</li> <li>Range: 1 to 31 Insight: Days are uniformly distributed, as expected for calendar days. The absence of outliers indicates a balanced sampling across the month.</li> </ul>"},{"location":"EDA/flights_eda/#month","title":"\ud83d\udcc5 Month\u00b6","text":"<ul> <li>Median: 6</li> <li>IQR: 7 (Q1: 3, Q3: 10)</li> <li>Range: 1 to 12 Insight: Months are also well-distributed, with no outliers detected. This suggests the dataset comprehensively covers the full year.</li> </ul>"},{"location":"EDA/flights_eda/#distance","title":"\u2708\ufe0f Distance\u00b6","text":"<ul> <li>Median: ~556</li> <li>IQR: ~349 (Q1: 328, Q3: 677)</li> <li>Range: 168 to 938 Insight: Flight distances vary widely, although the majority fall between 328 and 677 units (likely kilometers or miles). No extreme outliers are observed, but the spread indicates a mix of short and medium-haul flights.</li> </ul>"},{"location":"EDA/flights_eda/#time","title":"\u23f1\ufe0f Time\u00b6","text":"<ul> <li>Median: 1.44</li> <li>IQR: 0.91 (Q1: 0.85, Q3: 1.76)</li> <li>Range: 0.44 to 2.44 Insight: Flight durations are mostly concentrated between 0.85 and 1.76 (likely hours). The distribution is relatively tight with no outliers, suggesting that most flights are of similar duration.</li> </ul>"},{"location":"EDA/flights_eda/#overall-box-plot-summary","title":"\ud83d\udcca Overall Box Plot Summary\u00b6","text":"<ul> <li>The data demonstrates a good distribution across time-related features (Day, Month) and categories.</li> <li>Price and Distance exhibit the most variability, highlighting them as potentially significant factors for predictive modeling.</li> <li>The absence of significant outliers across these features points towards good data quality. \u2705</li> <li>The observed spread in price and distance may reflect different flight types or routes, which could serve as useful features for prediction tasks.</li> </ul>"},{"location":"EDA/flights_eda/#univariate-categorical-analysis","title":"Univariate Categorical Analysis\u00b6","text":""},{"location":"EDA/flights_eda/#categorical-variable-bar-plot-insights","title":"\ud83d\udcca Categorical Variable Bar Plot Insights\u00b6","text":""},{"location":"EDA/flights_eda/#day_name","title":"\ud83d\uddd3\ufe0f <code>day_name</code>\u00b6","text":"<ul> <li>Thursday shows significantly higher flight counts, almost double compared to other days.</li> <li>Monday, Friday, Saturday, and Sunday have similar, much lower counts. Insight: A strong day-of-week effect is evident, with Thursday emerging as a peak travel day. This could be influenced by business travel patterns or specific airline scheduling.</li> </ul>"},{"location":"EDA/flights_eda/#year","title":"\ud83d\udcc5 <code>year</code>\u00b6","text":"<ul> <li>2020 has the highest number of flights, followed by 2021 and 2019.</li> <li>2022 and 2023 have zero records. Insight: The dataset is primarily concentrated in the 2019\u20132021 period, with a notable spike in 2020. This distribution may reflect the data collection's specific timeframe or be indicative of real-world events (e.g., initial phases or disruptions related to COVID-19).</li> </ul>"},{"location":"EDA/flights_eda/#agency","title":"\ud83c\udfe2 <code>agency</code>\u00b6","text":"<ul> <li>CloudFly and Rainbow show similar, high flight counts.</li> <li>FlyingDrops has a considerably lower number of flights. Insight: The market appears to be dominated by two main agencies (CloudFly and Rainbow), with FlyingDrops playing a more minor role.</li> </ul>"},{"location":"EDA/flights_eda/#flighttype","title":"\u2708\ufe0f <code>flightType</code>\u00b6","text":"<ul> <li>firstClass is the most common flight type.</li> <li>economic and premium classes follow, with nearly equal, lower counts. Insight: There is a higher proportion of first-class flights in this dataset. This is somewhat unusual and may reflect the specific focus of the dataset or the primary offerings of the included agencies.</li> </ul>"},{"location":"EDA/flights_eda/#to-destination","title":"\ud83c\udfaf <code>to</code> (Destination)\u00b6","text":"<ul> <li>Florianopolis (SC) is the most frequent destination. \ud83c\udfd6\ufe0f</li> <li>Aracaju (SE), Campo Grande (MS), and Recife (PE) also have high destination counts.</li> <li>Rio de Janeiro (RJ) and Salvador (BH) have the lowest counts as destinations. Insight: Certain destinations are significantly more popular than others. This popularity could be driven by tourism, business activities, or these locations serving as major airline hubs.</li> </ul>"},{"location":"EDA/flights_eda/#from-origin","title":"\ud83d\udeeb <code>from</code> (Origin)\u00b6","text":"<ul> <li>Flight origins are more evenly distributed compared to destinations.</li> <li>Aracaju (SE), Brasilia (DF), Campo Grande (MS), and Recife (PE) have similar, relatively high origination counts.</li> <li>Rio de Janeiro (RJ) and Salvador (BH) are less frequent as origin points. Insight: Flights originate fairly evenly from most listed cities. However, a comparison with destination data reveals that some cities are more prominent as destinations than as origins.</li> </ul>"},{"location":"EDA/flights_eda/#overall-categorical-insights-summary","title":"\ud83d\udcdc Overall Categorical Insights Summary\u00b6","text":"<ul> <li>Strong patterns are observed in day-of-week, year, agency, and destination distributions.</li> <li>Thursday (for <code>day_name</code>) and the year 2020 (for <code>year</code>) stand out as notable peaks within their respective categories.</li> <li>The distributions for agency and flight type suggest a non-uniform market structure and service offering.</li> <li>These categorical features, with their distinct patterns, are highly likely to be important predictors in any subsequent modeling tasks. \u2728</li> </ul>"},{"location":"EDA/flights_eda/#bivariate-analysis","title":"Bivariate Analysis\u00b6","text":""},{"location":"EDA/flights_eda/#pairplot","title":"Pairplot\u00b6","text":"<ul> <li>Ploting scatter plots between all numerical columns to see the relation ship.</li> </ul>"},{"location":"EDA/flights_eda/#correlation-analysis-using-corr-matrix-and-heatmap","title":"Correlation Analysis using Corr Matrix And Heatmap\u00b6","text":""},{"location":"EDA/flights_eda/#correlation-analysis-insights","title":"\ud83d\udd17 Correlation Analysis Insights\u00b6","text":""},{"location":"EDA/flights_eda/#strong-correlation-time-and-distance","title":"\u2708\ufe0f\u23f1\ufe0f Strong Correlation: <code>time</code> and <code>distance</code>\u00b6","text":"<ul> <li>Correlation Coefficient \u2248 1.0 (specifically 0.99999) Insight: Flight <code>time</code> and <code>distance</code> are almost perfectly correlated in this dataset. This indicates an essentially linear relationship, meaning as distance increases, flight time increases proportionally. This is expected, as longer flights naturally cover more ground, assuming relatively consistent cruising speeds.</li> </ul>"},{"location":"EDA/flights_eda/#moderate-correlation-with-price","title":"\ud83d\udcb0 Moderate Correlation with <code>price</code>\u00b6","text":"<ul> <li><code>price</code> vs <code>time</code>: 0.65</li> <li><code>price</code> vs <code>distance</code>: 0.65 Insight: Both flight <code>time</code> and <code>distance</code> show a moderate positive correlation with <code>price</code>. This suggests that longer and farther flights generally tend to be more expensive. However, the correlation isn't perfect, implying that other factors (such as flight class, agency, demand, or booking time) also significantly influence the final price.</li> </ul>"},{"location":"EDA/flights_eda/#very-weak-or-no-correlation-with-month-and-day","title":"\ud83d\uddd3\ufe0f Very Weak or No Correlation with <code>month</code> and <code>day</code>\u00b6","text":"<ul> <li>All correlations involving <code>month</code> and <code>day</code> with <code>price</code>, <code>distance</code>, or <code>time</code> are near zero. Insight: The <code>month</code> and <code>day</code> of the flight do not exhibit a linear relationship with <code>price</code>, <code>distance</code>, or <code>time</code>. Any influence these temporal features have on pricing or flight characteristics is likely non-linear or categorical (e.g., specific effects of holidays, weekends, or seasonal demand peaks not captured by a simple linear trend).</li> </ul>"},{"location":"EDA/flights_eda/#pairplot-visuals","title":"\ud83d\udcca Pairplot Visuals\u00b6","text":"<ul> <li>Scatter Plots:<ul> <li>The plot of <code>time</code> vs <code>distance</code> displays a tight, clear linear pattern, reinforcing their strong correlation.</li> <li>Plots for <code>price</code> vs <code>time</code> and <code>price</code> vs <code>distance</code> show discernible positive trends, but with a greater degree of spread (variance), aligning with the moderate correlation values.</li> <li>Other variable pairs generally show no clear linear patterns.</li> </ul> </li> <li>Distributions:<ul> <li>The individual distribution plots for each variable along the diagonal of the pairplot match the shapes observed in the earlier histogram analysis.</li> </ul> </li> </ul>"},{"location":"EDA/flights_eda/#summary-of-correlation-insights","title":"\ud83d\udcdd Summary of Correlation Insights\u00b6","text":"<ul> <li>Key Numerical Drivers \ud83d\udd11: <code>Distance</code> and <code>time</code> are identified as the primary numerical features linearly influencing <code>price</code>.</li> <li>Multicollinearity Alert \u26a0\ufe0f: <code>time</code> and <code>distance</code> are highly redundant due to their near-perfect correlation. For modeling purposes, it's likely that only one of these features will be necessary to avoid multicollinearity.</li> <li>Handling Other Features: <code>Month</code> and <code>day</code> will likely need to be treated as categorical features or used to create interaction terms with other features to effectively capture their impact on the target variable.</li> </ul> <p>These insights are crucial for guiding feature selection and feature engineering efforts in the development of predictive models.</p>"},{"location":"EDA/flights_eda/#boxplots-for-numerical-insights-from-categorical-columns","title":"Boxplots For Numerical Insights From Categorical Columns\u00b6","text":""},{"location":"EDA/flights_eda/#analysis-of-flight-characteristics-by-category","title":"\u2708\ufe0f Analysis of Flight Characteristics by Category\u00b6","text":"<p>This section details how flight time, distance, and price vary across different categorical features.</p>"},{"location":"EDA/flights_eda/#by-day-of-the-week","title":"\ud83d\uddd3\ufe0f By Day of the Week\u00b6","text":"<ul> <li>Flight Duration &amp; Distance \ud83d\udd70\ufe0f\ud83d\udccf: Flight times and distances exhibit high consistency across all days of the week, with only minor variations observed.</li> <li>Ticket Prices \ud83d\udcb0: Prices generally remain stable across the days. A notable dip in prices occurs on Thursdays, which aligns with the observation that Thursdays experience higher flight volumes.</li> </ul>"},{"location":"EDA/flights_eda/#by-year","title":"\ud83d\udcc5 By Year\u00b6","text":"<ul> <li>Flight Duration, Distance &amp; Price Over Time \u23f3: Analysis of data from 2019 through 2021 indicates stability in flight times, distances, and prices. Median values and data spreads for these variables show minimal change year-over-year, suggesting that despite potential seasonal trends, the average price point remained consistent during this period.</li> </ul>"},{"location":"EDA/flights_eda/#by-agency","title":"\ud83c\udfe2 By Agency\u00b6","text":"<ul> <li>Flight Duration &amp; Distance \ud83d\ude80: The distributions for flight time and distance are nearly identical across all analyzed agencies (CloudFly, FlyingDrops, and Rainbow). No single agency demonstrates a significant deviation in terms of offering consistently faster or longer flights.</li> <li>Ticket Prices \ud83d\udcb8: A clear distinction in pricing is observed: FlyingDrops presents noticeably higher median prices compared to CloudFly and Rainbow. The latter two agencies offer prices that are largely similar to each other.</li> </ul>"},{"location":"EDA/flights_eda/#by-flight-type","title":"\ud83d\udcba By Flight Type\u00b6","text":"<ul> <li>Flight Duration &amp; Distance \ud83d\uddfa\ufe0f: Flights categorized as Economic, firstClass, and premium show almost identical distributions for both flight time and distance. The class of service does not correlate with significant differences in route length or flight duration.</li> <li>Ticket Prices \ud83c\udff7\ufe0f: As anticipated, firstClass flights are the most expensive. Premium flights are priced in the mid-tier, while economic class flights are the least expensive. This confirms standard airline pricing structures.</li> </ul>"},{"location":"EDA/flights_eda/#by-origin-from-and-destination-to","title":"\ud83c\udf0d By Origin (<code>from</code>) and Destination (<code>to</code>)\u00b6","text":"<ul> <li>Flight Duration \u23f1\ufe0f:<ul> <li>Flights originating from or destined for Salvador (BH) and Florianopolis (SC) generally have longer durations.</li> <li>Conversely, flights involving Natal (RN) and Sao Paulo (SP) tend to have shorter flight times.</li> </ul> </li> <li>Flight Distance \u2708\ufe0f:<ul> <li>Consistent with duration, routes to/from Salvador (BH) and Florianopolis (SC) also cover the longest distances.</li> <li>Routes involving Natal (RN) and Sao Paulo (SP) are typically the shortest in terms of distance.</li> </ul> </li> <li>Ticket Prices \ud83d\udcb2:<ul> <li>Journeys involving Salvador (BH), Florianopolis (SC), and Aracaju (SE) are associated with higher ticket prices.</li> <li>Flights to or from Sao Paulo (SP) and Brasilia (DF) are generally among the least expensive.</li> </ul> </li> </ul> <p>This revised version should be more suitable for your documentation needs.</p>"},{"location":"EDA/flights_eda/#barplots-between-categorical-columns-and-numerical-agregationsmean-mode-median-sum","title":"Barplots Between Categorical columns and Numerical Agregations(Mean, Mode, Median, Sum)\u00b6","text":""},{"location":"EDA/flights_eda/#categorical-vs-numerical-aggregations-bar-plot-insights","title":"\ud83d\udcca Categorical vs. Numerical Aggregations: Bar Plot Insights\u00b6","text":"<p>This section details insights derived from bar plots showing aggregated numerical values (sum and mean) for different categorical features.</p>"},{"location":"EDA/flights_eda/#sum-aggregation-insights","title":"\u2795 Sum Aggregation Insights\u00b6","text":"<p>These insights reflect the total sums (e.g., total sales, total distance covered) for each category.</p>"},{"location":"EDA/flights_eda/#by-origin-from","title":"\ud83d\udeeb By Origin (<code>from</code>)\u00b6","text":"<ul> <li>Total Price (Sales Volume) \ud83d\udcb0:<ul> <li>The highest total sales were generated from flights originating in Brasilia (DF), closely followed by Campo Grande (MS) and Aracaju (SE).</li> <li>The lowest total sales were from flights originating in Salvador (BH). This contrasts with its distance/time metrics, potentially indicating fewer, but possibly more expensive, tickets or simply fewer flights overall from this origin in the dataset.</li> </ul> </li> <li>Total Time &amp; Distance \ud83d\udd70\ufe0f\ud83d\udccf:<ul> <li>These two metrics show similar trends. The greatest total flight distance and time were covered by flights originating from Florianopolis (SC), followed by Aracaju (SE).</li> </ul> </li> </ul>"},{"location":"EDA/flights_eda/#by-destination-to","title":"\ud83c\udfaf By Destination (<code>to</code>)\u00b6","text":"<ul> <li>Total Price (Sales Volume) \ud83d\udcb0:<ul> <li>Flights destined for Florianopolis (SC) generated the highest total sales by a significant margin, followed by Aracaju (SE).</li> <li>The lowest total sales were for flights destined to Rio de Janeiro (RJ).</li> </ul> </li> <li>Total Time &amp; Distance \ud83d\udd70\ufe0f\ud83d\udccf:<ul> <li>The trend for total time and distance precisely mirrors the trend observed for total price, with Florianopolis (SC) leading.</li> </ul> </li> </ul>"},{"location":"EDA/flights_eda/#by-flight-type","title":"\u2708\ufe0f By Flight Type\u00b6","text":"<ul> <li>Total Time &amp; Distance \ud83d\udd70\ufe0f\ud83d\udccf:<ul> <li>A consistent pattern is observed: firstClass flights account for the largest total flight time and distance, followed by economic class, and then closely by premium class.</li> </ul> </li> <li>Total Price (Sales Volume) \ud83d\udcb0:<ul> <li>Total sales are highest for firstClass flights, followed by premium, and then economic class.</li> </ul> </li> </ul>"},{"location":"EDA/flights_eda/#by-agency","title":"\ud83c\udfe2 By Agency\u00b6","text":"<ul> <li>Total Time &amp; Distance \ud83d\udd70\ufe0f\ud83d\udccf:<ul> <li>FlyingDrops accounts for the lowest total flight time and distance.</li> <li>Rainbow shows the highest total time and distance by a substantial margin compared to FlyingDrops, and is closely followed by CloudFy.</li> </ul> </li> <li>Total Price (Sales Volume) \ud83d\udcb0:<ul> <li>The sales pattern mirrors the time and distance trends: Rainbow generated the highest total revenue, followed by CloudFy, with FlyingDrops earning the least.</li> </ul> </li> </ul>"},{"location":"EDA/flights_eda/#by-year","title":"\ud83d\udcc5 By Year\u00b6","text":"<ul> <li>Total Time &amp; Distance \ud83d\udd70\ufe0f\ud83d\udccf:<ul> <li>The year 2020 recorded the highest total flight time and distance by a significant margin, followed by 2021 and then 2019. This lower figure for 2019 may be attributable to the dataset starting in the 9th month of that year.</li> </ul> </li> <li>Total Price (Sales Volume) \ud83d\udcb0:<ul> <li>Total sales follow a similar trend to total time and distance, with 2020 being the highest.</li> </ul> </li> </ul>"},{"location":"EDA/flights_eda/#by-day-of-the-week-day_name","title":"\ud83d\uddd3\ufe0f By Day of the Week (<code>day_name</code>)\u00b6","text":"<ul> <li>Total Time, Distance, &amp; Price \ud83d\udd70\ufe0f\ud83d\udccf\ud83d\udcb0:<ul> <li>Thursday exhibits the highest total flight distance, total flight time, and total sales. This could be linked to the observation that flights are comparably cheaper on Thursdays, potentially driving higher volume, or other contributing factors.</li> </ul> </li> </ul>"},{"location":"EDA/flights_eda/#mean-aggregation-insights","title":"\u2696\ufe0f Mean Aggregation Insights\u00b6","text":"<p>These insights reflect the average values (e.g., average price, average flight duration) for each category.</p>"},{"location":"EDA/flights_eda/#by-origin-from","title":"\ud83d\udeeb By Origin (<code>from</code>)\u00b6","text":"<ul> <li>Mean Time &amp; Distance \ud83d\udd70\ufe0f\ud83d\udccf:<ul> <li>These metrics show similar trends. Flights originating from Salvador (BH) have the highest mean flight time and distance, followed by Florianopolis (SC) and Aracaju (SE).</li> </ul> </li> <li>Mean Price \ud83d\udcb0:<ul> <li>The highest mean price is for flights originating from Salvador (BH), followed by Brasilia (DF) and Rio de Janeiro (RJ).</li> </ul> </li> </ul>"},{"location":"EDA/flights_eda/#by-destination-to","title":"\ud83c\udfaf By Destination (<code>to</code>)\u00b6","text":"<ul> <li>Mean Time &amp; Distance \ud83d\udd70\ufe0f\ud83d\udccf:<ul> <li>A similar pattern is observed: Salvador (BH) has the highest mean flight time and distance for arrivals, followed by Florianopolis (SC) and Aracaju (SE).</li> <li>The lowest mean time and distance are for flights arriving at Sao Paulo (SP) and Brasilia (DF).</li> </ul> </li> <li>Mean Price \ud83d\udcb0:<ul> <li>The highest mean price is for flights destined to Salvador (BH), followed by Florianopolis (SC) and Aracaju (SE).</li> <li>The lowest mean price is for flights arriving at Sao Paulo (SP).</li> </ul> </li> </ul>"},{"location":"EDA/flights_eda/#by-flight-type","title":"\u2708\ufe0f By Flight Type\u00b6","text":"<ul> <li>Mean Time &amp; Distance \ud83d\udd70\ufe0f\ud83d\udccf:<ul> <li>Mean flight time and distance are similar across all flight types (Economic, firstClass, premium).</li> </ul> </li> <li>Mean Price \ud83d\udcb0:<ul> <li>firstClass flights have the highest mean price, followed by premium, and then economic class.</li> </ul> </li> </ul>"},{"location":"EDA/flights_eda/#by-agency","title":"\ud83c\udfe2 By Agency\u00b6","text":"<ul> <li>Mean Time &amp; Distance \ud83d\udd70\ufe0f\ud83d\udccf:<ul> <li>Mean flight time and distance show similar values across the different agencies.</li> </ul> </li> <li>Mean Price \ud83d\udcb0:<ul> <li>FlyingDrops has the highest mean price.</li> <li>Rainbow and CloudFy have similar, lower mean prices compared to FlyingDrops.</li> </ul> </li> </ul>"},{"location":"EDA/flights_eda/#by-year","title":"\ud83d\udcc5 By Year\u00b6","text":"<ul> <li>Mean Time, Distance, &amp; Price \ud83d\udd70\ufe0f\ud83d\udccf\ud83d\udcb0:<ul> <li>Mean flight distance, mean price, and mean time are all relatively similar across the years (2019-2022, where data is available).</li> </ul> </li> </ul>"},{"location":"EDA/flights_eda/#by-day-of-the-week-day_name","title":"\ud83d\uddd3\ufe0f By Day of the Week (<code>day_name</code>)\u00b6","text":"<ul> <li>Mean Time &amp; Distance \ud83d\udd70\ufe0f\ud83d\udccf:<ul> <li>Mean flight time and distance are marginally lowest on Thursday. Values for other days of the week are quite similar to each other.</li> </ul> </li> <li>Mean Price \ud83d\udcb0:<ul> <li>The mean price is lower on Thursday compared to other days of the week.</li> </ul> </li> </ul>"},{"location":"EDA/flights_eda/#eda-summary-and-model-pipeline-recommendations-for-flight-price-prediction","title":"EDA Summary And Model Pipeline Recommendations for Flight Price Prediction\u00b6","text":"<p>A comprehensive summary of the Exploratory Data Analysis (EDA), integrating detailed findings from univariate, bivariate, and multivariate analyses, including aggregations and temporal trend assessments. This summary provides actionable insights crucial for developing a robust and accurate flight price prediction model.</p>"},{"location":"EDA/flights_eda/#1-data-understanding-quality-assessment","title":"1. \ud83d\uddc2\ufe0f Data Understanding &amp; Quality Assessment\u00b6","text":"<ul> <li><p>Schema &amp; Data Types:</p> <ul> <li>The dataset initially includes <code>travelCode</code> and <code>userCode</code>. For modeling purposes, the 8 core features are: <code>from</code> (Origin City), <code>to</code> (Destination City), <code>flightType</code> (Service Class), <code>price</code> (Target Variable), <code>time</code> (Flight Duration), <code>distance</code> (Flight Distance), <code>agency</code> (Airline Agency), and <code>date</code> (Flight Date).</li> <li>Categorical Features: <code>from</code>, <code>to</code> (9 unique cities each), <code>flightType</code> (3 unique values: 'economic', 'premium', 'firstClass'), <code>agency</code> (3 unique values). These, along with extracted <code>year</code> and <code>day_name</code>, were converted to <code>category</code> dtype.</li> <li>Numerical Features: <code>price</code>, <code>time</code>, <code>distance</code> were downcast to <code>float32</code>. Extracted <code>month</code> and <code>day</code> were optimized to <code>uint8</code>.</li> <li>Temporal Feature: <code>date</code> was converted to <code>datetime64[ns]</code> objects, and features like <code>year</code>, <code>month</code>, <code>day</code>, <code>day_name</code> were extracted and <code>month</code>, <code>day</code> can be converted to <code>cyclic</code> encoding for better performance.</li> </ul> </li> <li><p>Dataset Size &amp; Integrity:</p> <ul> <li>Initial State: The raw dataset contained 271,888 rows.</li> <li>Duplicate Records Analysis:<ul> <li>No duplicate values were found when using <code>travelCode</code> and <code>userCode</code> as unique identifiers, confirming the integrity of each individual record.</li> <li>Furthermore, no erroneous duplicates were found where the same <code>userCode</code> had a duplicate entry for the same flight leg (i.e., on the same date and time).</li> <li>To prepare the data for modeling, the identifiers were removed to assess duplication based on core flight attributes. This revealed that 45.02% of the records were duplicates.</li> <li>Critical Justification: Although for data analysis to accurately answer business questions like total number of sales etc its very important to not remove these duplicate entries as they are from a different user however for model building and doing eda specifically for eda we have to remove these duplicates carrying same information. This prevents the model from being biased towards more frequently traveled flight routes.</li> <li>Rows with different <code>date</code> even if rest of the features are same will be preserved as they shows time trend with price.</li> </ul> </li> <li>Deduplicated Dataset: After removal of these duplicates, the dataset used for EDA and modeling consists of 149,484 unique flight instances.</li> </ul> </li> <li><p>Missing Values Assessment:</p> <ul> <li>\u2705 The current dataset (after deduplication and processing) shows no missing values in any column, as confirmed by <code>flights.info()</code> showing non-null counts equal to the total number of entries for all columns.</li> <li>Proactive Strategy: Despite current completeness, production pipelines must include robust imputation strategies (e.g., mean/median/mode imputation for numerical, constant/mode for categorical) to handle potential missing data in new, incoming flight records.</li> </ul> </li> <li><p>Memory Efficiency:</p> <ul> <li>Strategic downcasting of numerical types (e.g., <code>float64</code> to <code>float32</code>, appropriate <code>uint</code> types for date components) and conversion of object strings to <code>category</code> dtype resulted in a substantial memory usage reduction. The memory footprint for the 149,484 records was reduced from over 10.3 MB (with initial dtypes after loading and dropping original ID columns) to approximately 5.1 MB after all optimizations.</li> <li>Production Implication: Utilizing optimized data types is crucial for faster processing, reduced memory footprint, and improved scalability in production environments.</li> </ul> </li> </ul>"},{"location":"EDA/flights_eda/#2-target-variable-price-analysis","title":"2. \ud83c\udfaf Target Variable (<code>price</code>) Analysis\u00b6","text":"<ul> <li><p>Distributional Characteristics (from <code>flights.describe()</code> and skewness calculation):</p> <ul> <li>Central Tendency: Mean: $949.42, Median: $898.04. The mean being higher than the median indicates a positive skew.</li> <li>Spread &amp; Range: Min: $301.51, Max: $1754.17. Standard Deviation: $358.45.</li> <li>Quartiles: 25th Percentile (Q1): $669.83, 75th Percentile (Q3): $1214.82. Interquartile Range (IQR): $544.99.</li> <li>Skewness: The <code>price</code> distribution is slightly right-skewed (skewness ~0.53), with a longer tail towards higher prices. This was confirmed by visual inspection (histograms, box plots in the notebook) and skewness statistics.</li> </ul> </li> <li><p>Implications for Modeling:</p> <ul> <li>The presence of higher-priced outliers and the right skew can disproportionately influence models sensitive to data distribution (e.g., linear regression).</li> <li>Action: Consider applying a log transformation (e.g., <code>np.log1p</code>) to the <code>price</code> variable to normalize its distribution, stabilize variance, and potentially improve model performance and interpretability.</li> </ul> </li> </ul>"},{"location":"EDA/flights_eda/#3-feature-analysis-relationships-engineering-insights","title":"3. \ud83c\udfd7\ufe0f Feature Analysis, Relationships &amp; Engineering Insights\u00b6","text":"<ul> <li><p>Numerical Features (<code>distance</code>, <code>time</code>):</p> <ul> <li>Correlation with <code>price</code>: Both <code>distance</code> and <code>time</code> exhibit a moderately strong positive correlation with <code>price</code> (correlation coefficients around 0.65, as seen in the correlation heatmap). Longer flights (in distance or duration) generally cost more.</li> <li>Inter-Correlation (Multicollinearity): <code>distance</code> and <code>time</code> are almost perfectly positively correlated with each other (correlation coefficient ~1.0 from the heatmap). They essentially convey redundant information.<ul> <li><code>distance</code>: Mean: 524.68 units, Median: 555.74 units. Range: 168.22 to 937.77 units. Skewness ~0.00 (nearly symmetric).</li> <li><code>time</code>: Mean: 1.36 hours, Median: 1.44 hours. Range: 0.44 to 2.44 hours. Skewness ~0.00 (nearly symmetric).</li> </ul> </li> <li>Distribution: Both <code>distance</code> and <code>time</code> are nearly symmetric with very low skewness, confirmed by histograms and skewness statistics in the notebook.</li> </ul> </li> <li><p>Categorical Features (<code>from</code>, <code>to</code>, <code>flightType</code>, <code>agency</code>):</p> <ul> <li>Cardinality: <code>from</code> (9 unique cities), <code>to</code> (9 unique cities), <code>flightType</code> (3 unique values: 'economic', 'premium', 'firstClass'), <code>agency</code> (3 unique values).</li> <li>Impact on <code>price</code>, <code>distance</code>, and <code>time</code> (Insights from Mean, Mode, and Sum Aggregations via Bivariate Plots in Notebook):<ul> <li><code>flightType</code>:<ul> <li>Price: The strongest differentiator.<ul> <li>Mean/Median Price: 'firstClass' (e.g., mean ~$1500 from plots) &gt; 'premium' (mean ~$900) &gt; 'economic' (mean ~$800).</li> <li>Mode Price: Similar stratification observed in mode price bar plots.</li> <li>Sum Price (Total Revenue): 'firstClass' generates the most total revenue, followed by 'premium' and 'economic', indicating its high value.</li> </ul> </li> <li>Distance/Time: 'firstClass' flights are associated with the highest mean, mode, and sum of distances and times, suggesting they are often longer-haul. 'economic' and 'premium' show similar total distance/time.</li> </ul> </li> <li><code>agency</code>:<ul> <li>Price:<ul> <li>Mean Price: 'FlyingDrops' shows the highest mean price, followed by 'CloudFy', then 'Rainbow'.</li> <li>Mode Price: 'FlyingDrops' shows higher common prices.</li> <li>Sum Price (Total Revenue): 'CloudFy' and 'Rainbow' are market leaders in total revenue, while 'FlyingDrops' has significantly less, suggesting a niche, higher-margin strategy for FlyingDrops.</li> </ul> </li> <li>Distance/Time: 'Rainbow' and 'CloudFy' cover the largest total distance and time. 'FlyingDrops' has the highest mean distance/time.</li> </ul> </li> <li><code>from</code> &amp; <code>to</code> (Routes):<ul> <li>Price:<ul> <li>Mean/Median Price: Flights involving Salvador (BH) and Florianopolis (SC) (as origin or destination) consistently show higher average prices. Sao Paulo (SP) and Natal (RN) are generally more affordable.</li> <li>Sum Price (Total Revenue): Florianopolis (SC) as a destination and origins like Aracaju (SE), Campo Grande (MS), Brasilia (DF) are among top revenue generators.</li> </ul> </li> <li>Distance/Time:<ul> <li>Mean/Median Distance/Time: Routes involving Salvador (BH) and Florianopolis (SC) show longer average distances/times.</li> <li>Sum Distance/Time: Florianopolis (SC) (origin and destination) and Aracaju (SE) account for high cumulative flight distances/times.</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> <li><p>Temporal Feature (<code>date</code> and extracted components):</p> <ul> <li>Range: Data spans from 2019-09-26 to 2023-07-24.</li> <li>Flight Volume Patterns (from Bar Plots &amp; <code>describe()</code> output):<ul> <li>Yearly: Peak flight volume observed in 2020 (55,095 flights in the deduplicated dataset).</li> <li>Monthly: Strong seasonality with the highest flight volumes in Q4 (October, November, December). August shows the lowest volume.</li> <li>Weekly: Thursday is the busiest day with 52,115 flights in the deduplicated dataset. Other days like Monday, Sunday, Friday, Saturday show comparable, lower volumes.</li> </ul> </li> <li>Average Price, Time, and Distance Trends (from Line Plots in Notebook):<ul> <li>Yearly Trends:<ul> <li>Average Price: Shows volatility, with a potential peak around 2020-2021, followed by adjustments.</li> <li>Average Time/Distance: Appear relatively stable across the years with minor fluctuations.</li> </ul> </li> <li>Monthly Trends (Seasonality):<ul> <li>Average Price: Clear seasonality; peaks during summer (e.g., June-August) and end-of-year (e.g., December). Lower in off-peak months (e.g., February, November).</li> <li>Average Time/Distance: Subtle seasonal pattern, slight increase during peak vacation periods.</li> </ul> </li> <li>Weekly Trends (Day-of-Week):<ul> <li>Average Price: Fluctuates, commonly higher on Fridays and Sundays. Mid-week often lower. Thursday shows moderate average prices despite high volume.</li> <li>Average Time/Distance: Variations less pronounced; slight tendency for longer average flights on weekends.</li> </ul> </li> </ul> </li> <li>Action: These strong temporal trends necessitate feature engineering.</li> </ul> </li> <li><p>Recommended Feature Engineering (based on notebook actions and EDA insights):</p> <ul> <li>Route Definition: Create a <code>route</code> feature by combining <code>from</code> and <code>to</code>.</li> <li>Temporal Features (Implemented &amp; Potential): <code>year</code>, <code>month</code>, <code>day</code>, <code>day_name</code> were extracted. Further consider  <code>is_weekend</code>, <code>is_thursday</code> flag, and cyclical encoding for <code>month</code> and <code>day_of_week</code>.</li> <li>Categorical Encoding:<ul> <li><code>flightType</code>, <code>agency</code>: One-hot encoding is suitable.</li> <li><code>route</code>, <code>from</code>, <code>to</code>: Consider target encoding (with careful cross-validation), frequency encoding, or embedding layers. Cyclic Encoding:</li> <li><code>month</code>, <code>day</code></li> </ul> </li> <li>Interaction Features: Explore interactions like <code>route_flightType</code>, <code>agency_month</code>, or <code>flightType_day_of_week</code>.</li> </ul> </li> </ul>"},{"location":"EDA/flights_eda/#4-outlier-detection-treatment","title":"4. \ud83d\udea8 Outlier Detection &amp; Treatment\u00b6","text":"<ul> <li>Observations:<ul> <li><code>price</code> exhibits right-skewness, with outliers on the higher end. Box plots for categorical features vs. <code>price</code> (e.g., <code>flightType</code> vs. <code>price</code> in the notebook) visually confirm that 'firstClass' and certain agencies/routes contribute more to these higher-priced outliers. <code>time</code> and <code>distance</code> are nearly symmetric and show fewer extreme outliers compared to <code>price</code>.</li> </ul> </li> <li>Actionable Strategies:<ul> <li>Detection: IQR method or Z-scores can be used.</li> <li>Treatment: For <code>price</code>, log transformation is the primary strategy suggested. For other features, if extreme outliers persist and affect sensitive models, consider capping. Tree-based models are generally more robust to outliers.</li> </ul> </li> </ul>"},{"location":"EDA/flights_eda/#5-data-leakage-bias-risks","title":"5. \ud83d\udd75\ufe0f Data Leakage &amp; Bias Risks\u00b6","text":"<ul> <li>Duplicate Data: Mitigated by thorough deduplication after removing non-predictive identifiers.</li> <li>Temporal Consistency: Critical: The notebook utilizes a <code>chronological_split</code> function. A time-based split for model training/validation/testing is essential to simulate real-world deployment and prevent temporal leakage, ensuring the model generalizes to future unseen data.</li> </ul>"},{"location":"EDA/flights_eda/#6-modeling-considerations-strategy","title":"6. \ud83e\udd16 Modeling Considerations &amp; Strategy\u00b6","text":"<ul> <li>Baseline Model: Tree-based ensembles like Random Forest Regressor or Gradient Boosting Regressor are good starting points due to their robustness to outliers and ability to capture non-linearities.</li> <li>Feature Importance &amp; Selection:<ul> <li>Primary Drivers: <code>flightType</code>, <code>route</code> (or <code>from</code>/<code>to</code>), <code>agency</code>, and temporal features (<code>month</code>, <code>day_of_week</code>, <code>year</code>) are expected to be highly important based on EDA.</li> <li>Secondary Drivers: <code>distance</code> (or <code>time</code>).</li> <li>Multicollinearity: Given the ~1.0 correlation between <code>distance</code> and <code>time</code>, use only one (e.g., <code>distance</code>) to avoid multicollinearity issues in some models.</li> </ul> </li> <li>Evaluation Metrics: Standard regression metrics like RMSE (Root Mean Squared Error), MAE (Mean Absolute Error), R\u00b2 (R-squared), and MAPE (Mean Absolute Percentage Error).</li> <li>Iterative Refinement: Plan for hyperparameter tuning, further feature importance analysis, and residual analysis to improve the model.</li> <li>Capturing Temporal Dynamics: The model must be structured to effectively learn the strong yearly, monthly, and weekly patterns observed in both flight volume and average price/time/distance.</li> </ul>"},{"location":"EDA/flights_eda/#summary-table-of-key-actions-strategies","title":"\ud83d\udccb Summary Table of Key Actions &amp; Strategies\u00b6","text":"Lifecycle Stage Key Action / Strategy Rationale / Detail Data Ingestion &amp; Cleaning Rigorous deduplication (45.02% duplicates removed after dropping identifiers); Implement robust missing value imputation in production pipeline. Prevent bias/leakage; Handle real-world data imperfections. Final dataset: 149,484 rows. Optimize data types (categorical, float32, uint types). Improve memory efficiency (10.3MB to 5.1MB) and processing speed. Target Variable Prep Apply log transformation to <code>price</code> (skewness ~0.53). Normalize distribution, stabilize variance, improve model performance for skewed target. Feature Engineering Create <code>route</code>; Extract comprehensive temporal features (<code>year</code>, <code>month</code>, <code>day</code>, <code>day_name</code>; consider cyclical, <code>is_thursday</code>). Capture key predictive signals from location, time, and observed anomalies (e.g., Thursday volume). Implement appropriate encoding for categorical features (one-hot, target encoding). Convert categorical data into a model-usable format effectively. Outlier Management Detect outliers (visualized via boxplots); Log-transform <code>price</code>; Consider capping for other features if needed. Mitigate undue influence of extreme values, especially for <code>price</code>. Data Splitting Strict time-based split (e.g., using <code>chronological_split</code>) for training, validation, and test sets. Prevent temporal data leakage and ensure realistic performance evaluation. Model Selection Start with tree-based ensemble models (Random Forest, Gradient Boosting). Robust to outliers, handle mixed data types well, capture non-linearities and interactions. Feature Selection Address multicollinearity: use <code>distance</code> OR <code>time</code> (corr ~1.0). Evaluate feature importances from baseline model. Avoid unstable coefficients, improve interpretability, focus on impactful features. Model Evaluation Use RMSE/MAE as primary metrics; R\u00b2 and MAPE as secondary. Quantify prediction accuracy and relative error. Iteration &amp; Deployment Iteratively refine features and hyperparameters; Monitor model performance over time post-deployment. Continuously improve model and adapt to changing data patterns (concept drift)."},{"location":"EDA/utils/__init__/","title":"init","text":""},{"location":"EDA/utils/data_splitting/","title":"Data splitting","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nfrom typing import Tuple, Dict\n</pre> import pandas as pd from typing import Tuple, Dict In\u00a0[\u00a0]: Copied! <pre>def chronological_split(\n    df: pd.DataFrame,\n    date_column: str,\n    target_column: str,\n    train_end: str,\n    val_end: str,\n    test_end: str,\n) -&gt; Dict[str, Tuple[pd.DataFrame, pd.Series]]:\n    \"\"\"\n    Chronologically splits a DataFrame into train, validation, test, and Holdout(Production simulation) sets.\n\n    Args:\n        df: Input DataFrame.\n        date_column: Name of the date column.\n        target_column: Name of the target column.\n        train_end: End date (inclusive) for training set.\n        val_end: End date (inclusive) for validation set.\n        test_end: End date (inclusive) for test set.\n\n    Returns:\n        Dictionary with keys 'train', 'validation', 'test', 'simulation', each mapping to (X, y).\n    \"\"\"\n    # Ensure datetime and sort\n    df = df.copy()\n    df[date_column] = pd.to_datetime(df[date_column], errors=\"raise\")\n    df = df.sort_values(date_column).reset_index(drop=True)\n\n    # Convert split dates\n    train_end = pd.to_datetime(train_end)\n    val_end = pd.to_datetime(val_end)\n    test_end = pd.to_datetime(test_end)\n\n    # Masks\n    train_mask = df[date_column] &lt;= train_end\n    val_mask = (df[date_column] &gt; train_end) &amp; (df[date_column] &lt;= val_end)\n    test_mask = (df[date_column] &gt; val_end) &amp; (df[date_column] &lt;= test_end)\n    sim_mask = df[date_column] &gt; test_end\n\n    splits = {\n        \"train\": df[train_mask],\n        \"validation\": df[val_mask],\n        \"test\": df[test_mask],\n        \"holdout\": df[sim_mask],\n    }\n\n    # Prepare (X, y) tuples\n    result = {}\n    for name, subset in splits.items():\n        X = subset.drop(columns=[target_column])\n        y = subset[target_column]\n        result[name] = (X, y)\n        if subset.empty:\n            print(f\"Warning: '{name}' set is empty for the given date range.\")\n\n    return result\n</pre> def chronological_split(     df: pd.DataFrame,     date_column: str,     target_column: str,     train_end: str,     val_end: str,     test_end: str, ) -&gt; Dict[str, Tuple[pd.DataFrame, pd.Series]]:     \"\"\"     Chronologically splits a DataFrame into train, validation, test, and Holdout(Production simulation) sets.      Args:         df: Input DataFrame.         date_column: Name of the date column.         target_column: Name of the target column.         train_end: End date (inclusive) for training set.         val_end: End date (inclusive) for validation set.         test_end: End date (inclusive) for test set.      Returns:         Dictionary with keys 'train', 'validation', 'test', 'simulation', each mapping to (X, y).     \"\"\"     # Ensure datetime and sort     df = df.copy()     df[date_column] = pd.to_datetime(df[date_column], errors=\"raise\")     df = df.sort_values(date_column).reset_index(drop=True)      # Convert split dates     train_end = pd.to_datetime(train_end)     val_end = pd.to_datetime(val_end)     test_end = pd.to_datetime(test_end)      # Masks     train_mask = df[date_column] &lt;= train_end     val_mask = (df[date_column] &gt; train_end) &amp; (df[date_column] &lt;= val_end)     test_mask = (df[date_column] &gt; val_end) &amp; (df[date_column] &lt;= test_end)     sim_mask = df[date_column] &gt; test_end      splits = {         \"train\": df[train_mask],         \"validation\": df[val_mask],         \"test\": df[test_mask],         \"holdout\": df[sim_mask],     }      # Prepare (X, y) tuples     result = {}     for name, subset in splits.items():         X = subset.drop(columns=[target_column])         y = subset[target_column]         result[name] = (X, y)         if subset.empty:             print(f\"Warning: '{name}' set is empty for the given date range.\")      return result"},{"location":"EDA/utils/data_utils/","title":"Data utils","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nfrom ydata_profiling import ProfileReport\nfrom ydata_profiling.config import Settings\nimport numpy as np\nimport warnings\n</pre> import pandas as pd from ydata_profiling import ProfileReport from ydata_profiling.config import Settings import numpy as np import warnings In\u00a0[\u00a0]: Copied! <pre>def check_duplicates(df):\n    \"\"\"\n    Check for duplicate rows in the DataFrame and print the percentage of duplicates.\n\n    Parameters:\n        df (pd.DataFrame): Input DataFrame.\n\n    Returns:\n        None\n    \"\"\"\n    duplicate_rows = df.duplicated().sum()\n    duplicate_percentage = duplicate_rows / len(df) * 100\n    print(f\"Percentage of rows involved in duplication: {duplicate_percentage:.2f}%\")\n</pre> def check_duplicates(df):     \"\"\"     Check for duplicate rows in the DataFrame and print the percentage of duplicates.      Parameters:         df (pd.DataFrame): Input DataFrame.      Returns:         None     \"\"\"     duplicate_rows = df.duplicated().sum()     duplicate_percentage = duplicate_rows / len(df) * 100     print(f\"Percentage of rows involved in duplication: {duplicate_percentage:.2f}%\") In\u00a0[\u00a0]: Copied! <pre>def generate_eda_report(\n    df,\n    report_title=\"EDA Report\",\n    save_path=\"eda_report.html\",\n    minimal=True,\n    explorative=False,\n):\n    \"\"\"\n    Generate and save an automated EDA report for the given DataFrame.\n\n    Parameters:\n        df (pd.DataFrame): DataFrame to analyze.\n        report_title (str): Title of the report.\n        save_path (str): File path to save the HTML report.\n        minimal (bool): Whether to generate a minimal report.\n        explorative (bool): Whether to generate an explorative report.\n\n    Returns:\n        None\n    \"\"\"\n    try:\n        profile = ProfileReport(\n            df, title=report_title, explorative=explorative, minimal=minimal\n        )\n    except Exception as e:\n        print(f\"Error generating EDA report: {e}\")\n        return\n\n    try:\n        Settings().progress_bar = False  # Disable multiple progress bars\n        profile.to_file(save_path)\n        print(f\"EDA report saved to {save_path}\")\n    except Exception as e:\n        print(f\"Error saving EDA report: {e}\")\n</pre> def generate_eda_report(     df,     report_title=\"EDA Report\",     save_path=\"eda_report.html\",     minimal=True,     explorative=False, ):     \"\"\"     Generate and save an automated EDA report for the given DataFrame.      Parameters:         df (pd.DataFrame): DataFrame to analyze.         report_title (str): Title of the report.         save_path (str): File path to save the HTML report.         minimal (bool): Whether to generate a minimal report.         explorative (bool): Whether to generate an explorative report.      Returns:         None     \"\"\"     try:         profile = ProfileReport(             df, title=report_title, explorative=explorative, minimal=minimal         )     except Exception as e:         print(f\"Error generating EDA report: {e}\")         return      try:         Settings().progress_bar = False  # Disable multiple progress bars         profile.to_file(save_path)         print(f\"EDA report saved to {save_path}\")     except Exception as e:         print(f\"Error saving EDA report: {e}\") In\u00a0[\u00a0]: Copied! <pre>def get_date_stats(date_series, series_name=\"Date Column\"):\n    \"\"\"\n    Compute and print basic statistics for a date column.\n\n    Parameters:\n        date_series (pd.Series): Series containing date values.\n        series_name (str): Name of the series for display.\n\n    Returns:\n        None\n    \"\"\"\n    dates = pd.to_datetime(date_series, errors=\"coerce\").dropna()\n    if dates.empty:\n        print(f\"No valid dates found in {series_name}.\")\n        return\n\n    stats = {\n        \"min_date\": dates.min(),\n        \"max_date\": dates.max(),\n        \"time_span\": dates.max() - dates.min(),\n        \"unique_days\": dates.nunique(),\n        \"year_counts\": dates.dt.year.value_counts().sort_index(),\n        \"month_counts\": dates.dt.month.value_counts().sort_index(),\n        \"unique_year_months\": dates.dt.to_period(\"M\").nunique(),\n    }\n\n    print(f\"--- Date Stats for: {series_name} ---\")\n    print(f\"Min date: {stats['min_date'].date()}\")\n    print(f\"Max date: {stats['max_date'].date()}\")\n    print(f\"Time span: {stats['time_span']}\")\n    print(f\"Unique days: {stats['unique_days']}\")\n    print(\"\\nYear counts:\\n\", stats[\"year_counts\"])\n    print(\"\\nMonth counts:\\n\", stats[\"month_counts\"])\n    print(f\"\\nUnique year-months: {stats['unique_year_months']}\")\n    print(\"--- End of Stats ---\")\n</pre> def get_date_stats(date_series, series_name=\"Date Column\"):     \"\"\"     Compute and print basic statistics for a date column.      Parameters:         date_series (pd.Series): Series containing date values.         series_name (str): Name of the series for display.      Returns:         None     \"\"\"     dates = pd.to_datetime(date_series, errors=\"coerce\").dropna()     if dates.empty:         print(f\"No valid dates found in {series_name}.\")         return      stats = {         \"min_date\": dates.min(),         \"max_date\": dates.max(),         \"time_span\": dates.max() - dates.min(),         \"unique_days\": dates.nunique(),         \"year_counts\": dates.dt.year.value_counts().sort_index(),         \"month_counts\": dates.dt.month.value_counts().sort_index(),         \"unique_year_months\": dates.dt.to_period(\"M\").nunique(),     }      print(f\"--- Date Stats for: {series_name} ---\")     print(f\"Min date: {stats['min_date'].date()}\")     print(f\"Max date: {stats['max_date'].date()}\")     print(f\"Time span: {stats['time_span']}\")     print(f\"Unique days: {stats['unique_days']}\")     print(\"\\nYear counts:\\n\", stats[\"year_counts\"])     print(\"\\nMonth counts:\\n\", stats[\"month_counts\"])     print(f\"\\nUnique year-months: {stats['unique_year_months']}\")     print(\"--- End of Stats ---\") In\u00a0[\u00a0]: Copied! <pre>def count_rows_between_dates(df, date_col, start_date, end_date):\n    \"\"\"\n    Count rows in a DataFrame where date_col is between start_date and end_date (inclusive).\n\n    Parameters:\n        df (pd.DataFrame): Input DataFrame.\n        date_col (str): Name of the date column.\n        start_date (str or datetime): Start date.\n        end_date (str or datetime): End date.\n\n    Returns:\n        int: Number of rows in the date range.\n    \"\"\"\n    df = df.copy()\n    df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n    mask = (df[date_col] &gt;= pd.to_datetime(start_date)) &amp; (\n        df[date_col] &lt;= pd.to_datetime(end_date)\n    )\n    count = mask.sum()\n    total = len(df)\n    percent = (count / total) * 100 if total &gt; 0 else 0\n    print(\n        f\"Rows between {start_date} and {end_date}: {count} ({percent:.2f}% of total)\"\n    )\n    return count\n</pre> def count_rows_between_dates(df, date_col, start_date, end_date):     \"\"\"     Count rows in a DataFrame where date_col is between start_date and end_date (inclusive).      Parameters:         df (pd.DataFrame): Input DataFrame.         date_col (str): Name of the date column.         start_date (str or datetime): Start date.         end_date (str or datetime): End date.      Returns:         int: Number of rows in the date range.     \"\"\"     df = df.copy()     df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")     mask = (df[date_col] &gt;= pd.to_datetime(start_date)) &amp; (         df[date_col] &lt;= pd.to_datetime(end_date)     )     count = mask.sum()     total = len(df)     percent = (count / total) * 100 if total &gt; 0 else 0     print(         f\"Rows between {start_date} and {end_date}: {count} ({percent:.2f}% of total)\"     )     return count In\u00a0[\u00a0]: Copied! <pre>def check_missing(df):\n    \"\"\"\n    Check for missing values in the DataFrame.\n\n    Parameters:\n        df (pd.DataFrame): Input DataFrame.\n\n    Returns:\n        pd.DataFrame: DataFrame with counts and percentages of missing values per column.\n    \"\"\"\n    missing = df.isnull().sum()\n    missing_percentage = (missing / len(df)) * 100\n    missing_df = pd.DataFrame(\n        {\"Missing Values\": missing, \"Percentage\": missing_percentage}\n    )\n    return missing_df\n</pre> def check_missing(df):     \"\"\"     Check for missing values in the DataFrame.      Parameters:         df (pd.DataFrame): Input DataFrame.      Returns:         pd.DataFrame: DataFrame with counts and percentages of missing values per column.     \"\"\"     missing = df.isnull().sum()     missing_percentage = (missing / len(df)) * 100     missing_df = pd.DataFrame(         {\"Missing Values\": missing, \"Percentage\": missing_percentage}     )     return missing_df In\u00a0[\u00a0]: Copied! <pre>def optimize_dtypes(df, category_threshold=0.5, datetime_threshold=0.8):\n    \"\"\"\n    Optimize DataFrame column data types to reduce memory usage.\n\n    Parameters:\n        df (pd.DataFrame): Input DataFrame.\n        category_threshold (float): Max ratio of unique/total values to convert object to category.\n        datetime_threshold (float): Min ratio of valid datetimes to convert object to datetime.\n\n    Returns:\n        pd.DataFrame: Copy of DataFrame with optimized data types.\n    \"\"\"\n    df_optimized = df.copy()\n\n    # Optimize integer columns\n    int_columns = df_optimized.select_dtypes(include=[np.integer]).columns\n    for col in int_columns:\n        if df_optimized[col].isnull().any():\n            continue\n        col_min, col_max = df_optimized[col].min(), df_optimized[col].max()\n        if col_min &gt;= 0:\n            if col_max &lt;= np.iinfo(np.uint8).max:\n                df_optimized[col] = df_optimized[col].astype(np.uint8)\n            elif col_max &lt;= np.iinfo(np.uint16).max:\n                df_optimized[col] = df_optimized[col].astype(np.uint16)\n            elif col_max &lt;= np.iinfo(np.uint32).max:\n                df_optimized[col] = df_optimized[col].astype(np.uint32)\n            else:\n                df_optimized[col] = df_optimized[col].astype(np.uint64)\n        else:\n            if col_min &gt;= np.iinfo(np.int8).min and col_max &lt;= np.iinfo(np.int8).max:\n                df_optimized[col] = df_optimized[col].astype(np.int8)\n            elif (\n                col_min &gt;= np.iinfo(np.int16).min and col_max &lt;= np.iinfo(np.int16).max\n            ):\n                df_optimized[col] = df_optimized[col].astype(np.int16)\n            elif (\n                col_min &gt;= np.iinfo(np.int32).min and col_max &lt;= np.iinfo(np.int32).max\n            ):\n                df_optimized[col] = df_optimized[col].astype(np.int32)\n            else:\n                df_optimized[col] = df_optimized[col].astype(np.int64)\n\n    # Optimize float columns\n    float_columns = df_optimized.select_dtypes(include=[np.floating]).columns\n    for col in float_columns:\n        df_optimized[col] = pd.to_numeric(df_optimized[col], downcast=\"float\")\n\n    # Optimize object columns (categorical + datetime)\n    object_columns = df_optimized.select_dtypes(include=[\"object\"]).columns\n    for col in object_columns:\n        num_unique = df_optimized[col].nunique(dropna=False)\n        num_total = len(df_optimized[col])\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", category=UserWarning)\n            try:\n                converted = pd.to_datetime(df_optimized[col], errors=\"coerce\")\n                valid_count = converted.notna().sum()\n                if valid_count / num_total &gt;= datetime_threshold:\n                    df_optimized[col] = converted\n                    continue\n            except Exception:\n                pass\n\n        if num_unique / num_total &lt; category_threshold:\n            try:\n                df_optimized[col] = df_optimized[col].astype(\"category\")\n            except Exception:\n                continue\n\n    return df_optimized\n</pre> def optimize_dtypes(df, category_threshold=0.5, datetime_threshold=0.8):     \"\"\"     Optimize DataFrame column data types to reduce memory usage.      Parameters:         df (pd.DataFrame): Input DataFrame.         category_threshold (float): Max ratio of unique/total values to convert object to category.         datetime_threshold (float): Min ratio of valid datetimes to convert object to datetime.      Returns:         pd.DataFrame: Copy of DataFrame with optimized data types.     \"\"\"     df_optimized = df.copy()      # Optimize integer columns     int_columns = df_optimized.select_dtypes(include=[np.integer]).columns     for col in int_columns:         if df_optimized[col].isnull().any():             continue         col_min, col_max = df_optimized[col].min(), df_optimized[col].max()         if col_min &gt;= 0:             if col_max &lt;= np.iinfo(np.uint8).max:                 df_optimized[col] = df_optimized[col].astype(np.uint8)             elif col_max &lt;= np.iinfo(np.uint16).max:                 df_optimized[col] = df_optimized[col].astype(np.uint16)             elif col_max &lt;= np.iinfo(np.uint32).max:                 df_optimized[col] = df_optimized[col].astype(np.uint32)             else:                 df_optimized[col] = df_optimized[col].astype(np.uint64)         else:             if col_min &gt;= np.iinfo(np.int8).min and col_max &lt;= np.iinfo(np.int8).max:                 df_optimized[col] = df_optimized[col].astype(np.int8)             elif (                 col_min &gt;= np.iinfo(np.int16).min and col_max &lt;= np.iinfo(np.int16).max             ):                 df_optimized[col] = df_optimized[col].astype(np.int16)             elif (                 col_min &gt;= np.iinfo(np.int32).min and col_max &lt;= np.iinfo(np.int32).max             ):                 df_optimized[col] = df_optimized[col].astype(np.int32)             else:                 df_optimized[col] = df_optimized[col].astype(np.int64)      # Optimize float columns     float_columns = df_optimized.select_dtypes(include=[np.floating]).columns     for col in float_columns:         df_optimized[col] = pd.to_numeric(df_optimized[col], downcast=\"float\")      # Optimize object columns (categorical + datetime)     object_columns = df_optimized.select_dtypes(include=[\"object\"]).columns     for col in object_columns:         num_unique = df_optimized[col].nunique(dropna=False)         num_total = len(df_optimized[col])          with warnings.catch_warnings():             warnings.simplefilter(\"ignore\", category=UserWarning)             try:                 converted = pd.to_datetime(df_optimized[col], errors=\"coerce\")                 valid_count = converted.notna().sum()                 if valid_count / num_total &gt;= datetime_threshold:                     df_optimized[col] = converted                     continue             except Exception:                 pass          if num_unique / num_total &lt; category_threshold:             try:                 df_optimized[col] = df_optimized[col].astype(\"category\")             except Exception:                 continue      return df_optimized In\u00a0[\u00a0]: Copied! <pre>def skewness(df):\n    \"\"\"\n    Calculate skewness for each numerical column in the DataFrame.\n\n    Parameters:\n        df (pd.DataFrame): Input DataFrame.\n\n    Returns:\n        pd.DataFrame: DataFrame with columns and their skewness values.\n    \"\"\"\n    skewness_df = df.skew().reset_index().rename(columns={\"index\": \"Column\", 0: \"Skew\"})\n    return skewness_df\n</pre> def skewness(df):     \"\"\"     Calculate skewness for each numerical column in the DataFrame.      Parameters:         df (pd.DataFrame): Input DataFrame.      Returns:         pd.DataFrame: DataFrame with columns and their skewness values.     \"\"\"     skewness_df = df.skew().reset_index().rename(columns={\"index\": \"Column\", 0: \"Skew\"})     return skewness_df"},{"location":"EDA/utils/plots_utils/","title":"Plots utils","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport seaborn as sns\n</pre> import pandas as pd import matplotlib.pyplot as plt import os import seaborn as sns In\u00a0[\u00a0]: Copied! <pre>def plot_flights_per_year(date_series, series_name=\"Date Column\"):\n    \"\"\"\n    Plot the number of flights per year from a date series.\n\n    Parameters:\n        date_series (pd.Series): Series containing date values.\n        series_name (str): Name for labeling the plot.\n    \"\"\"\n    dates = pd.to_datetime(date_series, errors=\"coerce\").dropna()\n    if dates.empty:\n        print(f\"No valid dates in {series_name}.\")\n        return\n    year_counts = dates.dt.year.value_counts().sort_index()\n    plt.figure(figsize=(8, 5))\n    year_counts.plot(kind=\"bar\", color=\"skyblue\", edgecolor=\"black\")\n    plt.title(f\"Flights per Year - {series_name}\", fontsize=14)\n    plt.xlabel(\"Year\")\n    plt.ylabel(\"Number of Flights\")\n    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n    plt.tight_layout()\n    plt.show()\n</pre> def plot_flights_per_year(date_series, series_name=\"Date Column\"):     \"\"\"     Plot the number of flights per year from a date series.      Parameters:         date_series (pd.Series): Series containing date values.         series_name (str): Name for labeling the plot.     \"\"\"     dates = pd.to_datetime(date_series, errors=\"coerce\").dropna()     if dates.empty:         print(f\"No valid dates in {series_name}.\")         return     year_counts = dates.dt.year.value_counts().sort_index()     plt.figure(figsize=(8, 5))     year_counts.plot(kind=\"bar\", color=\"skyblue\", edgecolor=\"black\")     plt.title(f\"Flights per Year - {series_name}\", fontsize=14)     plt.xlabel(\"Year\")     plt.ylabel(\"Number of Flights\")     plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)     plt.tight_layout()     plt.show() In\u00a0[\u00a0]: Copied! <pre>def plot_flights_per_month(date_series, series_name=\"Date Column\"):\n    \"\"\"\n    Plot the number of flights per month aggregated over all years.\n\n    Parameters:\n        date_series (pd.Series): Series containing date values.\n        series_name (str): Name for labeling the plot.\n    \"\"\"\n    dates = pd.to_datetime(date_series, errors=\"coerce\").dropna()\n    if dates.empty:\n        print(f\"No valid dates in {series_name}.\")\n        return\n    month_counts = dates.dt.month.value_counts().sort_index()\n    month_names = pd.to_datetime(month_counts.index, format=\"%m\").month_name().str[:3]\n    plt.figure(figsize=(10, 5))\n    plt.bar(month_names, month_counts.values, color=\"coral\", edgecolor=\"black\")\n    plt.title(f\"Flights per Month (All Years) - {series_name}\", fontsize=14)\n    plt.xlabel(\"Month\")\n    plt.ylabel(\"Number of Flights\")\n    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n    plt.tight_layout()\n    plt.show()\n</pre> def plot_flights_per_month(date_series, series_name=\"Date Column\"):     \"\"\"     Plot the number of flights per month aggregated over all years.      Parameters:         date_series (pd.Series): Series containing date values.         series_name (str): Name for labeling the plot.     \"\"\"     dates = pd.to_datetime(date_series, errors=\"coerce\").dropna()     if dates.empty:         print(f\"No valid dates in {series_name}.\")         return     month_counts = dates.dt.month.value_counts().sort_index()     month_names = pd.to_datetime(month_counts.index, format=\"%m\").month_name().str[:3]     plt.figure(figsize=(10, 5))     plt.bar(month_names, month_counts.values, color=\"coral\", edgecolor=\"black\")     plt.title(f\"Flights per Month (All Years) - {series_name}\", fontsize=14)     plt.xlabel(\"Month\")     plt.ylabel(\"Number of Flights\")     plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)     plt.tight_layout()     plt.show() In\u00a0[\u00a0]: Copied! <pre>def histograms(df, cols, color=\"#0060ff\", save_dir=None):\n    \"\"\"\n    Display histograms with KDE for specified numerical columns.\n\n    Parameters:\n        df (pd.DataFrame): DataFrame containing data.\n        cols (list): List of column names to plot.\n        color (str): Color for the histogram.\n        save_dir (str or None): Directory to save plots; if None, plots are not saved.\n    \"\"\"\n    if save_dir is not None:\n        os.makedirs(save_dir, exist_ok=True)\n\n    for col in cols:\n        if col not in df.columns:\n            print(f\"Column '{col}' not found in DataFrame. Skipping.\")\n            continue\n        if not pd.api.types.is_numeric_dtype(df[col]):\n            print(f\"Column '{col}' is not numeric. Skipping.\")\n            continue\n\n        plt.figure(figsize=(10, 6))\n        sns.histplot(\n            df[col].dropna(),\n            bins=40,\n            kde=True,\n            color=color,\n            edgecolor=\"black\",\n            linewidth=1,\n            alpha=0.7,\n        )\n        plt.title(f\"Distribution of {col}\", fontsize=16, fontweight=\"bold\")\n        plt.xlabel(col, fontsize=14)\n        plt.ylabel(\"Frequency\", fontsize=14)\n        plt.grid(True, alpha=0.3)\n        plt.tight_layout()\n\n        if save_dir is not None:\n            filename = os.path.join(save_dir, f\"{col}.png\")\n            plt.savefig(filename, dpi=300, bbox_inches=\"tight\")\n            print(f\"Saved plot for '{col}' to {filename}\")\n\n        plt.show()\n        plt.close()\n</pre> def histograms(df, cols, color=\"#0060ff\", save_dir=None):     \"\"\"     Display histograms with KDE for specified numerical columns.      Parameters:         df (pd.DataFrame): DataFrame containing data.         cols (list): List of column names to plot.         color (str): Color for the histogram.         save_dir (str or None): Directory to save plots; if None, plots are not saved.     \"\"\"     if save_dir is not None:         os.makedirs(save_dir, exist_ok=True)      for col in cols:         if col not in df.columns:             print(f\"Column '{col}' not found in DataFrame. Skipping.\")             continue         if not pd.api.types.is_numeric_dtype(df[col]):             print(f\"Column '{col}' is not numeric. Skipping.\")             continue          plt.figure(figsize=(10, 6))         sns.histplot(             df[col].dropna(),             bins=40,             kde=True,             color=color,             edgecolor=\"black\",             linewidth=1,             alpha=0.7,         )         plt.title(f\"Distribution of {col}\", fontsize=16, fontweight=\"bold\")         plt.xlabel(col, fontsize=14)         plt.ylabel(\"Frequency\", fontsize=14)         plt.grid(True, alpha=0.3)         plt.tight_layout()          if save_dir is not None:             filename = os.path.join(save_dir, f\"{col}.png\")             plt.savefig(filename, dpi=300, bbox_inches=\"tight\")             print(f\"Saved plot for '{col}' to {filename}\")          plt.show()         plt.close() In\u00a0[\u00a0]: Copied! <pre>def boxplots(df, cols, color=\"#0060ff\", save_dir=None):\n    \"\"\"\n    Plot boxplots for numerical columns with statistical annotations.\n\n    Parameters:\n        df (pd.DataFrame): DataFrame containing data.\n        cols (list): List of numerical columns to plot.\n        color (str): Color for the boxplots.\n        save_dir (str or None): Directory to save plots; if None, plots are not saved.\n    \"\"\"\n    sns.set_theme(style=\"darkgrid\", font_scale=1.2)\n    if save_dir is not None:\n        save_dir = os.path.join(save_dir, \"flights_boxplots\")\n        os.makedirs(save_dir, exist_ok=True)\n\n    for col in cols:\n        if col not in df.columns:\n            print(f\"Column '{col}' not found in DataFrame. Skipping.\")\n            continue\n        if not pd.api.types.is_numeric_dtype(df[col]):\n            print(f\"Column '{col}' is not numeric. Skipping.\")\n            continue\n\n        data = df[col].dropna()\n        q1, q3 = data.quantile(0.25), data.quantile(0.75)\n        median = data.median()\n        iqr = q3 - q1\n        min_val, max_val = data.min(), data.max()\n        lower_bound, upper_bound = q1 - 1.5 * iqr, q3 + 1.5 * iqr\n\n        plt.figure(figsize=(10, 6))\n        ax = sns.boxplot(\n            x=data,\n            color=color,\n            linewidth=1.5,\n            fliersize=5,\n            boxprops=dict(alpha=0.7, edgecolor=\"black\"),\n            medianprops=dict(color=\"#ff6600\", linewidth=2),\n        )\n        plt.title(f\"Box Plot of {col}\", fontsize=16, fontweight=\"bold\")\n        plt.xlabel(col, fontsize=14)\n        plt.ylabel(\"Value\", fontsize=14)\n        plt.grid(True, alpha=0.3)\n        plt.tight_layout()\n\n        y = 0  # y-position for annotation (horizontal boxplot)\n\n        stats = [\n            (median, y + 0.15, f\"Median: {median:.2f}\", \"#ff6600\", 13, \"bold\"),\n            (q1, y - 0.18, f\"Q1: {q1:.2f}\", \"#ff6600\", 12, \"normal\"),\n            (q3, y - 0.18, f\"Q3: {q3:.2f}\", \"#ff6600\", 12, \"normal\"),\n            ((q1 + q3) / 2, y + 0.22, f\"IQR: {iqr:.2f}\", \"#6D597A\", 12, \"normal\"),\n            (min_val, y + 0.12, f\"Min: {min_val:.2f}\", \"#43AA8B\", 11, \"normal\"),\n            (max_val, y + 0.12, f\"Max: {max_val:.2f}\", \"#F94144\", 11, \"normal\"),\n            (\n                lower_bound,\n                y - 0.32,\n                f\"Lower Outlier Bound\\n(Q1 - 1.5\u00d7IQR): {lower_bound:.2f}\",\n                \"#4361EE\",\n                11,\n                \"normal\",\n            ),\n            (\n                upper_bound,\n                y - 0.32,\n                f\"Upper Outlier Bound\\n(Q3 + 1.5\u00d7IQR): {upper_bound:.2f}\",\n                \"#F3722C\",\n                11,\n                \"normal\",\n            ),\n        ]\n        for x, y_pos, text, color, size, weight in stats:\n            ax.annotate(\n                text,\n                xy=(x, y),\n                xytext=(x, y_pos),\n                textcoords=\"data\",\n                ha=\"center\",\n                color=color,\n                fontsize=size,\n                fontweight=weight,\n                arrowprops=dict(arrowstyle=\"-&gt;\", color=color)\n                if \"Bound\" not in text and \"IQR\" not in text\n                else None,\n                bbox=dict(\n                    boxstyle=\"round,pad=0.2\", fc=\"#e0e0e0\", ec=\"#bbbbbb\", alpha=0.7\n                )\n                if \"IQR\" in text\n                else None,\n            )\n\n        if save_dir is not None:\n            filename = os.path.join(save_dir, f\"{col}.png\")\n            plt.savefig(filename, dpi=300, bbox_inches=\"tight\")\n            print(f\"Saved box plot for '{col}' to {filename}\")\n\n        plt.show()\n        plt.close()\n</pre> def boxplots(df, cols, color=\"#0060ff\", save_dir=None):     \"\"\"     Plot boxplots for numerical columns with statistical annotations.      Parameters:         df (pd.DataFrame): DataFrame containing data.         cols (list): List of numerical columns to plot.         color (str): Color for the boxplots.         save_dir (str or None): Directory to save plots; if None, plots are not saved.     \"\"\"     sns.set_theme(style=\"darkgrid\", font_scale=1.2)     if save_dir is not None:         save_dir = os.path.join(save_dir, \"flights_boxplots\")         os.makedirs(save_dir, exist_ok=True)      for col in cols:         if col not in df.columns:             print(f\"Column '{col}' not found in DataFrame. Skipping.\")             continue         if not pd.api.types.is_numeric_dtype(df[col]):             print(f\"Column '{col}' is not numeric. Skipping.\")             continue          data = df[col].dropna()         q1, q3 = data.quantile(0.25), data.quantile(0.75)         median = data.median()         iqr = q3 - q1         min_val, max_val = data.min(), data.max()         lower_bound, upper_bound = q1 - 1.5 * iqr, q3 + 1.5 * iqr          plt.figure(figsize=(10, 6))         ax = sns.boxplot(             x=data,             color=color,             linewidth=1.5,             fliersize=5,             boxprops=dict(alpha=0.7, edgecolor=\"black\"),             medianprops=dict(color=\"#ff6600\", linewidth=2),         )         plt.title(f\"Box Plot of {col}\", fontsize=16, fontweight=\"bold\")         plt.xlabel(col, fontsize=14)         plt.ylabel(\"Value\", fontsize=14)         plt.grid(True, alpha=0.3)         plt.tight_layout()          y = 0  # y-position for annotation (horizontal boxplot)          stats = [             (median, y + 0.15, f\"Median: {median:.2f}\", \"#ff6600\", 13, \"bold\"),             (q1, y - 0.18, f\"Q1: {q1:.2f}\", \"#ff6600\", 12, \"normal\"),             (q3, y - 0.18, f\"Q3: {q3:.2f}\", \"#ff6600\", 12, \"normal\"),             ((q1 + q3) / 2, y + 0.22, f\"IQR: {iqr:.2f}\", \"#6D597A\", 12, \"normal\"),             (min_val, y + 0.12, f\"Min: {min_val:.2f}\", \"#43AA8B\", 11, \"normal\"),             (max_val, y + 0.12, f\"Max: {max_val:.2f}\", \"#F94144\", 11, \"normal\"),             (                 lower_bound,                 y - 0.32,                 f\"Lower Outlier Bound\\n(Q1 - 1.5\u00d7IQR): {lower_bound:.2f}\",                 \"#4361EE\",                 11,                 \"normal\",             ),             (                 upper_bound,                 y - 0.32,                 f\"Upper Outlier Bound\\n(Q3 + 1.5\u00d7IQR): {upper_bound:.2f}\",                 \"#F3722C\",                 11,                 \"normal\",             ),         ]         for x, y_pos, text, color, size, weight in stats:             ax.annotate(                 text,                 xy=(x, y),                 xytext=(x, y_pos),                 textcoords=\"data\",                 ha=\"center\",                 color=color,                 fontsize=size,                 fontweight=weight,                 arrowprops=dict(arrowstyle=\"-&gt;\", color=color)                 if \"Bound\" not in text and \"IQR\" not in text                 else None,                 bbox=dict(                     boxstyle=\"round,pad=0.2\", fc=\"#e0e0e0\", ec=\"#bbbbbb\", alpha=0.7                 )                 if \"IQR\" in text                 else None,             )          if save_dir is not None:             filename = os.path.join(save_dir, f\"{col}.png\")             plt.savefig(filename, dpi=300, bbox_inches=\"tight\")             print(f\"Saved box plot for '{col}' to {filename}\")          plt.show()         plt.close() In\u00a0[\u00a0]: Copied! <pre>def barplot_univariate(df, cols, top_n=None, color=\"#0060ff\", sort=None, save_dir=None):\n    \"\"\"\n    Plot bar graphs of value counts for categorical columns with frequency annotations.\n\n    Parameters:\n        df (pd.DataFrame): DataFrame containing data.\n        cols (list): List of categorical columns to plot.\n        top_n (int or None): Number of top categories to show.\n        color (str): Bar color.\n        sort (str or None): 'asc', 'desc', or None for sorting.\n        save_dir (str or None): Directory to save plots; if None, plots are not saved.\n    \"\"\"\n    sns.set_theme(style=\"darkgrid\", font_scale=1.2)\n    if save_dir is not None:\n        save_dir = os.path.join(save_dir, \"flights_barplots\")\n        os.makedirs(save_dir, exist_ok=True)\n\n    for col in cols:\n        if col not in df.columns:\n            print(f\"Column '{col}' not found in DataFrame. Skipping.\")\n            continue\n        if not pd.api.types.is_categorical_dtype(\n            df[col]\n        ) and not pd.api.types.is_object_dtype(df[col]):\n            print(f\"Column '{col}' is not categorical. Skipping.\")\n            continue\n\n        counts = df[col].value_counts(dropna=False)\n        if sort == \"asc\":\n            counts = counts.sort_values(ascending=True)\n        elif sort == \"desc\":\n            counts = counts.sort_values(ascending=False)\n        else:\n            counts = counts.sort_index()\n        if top_n:\n            counts = counts.head(top_n)\n\n        plt.figure(figsize=(12, 6))\n        ax = sns.barplot(x=counts.index.astype(str), y=counts.values, color=color)\n        plt.title(f\"Frequency of {col}\", fontsize=18, fontweight=\"bold\")\n        plt.xlabel(col, fontsize=14)\n        plt.ylabel(\"Count\", fontsize=14)\n        plt.xticks(rotation=45, ha=\"right\")\n\n        for p in ax.patches:\n            ax.annotate(\n                f\"{int(p.get_height())}\",\n                (p.get_x() + p.get_width() / 2.0, p.get_height()),\n                ha=\"center\",\n                va=\"bottom\",\n                fontsize=12,\n                color=\"black\",\n                fontweight=\"bold\",\n                xytext=(0, 3),\n                textcoords=\"offset points\",\n            )\n        plt.tight_layout()\n\n        if save_dir is not None:\n            filename = os.path.join(save_dir, f\"barplot_flights_{col}.png\")\n            plt.savefig(filename, dpi=300, bbox_inches=\"tight\")\n            print(f\"Saved plot for '{col}' to {filename}\")\n\n        plt.show()\n        plt.close()\n</pre> def barplot_univariate(df, cols, top_n=None, color=\"#0060ff\", sort=None, save_dir=None):     \"\"\"     Plot bar graphs of value counts for categorical columns with frequency annotations.      Parameters:         df (pd.DataFrame): DataFrame containing data.         cols (list): List of categorical columns to plot.         top_n (int or None): Number of top categories to show.         color (str): Bar color.         sort (str or None): 'asc', 'desc', or None for sorting.         save_dir (str or None): Directory to save plots; if None, plots are not saved.     \"\"\"     sns.set_theme(style=\"darkgrid\", font_scale=1.2)     if save_dir is not None:         save_dir = os.path.join(save_dir, \"flights_barplots\")         os.makedirs(save_dir, exist_ok=True)      for col in cols:         if col not in df.columns:             print(f\"Column '{col}' not found in DataFrame. Skipping.\")             continue         if not pd.api.types.is_categorical_dtype(             df[col]         ) and not pd.api.types.is_object_dtype(df[col]):             print(f\"Column '{col}' is not categorical. Skipping.\")             continue          counts = df[col].value_counts(dropna=False)         if sort == \"asc\":             counts = counts.sort_values(ascending=True)         elif sort == \"desc\":             counts = counts.sort_values(ascending=False)         else:             counts = counts.sort_index()         if top_n:             counts = counts.head(top_n)          plt.figure(figsize=(12, 6))         ax = sns.barplot(x=counts.index.astype(str), y=counts.values, color=color)         plt.title(f\"Frequency of {col}\", fontsize=18, fontweight=\"bold\")         plt.xlabel(col, fontsize=14)         plt.ylabel(\"Count\", fontsize=14)         plt.xticks(rotation=45, ha=\"right\")          for p in ax.patches:             ax.annotate(                 f\"{int(p.get_height())}\",                 (p.get_x() + p.get_width() / 2.0, p.get_height()),                 ha=\"center\",                 va=\"bottom\",                 fontsize=12,                 color=\"black\",                 fontweight=\"bold\",                 xytext=(0, 3),                 textcoords=\"offset points\",             )         plt.tight_layout()          if save_dir is not None:             filename = os.path.join(save_dir, f\"barplot_flights_{col}.png\")             plt.savefig(filename, dpi=300, bbox_inches=\"tight\")             print(f\"Saved plot for '{col}' to {filename}\")          plt.show()         plt.close() In\u00a0[\u00a0]: Copied! <pre>def pairplots(df, color=\"#0060ff\", save_dir=None):\n    \"\"\"\n    Generate pair plot (scatter matrix) for numerical columns.\n\n    Parameters:\n        df (pd.DataFrame): DataFrame containing data.\n        color (str): Color for plots.\n        save_dir (str or None): Directory to save plot; if None, plot is not saved.\n    \"\"\"\n    num_df = df.select_dtypes(include=\"number\")\n    if num_df.shape[1] &lt; 2:\n        print(\"Not enough numerical columns for a pairplot. Skipping.\")\n        return\n\n    sns.set_theme(style=\"ticks\")\n    pair_grid = sns.pairplot(\n        num_df,\n        diag_kind=\"kde\",\n        plot_kws={\"color\": color, \"edgecolor\": \"black\", \"alpha\": 0.6},\n    )\n    pair_grid.figure.suptitle(\n        \"Pair Plot (Scatter Matrix) of Numerical Features\",\n        fontsize=20,\n        fontweight=\"bold\",\n        color=\"#333333\",\n        y=1.02,\n    )\n    plt.tight_layout()\n\n    if save_dir is not None:\n        save_dir = os.path.join(save_dir, \"flights_pairplot\")\n        os.makedirs(save_dir, exist_ok=True)\n        filename = os.path.join(save_dir, \"pairplot.png\")\n        plt.savefig(filename, dpi=300, bbox_inches=\"tight\")\n        print(f\"Saved pair plot to {filename}\")\n\n    plt.show()\n    plt.close()\n</pre> def pairplots(df, color=\"#0060ff\", save_dir=None):     \"\"\"     Generate pair plot (scatter matrix) for numerical columns.      Parameters:         df (pd.DataFrame): DataFrame containing data.         color (str): Color for plots.         save_dir (str or None): Directory to save plot; if None, plot is not saved.     \"\"\"     num_df = df.select_dtypes(include=\"number\")     if num_df.shape[1] &lt; 2:         print(\"Not enough numerical columns for a pairplot. Skipping.\")         return      sns.set_theme(style=\"ticks\")     pair_grid = sns.pairplot(         num_df,         diag_kind=\"kde\",         plot_kws={\"color\": color, \"edgecolor\": \"black\", \"alpha\": 0.6},     )     pair_grid.figure.suptitle(         \"Pair Plot (Scatter Matrix) of Numerical Features\",         fontsize=20,         fontweight=\"bold\",         color=\"#333333\",         y=1.02,     )     plt.tight_layout()      if save_dir is not None:         save_dir = os.path.join(save_dir, \"flights_pairplot\")         os.makedirs(save_dir, exist_ok=True)         filename = os.path.join(save_dir, \"pairplot.png\")         plt.savefig(filename, dpi=300, bbox_inches=\"tight\")         print(f\"Saved pair plot to {filename}\")      plt.show()     plt.close() In\u00a0[\u00a0]: Copied! <pre>def correlation_heatmap(\n    df,\n    cols=None,\n    annot=True,\n    cmap=\"Blues\",\n    save_dir=None,\n    filename=\"correlation_heatmap.png\",\n    figsize=(8, 6),\n):\n    \"\"\"\n    Plot heatmap of correlation matrix for specified columns.\n\n    Parameters:\n        df (pd.DataFrame): DataFrame containing data.\n        cols (list or None): Columns to include; if None, all numeric columns used.\n        annot (bool): Annotate heatmap with correlation values.\n        cmap (str): Colormap.\n        save_dir (str or None): Directory to save plot; if None, plot is not saved.\n        filename (str): Filename for saved plot.\n        figsize (tuple): Figure size.\n    \"\"\"\n    if cols is None:\n        data = df.select_dtypes(include=\"number\")\n    else:\n        data = df[cols]\n\n    if data.shape[1] &lt; 2:\n        print(\"Not enough columns for a correlation heatmap. Skipping.\")\n        return\n\n    corr = data.corr()\n\n    plt.figure(figsize=figsize)\n    sns.heatmap(\n        corr,\n        annot=annot,\n        cmap=cmap,\n        fmt=\".2f\",\n        linewidths=0.5,\n        cbar=True,\n        square=True,\n        annot_kws={\"size\": 12, \"weight\": \"bold\", \"color\": \"#333333\"},\n    )\n    plt.title(\"Correlation Heatmap\", fontsize=16, fontweight=\"bold\", color=\"#333333\")\n    plt.xticks(fontsize=12, color=\"#555555\")\n    plt.yticks(fontsize=12, color=\"#555555\")\n    plt.tight_layout()\n\n    if save_dir is not None:\n        os.makedirs(save_dir, exist_ok=True)\n        path = os.path.join(save_dir, filename)\n        plt.savefig(path, dpi=300, bbox_inches=\"tight\")\n        print(f\"Saved correlation heatmap to {path}\")\n\n    plt.show()\n    plt.close()\n</pre> def correlation_heatmap(     df,     cols=None,     annot=True,     cmap=\"Blues\",     save_dir=None,     filename=\"correlation_heatmap.png\",     figsize=(8, 6), ):     \"\"\"     Plot heatmap of correlation matrix for specified columns.      Parameters:         df (pd.DataFrame): DataFrame containing data.         cols (list or None): Columns to include; if None, all numeric columns used.         annot (bool): Annotate heatmap with correlation values.         cmap (str): Colormap.         save_dir (str or None): Directory to save plot; if None, plot is not saved.         filename (str): Filename for saved plot.         figsize (tuple): Figure size.     \"\"\"     if cols is None:         data = df.select_dtypes(include=\"number\")     else:         data = df[cols]      if data.shape[1] &lt; 2:         print(\"Not enough columns for a correlation heatmap. Skipping.\")         return      corr = data.corr()      plt.figure(figsize=figsize)     sns.heatmap(         corr,         annot=annot,         cmap=cmap,         fmt=\".2f\",         linewidths=0.5,         cbar=True,         square=True,         annot_kws={\"size\": 12, \"weight\": \"bold\", \"color\": \"#333333\"},     )     plt.title(\"Correlation Heatmap\", fontsize=16, fontweight=\"bold\", color=\"#333333\")     plt.xticks(fontsize=12, color=\"#555555\")     plt.yticks(fontsize=12, color=\"#555555\")     plt.tight_layout()      if save_dir is not None:         os.makedirs(save_dir, exist_ok=True)         path = os.path.join(save_dir, filename)         plt.savefig(path, dpi=300, bbox_inches=\"tight\")         print(f\"Saved correlation heatmap to {path}\")      plt.show()     plt.close() In\u00a0[\u00a0]: Copied! <pre>def boxplot_bivariate(\n    df, cat_cols, num_cols, color=\"#0060ff\", save_dir=None, rotation=30\n):\n    \"\"\"\n    Plot boxplots for combinations of categorical and numerical columns with median annotations.\n\n    Parameters:\n        df (pd.DataFrame): DataFrame containing data.\n        cat_cols (list): Categorical columns for x-axis.\n        num_cols (list): Numerical columns for y-axis.\n        color (str): Boxplot color.\n        save_dir (str or None): Directory to save plots; if None, plots are not saved.\n        rotation (int): Rotation angle for x-axis labels.\n    \"\"\"\n    sns.set_theme(style=\"darkgrid\", font_scale=1.2)\n    if save_dir is not None:\n        save_dir = os.path.join(save_dir, \"flights_boxplots_cat_num\")\n        os.makedirs(save_dir, exist_ok=True)\n\n    for cat_col in cat_cols:\n        if cat_col not in df.columns or not (\n            pd.api.types.is_categorical_dtype(df[cat_col])\n            or pd.api.types.is_object_dtype(df[cat_col])\n        ):\n            print(f\"Column '{cat_col}' is not categorical or not found. Skipping.\")\n            continue\n        for num_col in num_cols:\n            if num_col not in df.columns or not pd.api.types.is_numeric_dtype(\n                df[num_col]\n            ):\n                print(f\"Column '{num_col}' is not numeric or not found. Skipping.\")\n                continue\n\n            plt.figure(figsize=(14, 8))\n            ax = sns.boxplot(\n                x=df[cat_col],\n                y=df[num_col],\n                color=color,\n                linewidth=1.5,\n                fliersize=5,\n                boxprops=dict(alpha=0.7, edgecolor=\"black\"),\n                medianprops=dict(color=\"#ff6600\", linewidth=2),\n            )\n            plt.title(\n                f\"Box Plot of {num_col} by {cat_col}\",\n                fontsize=20,\n                fontweight=\"bold\",\n                color=\"#333333\",\n            )\n            plt.xlabel(cat_col, fontsize=16, color=\"#555555\")\n            plt.ylabel(num_col, fontsize=16, color=\"#555555\")\n            plt.tick_params(axis=\"both\", which=\"major\", labelsize=14, colors=\"#777777\")\n            plt.xticks(rotation=rotation)\n            plt.grid(True, alpha=0.3)\n            plt.tight_layout()\n\n            medians = df.groupby(cat_col, observed=False)[num_col].median()\n            for i, category in enumerate(medians.index):\n                median_val = medians[category]\n                ax.annotate(\n                    f\"{median_val:.2f}\",\n                    xy=(i, median_val),\n                    xytext=(0, 10),\n                    textcoords=\"offset points\",\n                    ha=\"center\",\n                    va=\"bottom\",\n                    fontsize=13,\n                    fontweight=\"bold\",\n                    color=\"#ff6600\",\n                    bbox=dict(\n                        boxstyle=\"round,pad=0.2\",\n                        fc=\"white\",\n                        ec=\"#ff6600\",\n                        lw=1,\n                        alpha=0.7,\n                    ),\n                )\n\n            if save_dir is not None:\n                filename = os.path.join(save_dir, f\"{num_col}_by_{cat_col}.png\")\n                plt.savefig(filename, dpi=300, bbox_inches=\"tight\")\n                print(f\"Saved box plot for {num_col} by {cat_col} to {filename}\")\n\n            plt.show()\n            plt.close()\n</pre> def boxplot_bivariate(     df, cat_cols, num_cols, color=\"#0060ff\", save_dir=None, rotation=30 ):     \"\"\"     Plot boxplots for combinations of categorical and numerical columns with median annotations.      Parameters:         df (pd.DataFrame): DataFrame containing data.         cat_cols (list): Categorical columns for x-axis.         num_cols (list): Numerical columns for y-axis.         color (str): Boxplot color.         save_dir (str or None): Directory to save plots; if None, plots are not saved.         rotation (int): Rotation angle for x-axis labels.     \"\"\"     sns.set_theme(style=\"darkgrid\", font_scale=1.2)     if save_dir is not None:         save_dir = os.path.join(save_dir, \"flights_boxplots_cat_num\")         os.makedirs(save_dir, exist_ok=True)      for cat_col in cat_cols:         if cat_col not in df.columns or not (             pd.api.types.is_categorical_dtype(df[cat_col])             or pd.api.types.is_object_dtype(df[cat_col])         ):             print(f\"Column '{cat_col}' is not categorical or not found. Skipping.\")             continue         for num_col in num_cols:             if num_col not in df.columns or not pd.api.types.is_numeric_dtype(                 df[num_col]             ):                 print(f\"Column '{num_col}' is not numeric or not found. Skipping.\")                 continue              plt.figure(figsize=(14, 8))             ax = sns.boxplot(                 x=df[cat_col],                 y=df[num_col],                 color=color,                 linewidth=1.5,                 fliersize=5,                 boxprops=dict(alpha=0.7, edgecolor=\"black\"),                 medianprops=dict(color=\"#ff6600\", linewidth=2),             )             plt.title(                 f\"Box Plot of {num_col} by {cat_col}\",                 fontsize=20,                 fontweight=\"bold\",                 color=\"#333333\",             )             plt.xlabel(cat_col, fontsize=16, color=\"#555555\")             plt.ylabel(num_col, fontsize=16, color=\"#555555\")             plt.tick_params(axis=\"both\", which=\"major\", labelsize=14, colors=\"#777777\")             plt.xticks(rotation=rotation)             plt.grid(True, alpha=0.3)             plt.tight_layout()              medians = df.groupby(cat_col, observed=False)[num_col].median()             for i, category in enumerate(medians.index):                 median_val = medians[category]                 ax.annotate(                     f\"{median_val:.2f}\",                     xy=(i, median_val),                     xytext=(0, 10),                     textcoords=\"offset points\",                     ha=\"center\",                     va=\"bottom\",                     fontsize=13,                     fontweight=\"bold\",                     color=\"#ff6600\",                     bbox=dict(                         boxstyle=\"round,pad=0.2\",                         fc=\"white\",                         ec=\"#ff6600\",                         lw=1,                         alpha=0.7,                     ),                 )              if save_dir is not None:                 filename = os.path.join(save_dir, f\"{num_col}_by_{cat_col}.png\")                 plt.savefig(filename, dpi=300, bbox_inches=\"tight\")                 print(f\"Saved box plot for {num_col} by {cat_col} to {filename}\")              plt.show()             plt.close() In\u00a0[\u00a0]: Copied! <pre>def barplot_bivariate(\n    df,\n    cat_cols,\n    num_cols,\n    aggfunc=\"mean\",\n    palette=None,\n    color=\"#0060ff\",\n    save_dir=None,\n    rotation=30,\n):\n    \"\"\"\n    Plot bar plots for combinations of categorical and numerical columns with aggregated value annotations.\n\n    Parameters:\n        df (pd.DataFrame): DataFrame containing data.\n        cat_cols (list): Categorical columns for x-axis.\n        num_cols (list): Numerical columns for y-axis.\n        aggfunc (str): Aggregation function ('mean', 'median', 'mode', 'sum', etc.).\n        palette (str or None): Color palette for bars; if None, single color used.\n        color (str): Single color for bars if palette is None.\n        save_dir (str or None): Directory to save plots; if None, plots are not saved.\n        rotation (int): Rotation angle for x-axis labels.\n    \"\"\"\n    sns.set_theme(style=\"darkgrid\", font_scale=1.2)\n    if save_dir is not None:\n        save_dir = os.path.join(save_dir, \"flights_barplots_cat_num\")\n        os.makedirs(save_dir, exist_ok=True)\n\n    for cat_col in cat_cols:\n        if cat_col not in df.columns or not (\n            pd.api.types.is_categorical_dtype(df[cat_col])\n            or pd.api.types.is_object_dtype(df[cat_col])\n        ):\n            print(f\"Column '{cat_col}' is not categorical or not found. Skipping.\")\n            continue\n        for num_col in num_cols:\n            if num_col not in df.columns or not pd.api.types.is_numeric_dtype(\n                df[num_col]\n            ):\n                print(f\"Column '{num_col}' is not numeric or not found. Skipping.\")\n                continue\n\n            if aggfunc == \"mode\":\n                agg_df = (\n                    df.groupby(cat_col, observed=False)[num_col]\n                    .agg(\n                        lambda x: x.mode().iloc[0]\n                        if not x.mode().empty\n                        else float(\"nan\")\n                    )\n                    .reset_index()\n                )\n            else:\n                agg_df = (\n                    df.groupby(cat_col, observed=False)[num_col]\n                    .agg(aggfunc)\n                    .reset_index()\n                )\n\n            plt.figure(figsize=(14, 8))\n            barplot_kwargs = dict(\n                data=agg_df,\n                x=cat_col,\n                y=num_col,\n                order=agg_df[cat_col],\n            )\n            if palette is not None:\n                barplot_kwargs.update(dict(hue=cat_col, palette=palette, legend=False))\n            else:\n                barplot_kwargs.update(dict(color=color))\n\n            ax = sns.barplot(**barplot_kwargs)\n\n            plt.title(\n                f\"{aggfunc.capitalize()} of {num_col} by {cat_col}\",\n                fontsize=20,\n                fontweight=\"bold\",\n                color=\"#333333\",\n            )\n            plt.xlabel(cat_col, fontsize=16, color=\"#555555\")\n            plt.ylabel(\n                f\"{aggfunc.capitalize()} of {num_col}\", fontsize=16, color=\"#555555\"\n            )\n            plt.tick_params(axis=\"both\", which=\"major\", labelsize=14, colors=\"#777777\")\n            plt.xticks(rotation=rotation)\n            plt.grid(True, alpha=0.3)\n            plt.tight_layout()\n\n            for bar, value in zip(ax.patches, agg_df[num_col]):\n                ax.annotate(\n                    f\"{value:.2f}\",\n                    (bar.get_x() + bar.get_width() / 2, bar.get_height()),\n                    ha=\"center\",\n                    va=\"bottom\",\n                    fontsize=13,\n                    fontweight=\"bold\",\n                    color=\"#ff6600\",\n                    xytext=(0, 8),\n                    textcoords=\"offset points\",\n                    bbox=dict(\n                        boxstyle=\"round,pad=0.2\",\n                        fc=\"white\",\n                        ec=\"#ff6600\",\n                        lw=1,\n                        alpha=0.7,\n                    ),\n                )\n\n            if save_dir is not None:\n                filename = os.path.join(\n                    save_dir, f\"{aggfunc}_{num_col}_by_{cat_col}.png\"\n                )\n                plt.savefig(filename, dpi=300, bbox_inches=\"tight\")\n                print(\n                    f\"Saved bar plot for {aggfunc} of {num_col} by {cat_col} to {filename}\"\n                )\n\n            plt.show()\n            plt.close()\n</pre> def barplot_bivariate(     df,     cat_cols,     num_cols,     aggfunc=\"mean\",     palette=None,     color=\"#0060ff\",     save_dir=None,     rotation=30, ):     \"\"\"     Plot bar plots for combinations of categorical and numerical columns with aggregated value annotations.      Parameters:         df (pd.DataFrame): DataFrame containing data.         cat_cols (list): Categorical columns for x-axis.         num_cols (list): Numerical columns for y-axis.         aggfunc (str): Aggregation function ('mean', 'median', 'mode', 'sum', etc.).         palette (str or None): Color palette for bars; if None, single color used.         color (str): Single color for bars if palette is None.         save_dir (str or None): Directory to save plots; if None, plots are not saved.         rotation (int): Rotation angle for x-axis labels.     \"\"\"     sns.set_theme(style=\"darkgrid\", font_scale=1.2)     if save_dir is not None:         save_dir = os.path.join(save_dir, \"flights_barplots_cat_num\")         os.makedirs(save_dir, exist_ok=True)      for cat_col in cat_cols:         if cat_col not in df.columns or not (             pd.api.types.is_categorical_dtype(df[cat_col])             or pd.api.types.is_object_dtype(df[cat_col])         ):             print(f\"Column '{cat_col}' is not categorical or not found. Skipping.\")             continue         for num_col in num_cols:             if num_col not in df.columns or not pd.api.types.is_numeric_dtype(                 df[num_col]             ):                 print(f\"Column '{num_col}' is not numeric or not found. Skipping.\")                 continue              if aggfunc == \"mode\":                 agg_df = (                     df.groupby(cat_col, observed=False)[num_col]                     .agg(                         lambda x: x.mode().iloc[0]                         if not x.mode().empty                         else float(\"nan\")                     )                     .reset_index()                 )             else:                 agg_df = (                     df.groupby(cat_col, observed=False)[num_col]                     .agg(aggfunc)                     .reset_index()                 )              plt.figure(figsize=(14, 8))             barplot_kwargs = dict(                 data=agg_df,                 x=cat_col,                 y=num_col,                 order=agg_df[cat_col],             )             if palette is not None:                 barplot_kwargs.update(dict(hue=cat_col, palette=palette, legend=False))             else:                 barplot_kwargs.update(dict(color=color))              ax = sns.barplot(**barplot_kwargs)              plt.title(                 f\"{aggfunc.capitalize()} of {num_col} by {cat_col}\",                 fontsize=20,                 fontweight=\"bold\",                 color=\"#333333\",             )             plt.xlabel(cat_col, fontsize=16, color=\"#555555\")             plt.ylabel(                 f\"{aggfunc.capitalize()} of {num_col}\", fontsize=16, color=\"#555555\"             )             plt.tick_params(axis=\"both\", which=\"major\", labelsize=14, colors=\"#777777\")             plt.xticks(rotation=rotation)             plt.grid(True, alpha=0.3)             plt.tight_layout()              for bar, value in zip(ax.patches, agg_df[num_col]):                 ax.annotate(                     f\"{value:.2f}\",                     (bar.get_x() + bar.get_width() / 2, bar.get_height()),                     ha=\"center\",                     va=\"bottom\",                     fontsize=13,                     fontweight=\"bold\",                     color=\"#ff6600\",                     xytext=(0, 8),                     textcoords=\"offset points\",                     bbox=dict(                         boxstyle=\"round,pad=0.2\",                         fc=\"white\",                         ec=\"#ff6600\",                         lw=1,                         alpha=0.7,                     ),                 )              if save_dir is not None:                 filename = os.path.join(                     save_dir, f\"{aggfunc}_{num_col}_by_{cat_col}.png\"                 )                 plt.savefig(filename, dpi=300, bbox_inches=\"tight\")                 print(                     f\"Saved bar plot for {aggfunc} of {num_col} by {cat_col} to {filename}\"                 )              plt.show()             plt.close()"},{"location":"LGBM_summary/LGBMR_production_model_details/","title":"\ud83c\udfc6 LightGBM Production Model: Detailed Report","text":""},{"location":"LGBM_summary/LGBMR_production_model_details/#1-executive-summary","title":"\ud83d\udcdd 1. Executive Summary","text":"<p>This report provides a comprehensive overview of the final LightGBM model, which was selected as the champion model for the flight price prediction task after a rigorous process of evaluation, investigation, and refinement.</p> <p>Initial modeling yielded a version that was suspiciously accurate. A deep-dive analysis revealed a subtle overfitting issue caused by a leaky feature (<code>route</code>). After correcting this, the final model emerged, demonstrating a superior combination of predictive accuracy and stability. It achieves a Cross-Validation RMSE of $9.57 and a final Test Set RMSE of $7.60. This document details the model's final performance, parameters, and an in-depth analysis of its behavior.</p>"},{"location":"LGBM_summary/LGBMR_production_model_details/#2-model-performance-metrics","title":"\ud83d\udcc8 2. Model Performance Metrics","text":"<p>The model's performance was validated through both rigorous cross-validation and a final evaluation on an unseen test set, confirming its stability and real-world predictive power.</p>"},{"location":"LGBM_summary/LGBMR_production_model_details/#21-cross-validation-performance-mean-over-5-folds","title":"2.1. Cross-Validation Performance (Mean over 5 Folds)","text":"<p>These metrics represent the model's average performance, demonstrating its stability and generalization capabilities.</p> Metric Value R\u00b2 Score 0.99928 Root Mean Squared Error (RMSE) $9.57 Mean Absolute Error (MAE) $7.09 CV RMSE Standard Deviation $0.43"},{"location":"LGBM_summary/LGBMR_production_model_details/#22-final-test-set-performance","title":"2.2. Final Test Set Performance","text":"<p>These metrics represent the final, official performance of the trained model on a hold-out test set.</p> Metric Value R\u00b2 Score 0.99956 Root Mean Squared Error (RMSE) $7.60 Mean Absolute Error (MAE) $5.50"},{"location":"LGBM_summary/LGBMR_production_model_details/#3-model-configuration-and-parameters","title":"\u2699\ufe0f 3. Model Configuration and Parameters","text":""},{"location":"LGBM_summary/LGBMR_production_model_details/#31-optimal-hyperparameters","title":"3.1. Optimal Hyperparameters","text":"<p>The following hyperparameters were identified during the tuning phase and used for the final model.</p> Parameter Value <code>n_estimators</code> 700 <code>learning_rate</code> 0.1663 <code>num_leaves</code> 15 <code>max_depth</code> 5 <code>subsample</code> 0.8337 <code>colsample_bytree</code> 0.6145 <code>reg_alpha</code> 4.99e-05 <code>reg_lambda</code> 0.3305 <code>random_state</code> 42 <code>n_jobs</code> -1"},{"location":"LGBM_summary/LGBMR_production_model_details/#32-production-preprocessing-pipeline","title":"3.2. Production Preprocessing Pipeline","text":"<p>Our final preprocessing pipeline is optimized for tree-based models by using the <code>is_tree_model: true</code> parameter. This streamlines the process and avoids unnecessary transformations.</p> <p>Key Optimizations:</p> <ul> <li>No Scaling or Power Transforms: LightGBM is not sensitive to the scale or distribution of numerical features, so these steps were removed.</li> <li>Integer Encoding: We use integer-based encoding for categorical features, which is handled natively and efficiently by LightGBM.</li> <li>Feature Selection: The engineered <code>interation features</code> and later <code>route</code> feature was explicitly removed after being identified as a source of data leakage and overfitting. The model now relies on the fundamental <code>from_location</code> and <code>to_location</code> features.</li> <li>Temporal Features: Cyclical features for <code>month</code>, <code>day</code>, and <code>day_of_week</code> are included, as their signal is no longer masked by other features.</li> </ul>"},{"location":"LGBM_summary/LGBMR_production_model_details/#optimized-pipeline-workflow","title":"Optimized Pipeline Workflow","text":"<pre><code>%%{init: {'theme': 'dark'}}%%\ngraph TD\n    subgraph \"Training Data Path\"\n        A[Load Silver Train Data] --&gt; B{Data Cleaning &amp; Imputation}\n        B --&gt; C{Fit &amp; Transform&lt;br/&gt; Cyclical Features, Ordinal Encoder}\n        C --&gt; D{Run GE Validation}\n        D --&gt; E[Save Gold Train Data &amp; Fitted Transformers]\n    end\n\n    subgraph \"Validation/Test Data Path\"\n        F[Load Silver Validation/Test Data] --&gt; G[Load Fitted Transformers]\n        G --&gt; H{Apply Transformations}\n        H --&gt; I{Run GE Validation}\n        I --&gt; J[Save Gold Validation/Test Data]\n    end\n</code></pre>"},{"location":"LGBM_summary/LGBMR_production_model_details/#4-in-depth-analysis-of-model-behavior","title":"\ud83d\udd0d 4. In-Depth Analysis of Model Behavior","text":""},{"location":"LGBM_summary/LGBMR_production_model_details/#a-prediction-accuracy-actual-vs-predicted","title":"A. Prediction Accuracy (Actual vs. Predicted)","text":"<p>The Actual vs. Predicted plot shows a tight diagonal line, confirming the model's high precision.</p> <ul> <li>Insight: The points are tightly clustered around the ideal 45-degree line, a visual confirmation of the high R\u00b2 score and low error rates.</li> </ul> <p></p>"},{"location":"LGBM_summary/LGBMR_production_model_details/#b-error-analysis-residuals-vs-predicted","title":"B. Error Analysis (Residuals vs. Predicted)","text":"<p>The residuals plot is healthy, showing a random and unbiased distribution of errors.</p> <ul> <li>Insight: The errors are clustered around the zero-line with no discernible patterns. This is the picture of a healthy, well-behaved model with no systematic biases.</li> </ul> <p></p>"},{"location":"LGBM_summary/LGBMR_production_model_details/#c-normality-of-residuals-q-q-plot","title":"C. Normality of Residuals (Q-Q Plot)","text":"<p>The Q-Q plot shows that the model's errors are very close to a normal distribution.</p> <ul> <li>Insight: The points follow the red line well, indicating that the error distribution is largely normal, which is a sign of a well-calibrated model.</li> </ul> <p></p>"},{"location":"LGBM_summary/LGBMR_production_model_details/#d-feature-importance-analysis","title":"D. Feature Importance Analysis","text":"<ul> <li>Check out the model explainability document for shap analysis for feature importance.</li> <li>Top Influencers: <code>time</code> (flight duration), <code>locations</code>, <code>agency</code> and  <code>flight_type</code>** remain the most dominant predictors.</li> <li>Re-emergence of Temporal Features: Crucially, the cyclical features for <code>day_of_week</code> and <code>day</code> now appear to have some level of importance. Their signal was previously being masked by the overfitting engineered features.</li> <li>Actionable Insight: The model has learned a robust hierarchy of what drives price: flight duration, location, agency,  service class, and specific temporal patterns.</li> </ul>"},{"location":"LGBM_summary/LGBMR_production_model_details/#5-final-verdict-a-true-champion","title":"\u2705 5. Final Verdict: A True Champion","text":"<p>After a thorough investigation, we can confidently declare this LightGBM model as the champion. The initial, suspiciously perfect model was correctly identified as overfit. By diagnosing the problem and correcting it, we have produced a final model that is not only highly accurate but also stable, robust, and reliable.</p> <p>The consistency between its cross-validation performance (CV RMSE: $9.57) and its final test set performance (Test RMSE: $7.60) is the key piece of evidence that it generalizes well to new, unseen data.</p>"},{"location":"LGBM_summary/LGBMR_production_model_details/#next-steps","title":"\u27a1\ufe0f Next Steps","text":"<p>This model is ready for production. The next steps involve deploying it as a service and monitoring its performance over time.</p> <ul> <li>Deep Dive into the Champion Model's Explainability \u00bb</li> </ul>"},{"location":"MLOps/airflow/","title":"Orchestration with Apache Airflow (Work in Progress)","text":"<p>Note on Orchestration: The primary method for pipeline orchestration in this project is DVC pipelines (defined in <code>dvc.yaml</code>). This document describes an alternative, more advanced setup using Apache Airflow for scheduled and production-grade workflow management. This setup is considered a future enhancement.</p> <p>This document outlines the custom setup for Apache Airflow within this project, designed for clarity, maintainability, and production-readiness.</p>"},{"location":"MLOps/airflow/#1-architecture-overview","title":"1. Architecture Overview","text":"<p>Our custom Airflow setup is containerized using Docker and orchestrated with Docker Compose. It consists of several key services that work together to provide a robust environment for orchestrating MLOps pipelines.</p> <p>The core components are: - Custom Airflow Image: A bespoke Docker image built from <code>airflow/Dockerfile</code>, containing Airflow, the project's source code, and all necessary dependencies. - PostgreSQL Database: The metadata backend where Airflow stores information about DAGs, task instances, and connections. - Airflow Services: The scheduler and webserver that form the heart of Airflow's execution engine (using <code>LocalExecutor</code> for simplicity in this setup).</p>"},{"location":"MLOps/airflow/#2-service-breakdown","title":"2. Service Breakdown","text":"<p>The <code>airflow/docker-compose.yaml</code> file orchestrates the following services:</p> <ul> <li><code>postgres-airflow</code>:<ul> <li>Image: <code>postgres:13</code></li> <li>Purpose: Acts as the persistent metadata database for Airflow.</li> </ul> </li> <li><code>airflow-init</code>:<ul> <li>Image: Custom <code>flights-mlops-airflow:latest</code></li> <li>Purpose: A one-time initialization service that sets up the database schema and creates the admin user.</li> </ul> </li> <li><code>airflow-webserver</code>:<ul> <li>Image: Custom <code>flights-mlops-airflow:latest</code></li> <li>Purpose: Runs the Airflow UI, accessible on <code>http://localhost:8080</code>.</li> </ul> </li> <li><code>airflow-scheduler</code>:<ul> <li>Image: Custom <code>flights-mlops-airflow:latest</code></li> <li>Purpose: The core service that monitors DAGs and triggers their execution.</li> </ul> </li> </ul> <p>Note: This setup uses the <code>LocalExecutor</code>, so Celery-related services like Redis and workers are not required.</p>"},{"location":"MLOps/airflow/#3-how-to-run","title":"3. How to Run","text":""},{"location":"MLOps/airflow/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker and Docker Compose</li> <li>An <code>.env</code> file in the <code>airflow</code> directory with <code>AIRFLOW_UID=$(id -u)</code> to ensure correct file permissions.</li> </ul>"},{"location":"MLOps/airflow/#step-1-build-the-custom-image","title":"Step 1: Build the Custom Image","text":"<p>This command builds the custom Docker image containing Airflow and your project code.</p> <pre><code># Run from the project root\ndocker compose -f airflow/docker-compose.yaml build\n</code></pre>"},{"location":"MLOps/airflow/#step-2-initialize-the-database","title":"Step 2: Initialize the Database","text":"<p>This one-time command sets up the Airflow metadata database.</p> <pre><code># Run from the project root\ndocker compose -f airflow/docker-compose.yaml up airflow-init\n</code></pre>"},{"location":"MLOps/airflow/#step-3-start-airflow","title":"Step 3: Start Airflow","text":"<p>This command starts the webserver and scheduler.</p> <pre><code># Run from the project root\ndocker compose -f airflow/docker-compose.yaml up -d\n</code></pre> <p>The Airflow UI will be available at <code>http://localhost:8080</code> (default credentials: <code>airflow</code>/<code>airflow</code>).</p>"},{"location":"MLOps/airflow/#4-dag-development","title":"4. DAG Development","text":"<p>With this setup, the entire project directory is available inside the Airflow containers at the <code>/app</code> path.</p> <ul> <li>Use Relative Paths: Access scripts from your DAGs using paths relative to the project root (e.g., <code>src/pipelines/bronze_pipeline.py</code>).</li> <li>PYTHONPATH: The <code>PYTHONPATH</code> is set to <code>/app</code>, so you can import custom modules directly (e.g., <code>from src.shared.utils import ...</code>).</li> </ul>"},{"location":"MLOps/architecture/","title":"\ud83c\udfdb\ufe0f Project Architecture","text":"<p>This document outlines the MLOps architecture for the flight price prediction project. The architecture is designed to be modular, scalable, and reproducible, ensuring a robust workflow from data ingestion to model deployment.</p>"},{"location":"MLOps/architecture/#high-level-overview","title":"\ud83d\uddfa\ufe0f High-Level Overview","text":"<p>The architecture is composed of four main pillars:</p> <ol> <li>Code &amp; Data Versioning: The foundation for reproducibility.</li> <li>Automated Pipelines: For data processing and validation.</li> <li>Modeling &amp; Tracking: For experimentation and model management.</li> <li>CI/CD &amp; Deployment: For automated testing, validation, and serving.</li> </ol> <pre><code>graph TD\n    subgraph \"Code &amp; Data Versioning\"\n        Dev[Developer] -- \"git push\" --&gt; GitHub[GitHub Repo];\n        Dev -- \"dvc push\" --&gt; S3[S3 Storage];\n        GitHub --&gt; DVC[DVC];\n        DVC --&gt; S3;\n    end\n\n    subgraph \"Automated Data Pipelines\"\n        DVC_Pipeline[DVC Pipeline: `dvc repro`] --&gt; Bronze[Bronze Pipeline];\n        Bronze -- \"Uses\" --&gt; GE[Great Expectations];\n        Bronze --&gt; Silver[Silver Pipeline];\n        Silver -- \"Uses\" --&gt; GE;\n        Silver --&gt; Gold[Gold Pipeline];\n        Gold -- \"Uses\" --&gt; GE;\n    end\n\n    subgraph \"Modeling &amp; Tracking\"\n        Gold --&gt; Training[Training/Tuning&lt;br/&gt;Pipelines];\n        Training -- \"Logs to\" --&gt; MLflow[MLflow Tracking&lt;br/&gt;&amp; Registry];\n    end\n\n    subgraph \"Deployment &amp; Serving\"\n        GitHub -- \"CI/CD Trigger\" --&gt; GHA[GitHub Actions];\n        GHA --&gt; Build[Build &amp; Test];\n        Build --&gt; Deploy[Deploy to Google Cloud Run];\n        Deploy -- \"Loads Model from\" --&gt; MLflow;\n    end\n</code></pre>"},{"location":"MLOps/architecture/#component-breakdown","title":"\ud83e\udde9 Component Breakdown","text":""},{"location":"MLOps/architecture/#1-code-data-versioning","title":"\ud83d\udcbe 1. Code &amp; Data Versioning","text":"<ul> <li>Developer: The user who writes code, runs experiments, and manages the project.</li> <li>GitHub: The central repository for all source code, documentation, and DVC metadata files. It acts as the single source of truth for the project's logic.</li> <li>DVC (Data Version Control): Used to version large files (datasets, models, artifacts) that cannot be stored in Git. DVC creates small metadata files that point to the actual data stored in a remote location.</li> <li>S3 Storage: The remote storage backend for DVC. All large files versioned by DVC are stored here.</li> </ul>"},{"location":"MLOps/architecture/#2-automated-data-pipelines","title":"\u2699\ufe0f 2. Automated Data Pipelines","text":"<ul> <li>DVC Pipelines: The primary orchestration tool is DVC itself. The <code>dvc.yaml</code> file defines the stages of the pipeline (Bronze, Silver, Gold), their dependencies, and outputs. Running <code>dvc repro</code> executes the entire workflow, ensuring reproducibility and only re-running stages where inputs have changed.</li> <li>Bronze, Silver, Gold Pipelines: These are the sequential stages of data processing, following the Medallion Architecture to progressively clean, transform, and enrich the data.</li> <li>Great Expectations: The data quality framework integrated into each data pipeline. It validates the data at each stage, ensuring that only high-quality data proceeds to the next step.</li> </ul>"},{"location":"MLOps/architecture/#3-modeling-tracking","title":"\ud83c\udfaf 3. Modeling &amp; Tracking","text":"<ul> <li>Training/Tuning Pipelines: These are Python scripts responsible for model training and hyperparameter tuning. They consume the model-ready data from the Gold pipeline.</li> <li>MLflow: The core of the experimentation process. It is used to:<ul> <li>Track Experiments: Log parameters, metrics, and artifacts for every training and tuning run.</li> <li>Manage Models: Store the best models in the MLflow Model Registry, providing versioning and stage management (e.g., Staging, Production).</li> </ul> </li> </ul>"},{"location":"MLOps/architecture/#4-cicd-deployment","title":"\ud83d\ude80 4. CI/CD &amp; Deployment","text":"<ul> <li>GitHub Actions: The automation engine for CI/CD.<ul> <li>Continuous Integration: On every push/PR to <code>main</code>, workflows automatically lint, test, and validate the entire DVC pipeline. See the CI Documentation for details.</li> <li>Continuous Deployment: On git tags (e.g., <code>v1.0</code>), workflows automatically build the prediction server image and deploy it to Google Cloud Run. See the CD Documentation for details.</li> </ul> </li> <li>FastAPI &amp; Docker: The final, champion model from the MLflow Registry is served via a high-performance FastAPI application. This application is containerized with Docker for portability and scalable deployment on Google Cloud Run.</li> </ul>"},{"location":"MLOps/docker/","title":"\ud83d\udc33 Docker Integration","text":"<p>This document outlines the project's containerization strategy, explaining how Docker is used to create consistent, portable, and production-ready environments for different components of the MLOps lifecycle.</p>"},{"location":"MLOps/docker/#1-overview-of-docker-strategy","title":"\ud83c\udfaf 1. Overview of Docker Strategy","text":"<p>Docker is a cornerstone of this project, used to solve two primary challenges:</p> <ol> <li>Orchestration Environment<code>(WIP)</code>: It provides a self-contained, reproducible environment for running the Apache Airflow scheduler and webserver, ensuring that the data pipeline orchestration is consistent across all systems.</li> <li>Deployment Environment: It packages the FastAPI prediction server into a lightweight, secure, and portable image that can be deployed anywhere\u2014from a local machine to a serverless cloud platform like Google Cloud Run.</li> </ol>"},{"location":"MLOps/docker/#2-use-case-1-orchestration-with-airflowwip","title":"\u2699\ufe0f 2. Use Case 1: Orchestration with Airflow(<code>WIP</code>)","text":"<p>The project includes a complete setup to run Airflow within Docker for orchestrating the data pipelines.</p> <ul> <li>Setup: The environment is defined in <code>airflow/docker-compose.yaml</code>, which orchestrates the necessary Airflow services (scheduler, webserver, postgres metadata database).</li> <li>Custom Image: A custom image is built via <code>airflow/Dockerfile</code> to install project-specific dependencies.</li> </ul> <p>For a detailed guide on the Airflow setup and how to run the orchestration DAGs, please see the Airflow Documentation.</p>"},{"location":"MLOps/docker/#3-use-case-2-serving-the-prediction-api","title":"\ud83d\ude80 3. Use Case 2: Serving the Prediction API","text":"<p>Containerizing the prediction server is critical for deployment. Our strategy focuses on creating a minimal, secure, and efficient production image.</p>"},{"location":"MLOps/docker/#dockerfile-strategy-multi-stage-builds","title":"Dockerfile Strategy: Multi-Stage Builds","text":"<p>The <code>src/prediction_server/Dockerfile</code> uses a multi-stage build to create a lean final image. This is a best practice that significantly reduces the image size and attack surface.</p> <ol> <li>The <code>builder</code> Stage: This first stage installs <code>uv</code> and uses it to download and compile all Python dependencies into a virtual environment. This stage contains all the build tools and cache, making it large.</li> <li>The <code>final</code> Stage: This second stage starts from a fresh, slim Python base image. It copies only the installed packages from the <code>builder</code> stage and the application source code. It does not include <code>uv</code>, build tools, or any intermediate layers, resulting in a much smaller and more secure production image.</li> </ol>"},{"location":"MLOps/docker/#entrypoint-script-strategy-dynamic-runtime-configuration","title":"Entrypoint Script Strategy: Dynamic Runtime Configuration","text":"<p>The container's startup is managed by the <code>src/prediction_server/docker-entrypoint.sh</code> script. This script runs before the FastAPI application and is responsible for preparing the runtime environment:</p> <ol> <li>DVC Configuration: It uses environment variables to configure DVC to connect to the remote S3 storage.</li> <li>Artifact Pulling: It runs <code>dvc pull</code> to download the required model and data transformer artifacts into the container.</li> <li>MLflow Configuration: It sets the necessary environment variables for the application to communicate with the MLflow tracking server.</li> <li>Application Execution: Finally, it executes the main <code>uvicorn</code> command to start the FastAPI server.</li> </ol> <p>This approach decouples the static container image from the dynamic runtime configuration, making the image more portable.</p>"},{"location":"MLOps/docker/#quick-commands","title":"Quick Commands","text":"<ul> <li> <p>Build the Image:     <code>bash     docker build -t prediction-server:latest -f src/prediction_server/Dockerfile .</code></p> </li> <li> <p>Run the Container:     <code>bash     docker run --env-file ./src/prediction_server/prediction_app.env -p 9000:9000 prediction-server:latest</code></p> </li> </ul> <p>For a line-by-line breakdown of the <code>Dockerfile</code>, the <code>docker-entrypoint.sh</code> script, and a detailed guide on deploying to the cloud, please refer to the API Reference &amp; Deployment Guide.</p>"},{"location":"MLOps/dvc/","title":"\ud83d\udcbf DVC (Data Version Control) Integration","text":"<p>This document details the role of DVC in managing and versioning large data files and models within the project, ensuring reproducibility and collaboration.</p>"},{"location":"MLOps/dvc/#1-overview-and-purpose","title":"\ud83d\udcdd 1. Overview and Purpose","text":"<p>DVC (Data Version Control) extends Git's capabilities to handle large files, datasets, and machine learning models. In this project, DVC is used to:</p> <ul> <li>Version Data and Models: Track changes to datasets and trained models alongside the code, ensuring that every experiment is fully reproducible.</li> <li>Enable Reproducibility: Allows switching between different versions of data and models by simply checking out a Git commit and running <code>dvc pull</code>.</li> <li>Manage Large Files: Keeps large files out of the Git repository, storing them efficiently in remote storage (like AWS S3) while Git tracks only small <code>.dvc</code> metadata files.</li> <li>Facilitate Collaboration: Provides a streamlined way for team members to work with consistent versions of data and models.</li> </ul>"},{"location":"MLOps/dvc/#2-dvc-setup-and-configuration","title":"\u2699\ufe0f 2. DVC Setup and Configuration","text":""},{"location":"MLOps/dvc/#21-initial-setup","title":"2.1. Initial Setup","text":"<p>After initializing a Git repository, DVC is initialized in the project root:</p> <pre><code>dvc init\n</code></pre> <p>The primary configuration involves setting up a remote storage location, which in this project is a Backblaze B2 S3-compatible bucket.</p>"},{"location":"MLOps/dvc/#22-dynamic-remote-configuration","title":"2.2. Dynamic Remote Configuration","text":"<p>For production and containerized environments, this project uses a dynamic approach to configure the DVC remote storage at runtime. This is handled by the <code>docker-entrypoint.sh</code> script and relies on environment variables.</p> <p>The script executes the following commands to configure the DVC remote:</p> <pre><code># Add the remote storage location\ndvc remote add -d -f \"$DVC_REMOTE_NAME\" s3://\"$DVC_BUCKET_NAME\"\n\n# Modify the remote with specific credentials and settings\ndvc remote modify \"$DVC_REMOTE_NAME\" endpointurl \"$DVC_S3_ENDPOINT_URL\"\ndvc remote modify --local \"$DVC_REMOTE_NAME\" region \"$DVC_S3_ENDPOINT_REGION\"\ndvc remote modify --local \"$DVC_REMOTE_NAME\" access_key_id \"$DVC_AWS_ACCESS_KEY_ID\"\ndvc remote modify --local \"$DVC_REMOTE_NAME\" secret_access_key \"$DVC_AWS_SECRET_ACCESS_KEY\"\n</code></pre>"},{"location":"MLOps/dvc/#23-environment-variables-for-configuration","title":"2.3. Environment Variables for Configuration","text":"<p>DVC requires credentials to access the S3 bucket. These are provided via the following environment variables:</p> <ul> <li><code>DVC_REMOTE_NAME</code>: The name to assign to the DVC remote (e.g., <code>myremote</code>).</li> <li><code>DVC_BUCKET_NAME</code>: The name of the S3 bucket where data and models are stored.</li> <li><code>DVC_S3_ENDPOINT_URL</code>: The S3 endpoint URL (for S3-compatible storage like Backblaze).</li> <li><code>DVC_S3_ENDPOINT_REGION</code>: The AWS region of the S3 bucket.</li> <li><code>DVC_AWS_ACCESS_KEY_ID</code>: The AWS access key ID.</li> <li><code>DVC_AWS_SECRET_ACCESS_KEY</code>: The AWS secret access key.</li> </ul> <p>For more details on securely managing these credentials, refer to the MLflow Integration documentation.</p>"},{"location":"MLOps/dvc/#3-core-workflow","title":"\ud83d\udd04 3. Core Workflow","text":""},{"location":"MLOps/dvc/#31-versioning-data-and-models","title":"3.1. Versioning Data and Models","text":"<p>To version a file or directory with DVC:</p> <pre><code>dvc add data/raw/flights.csv\ndvc add models/\n</code></pre> <p>This command replaces the actual file/directory with a small <code>.dvc</code> file, which is then tracked by Git:</p> <pre><code>git add data/raw/flights.csv.dvc models.dvc\ngit commit -m \"Add DVC-tracked raw data and models\"\n</code></pre> <p>To push the actual data to remote storage:</p> <pre><code>dvc push\n</code></pre>"},{"location":"MLOps/dvc/#32-retrieving-data-and-models","title":"3.2. Retrieving Data and Models","text":"<p>To retrieve the data and models associated with your current Git commit, use <code>dvc pull</code>:</p> <pre><code>dvc pull\n</code></pre> <p>This command downloads the data from the DVC remote to your local machine.</p>"},{"location":"MLOps/dvc/#33-reproducing-past-states","title":"3.3. Reproducing Past States","text":"<p>One of DVC's most powerful features is the ability to reproduce past experiments by checking out a specific Git commit and retrieving the exact data and models used at that time:</p> <pre><code>git checkout &lt;commit_hash&gt;\ndvc pull\n</code></pre> <p>This ensures full reproducibility of any experiment or model version.</p>"},{"location":"MLOps/dvc/#4-dvc-in-the-project","title":"\ud83c\udfd7\ufe0f 4. DVC in the Project","text":""},{"location":"MLOps/dvc/#41-dvc-in-the-docker-entrypoint","title":"4.1. DVC in the Docker Entrypoint","text":"<p>For the prediction server, DVC plays a critical role during container startup. The <code>docker-entrypoint.sh</code> script is configured to pull the necessary models before the FastAPI application launches. This ensures the server always starts with the correct model version.</p> <p>For the full script and its context, refer to the API Reference documentation.</p>"},{"location":"MLOps/dvc/#42-dvc-pipelines","title":"4.2. DVC Pipelines","text":"<p>This project also utilizes DVC pipelines to define and manage the data processing and model training workflows. The entire end-to-end pipeline is defined in <code>dvc.yaml</code>.</p> <p>To run the full DVC pipeline:</p> <pre><code>dvc repro\n</code></pre> <p>This command executes all stages defined in <code>dvc.yaml</code> in the correct order, ensuring that data dependencies are met and only necessary stages are re-executed when inputs change.</p>"},{"location":"MLOps/dvc_pipeline/","title":"\u26d3\ufe0f DVC Pipeline Orchestration","text":"<p>This document provides a detailed breakdown of the <code>dvc.yaml</code> file, which acts as the central orchestrator for the entire data processing and model training workflow in this project.</p>"},{"location":"MLOps/dvc_pipeline/#overview","title":"\ud83d\uddfa\ufe0f Overview","text":"<p>The <code>dvc.yaml</code> file defines a Directed Acyclic Graph (DAG) of all the stages in our project. Each stage has defined inputs (dependencies), outputs, and the command to execute. This structure allows DVC to intelligently manage the workflow.</p> <p>When you run <code>dvc repro</code>, DVC checks if any of the dependencies have changed since the last run. If they have, it re-executes the affected stage and all subsequent stages that depend on its output. This ensures reproducibility and efficiency, as only necessary computations are performed.</p>"},{"location":"MLOps/dvc_pipeline/#pipeline-visualization","title":"\ud83d\udcca Pipeline Visualization","text":"<p>The end-to-end pipeline defined in <code>dvc.yaml</code> follows this sequence:</p> <pre><code>graph TD\n    A[split_data] --&gt; B[bronze_pipeline];\n    B --&gt; C[silver_pipeline];\n    C --&gt; D[gold_pipeline];\n    D --&gt; E[training_pipeline];\n</code></pre>"},{"location":"MLOps/dvc_pipeline/#stage-breakdown","title":"\ud83e\udde9 Stage Breakdown","text":"<p>Here is a detailed look at each stage defined in <code>dvc.yaml</code>.</p>"},{"location":"MLOps/dvc_pipeline/#1-split_data","title":"1. <code>split_data</code>","text":"<ul> <li>Purpose: Takes the single raw dataset (<code>flights.csv</code>) and splits it into <code>train</code>, <code>validation</code>, and <code>test</code> sets to ensure a consistent data division for all subsequent steps.</li> <li>Command: <code>python src/data_split/split_data.py</code></li> <li>Dependencies: The main dependency is the raw <code>data/raw/flights.csv</code> file and the splitting script itself.</li> <li>Outputs: The <code>data/raw/train_validation_test</code> directory, which contains the three new CSV files.</li> </ul>"},{"location":"MLOps/dvc_pipeline/#2-bronze_pipeline","title":"2. <code>bronze_pipeline</code>","text":"<ul> <li>Purpose: Acts as the first quality gate. It runs the Bronze data validation pipeline on each of the data splits.</li> <li>Command: <code>python src/pipelines/bronze_pipeline.py ${item}.csv</code></li> <li>Special Logic: This stage uses a <code>foreach</code> loop over the <code>data_splits</code> variable (defined in <code>params.yaml</code>). This means it executes three times, once for each of the <code>train</code>, <code>validation</code>, and <code>test</code> files.</li> <li>Dependencies: The corresponding raw data split (e.g., <code>train.csv</code>) and the Bronze pipeline source code.</li> <li>Outputs: A validated CSV file in the <code>data/bronze_data/processed/</code> directory for each split.</li> </ul>"},{"location":"MLOps/dvc_pipeline/#3-silver_pipeline","title":"3. <code>silver_pipeline</code>","text":"<ul> <li>Purpose: Cleans, standardizes, and performs initial feature engineering on the validated Bronze data.</li> <li>Command: <code>python src/pipelines/silver_pipeline.py ${item}.csv</code></li> <li>Special Logic: Like the Bronze stage, this also uses a <code>foreach</code> loop to process each data split independently.</li> <li>Dependencies: The corresponding processed Bronze file and the Silver pipeline source code.</li> <li>Outputs: A processed Parquet file in the <code>data/silver_data/processed/</code> directory for each split.</li> </ul>"},{"location":"MLOps/dvc_pipeline/#4-gold_pipeline","title":"4. <code>gold_pipeline</code>","text":"<ul> <li>Purpose: The final and most complex data transformation stage. It applies advanced feature engineering and preprocessing to prepare the data for machine learning.</li> <li>Command: <code>python src/pipelines/gold_pipeline.py</code></li> <li>Dependencies: All three processed Parquet files from the Silver layer (<code>train.parquet</code>, <code>validation.parquet</code>, <code>test.parquet</code>).</li> <li>Parameters (<code>params</code>): This stage is sensitive to parameters defined in <code>params.yaml</code> under the <code>gold_pipeline</code> key. This allows for tuning the preprocessing steps (e.g., imputation strategy, outlier handling) without changing the code.</li> <li>Outputs: The model-ready datasets in <code>data/gold_engineered_data/processed/</code> and all fitted data transformers (scalers, encoders, etc.) in the <code>models/</code> directory.</li> </ul>"},{"location":"MLOps/dvc_pipeline/#5-training_pipeline","title":"5. <code>training_pipeline</code>","text":"<ul> <li>Purpose: Executes the model training, evaluation, and logging workflow.</li> <li>Command: <code>python src/pipelines/training_pipeline.py train.parquet validation.parquet --test_file_name test.parquet</code></li> <li>Dependencies: The final processed data from the Gold layer.</li> <li>Parameters (<code>params</code>): This stage's behavior is heavily controlled by the <code>training_pipeline</code> and <code>mlflow_params</code> sections in <code>params.yaml</code>. This is where you can specify which model to run, its hyperparameters, and whether to perform cross-validation.</li> <li>Outputs: This stage does not produce DVC-tracked file outputs. Instead, its primary outputs are the experiments, metrics, and model artifacts logged to the MLflow Tracking Server.</li> </ul>"},{"location":"MLOps/dvc_pipeline/#how-to-run-the-pipeline","title":"\u25b6\ufe0f How to Run the Pipeline","text":"<ul> <li>Run the entire pipeline: To execute all stages in order, from start to finish, run:</li> </ul> <pre><code>dvc repro\n</code></pre> <ul> <li>Run a specific stage: To run the pipeline up to and including a specific stage, use its name. For example, to run everything through the <code>gold_pipeline</code>:</li> </ul> <pre><code>dvc repro gold_pipeline\n</code></pre> <ul> <li>Visualize the pipeline: To see a text-based representation of the DAG in your terminal, run:</li> </ul> <pre><code>dvc dag\n</code></pre>"},{"location":"MLOps/mlflow/","title":"\ud83d\ude80 MLflow Integration and Deployment","text":"<p>This document provides a comprehensive guide to integrating MLflow for experiment tracking and model management. It covers connecting a client application to a remote MLflow server and deploying a robust tracking server on AWS.</p>"},{"location":"MLOps/mlflow/#1-connecting-a-client-to-mlflow","title":"\ud83d\udd17 1. Connecting a Client to MLflow","text":"<p>This section explains how a client application (e.g., a prediction API in a Docker container) securely connects to a remote MLflow server to fetch models and artifacts.</p>"},{"location":"MLOps/mlflow/#11-the-connection-mechanism","title":"\u2699\ufe0f 1.1. The Connection Mechanism","text":"<p>The client connection is a two-step process managed by environment variables.</p> <ol> <li> <p>Initial Connection to the Tracking Server:</p> <ul> <li>The client application needs the address of the MLflow tracking server on the EC2 instance.</li> <li>This is set using the <code>MLFLOW_TRACKING_URI</code> environment variable.</li> <li>The MLflow client library automatically uses this variable to initiate a network request to the server.</li> </ul> </li> <li> <p>Fetching Artifacts from S3:</p> <ul> <li>After connecting, the tracking server tells the client where the model files are located (typically an S3 bucket path).</li> <li>The client then needs permissions to access S3, which are provided via AWS credentials.</li> <li>These are set using the <code>AWS_ACCESS_KEY_ID</code>, <code>AWS_SECRET_ACCESS_KEY</code>, and <code>AWS_DEFAULT_REGION</code> environment variables.</li> </ul> </li> </ol>"},{"location":"MLOps/mlflow/#12-automatic-credential-detection","title":"\ud83d\udd75\ufe0f 1.2. Automatic Credential Detection","text":"<p>You do not need to install the AWS CLI or run <code>aws configure</code> inside the Docker container. The AWS SDK for Python (<code>boto3</code>), used by MLflow, automatically finds and uses credentials from environment variables.</p> <p>When <code>boto3</code> detects <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code> in the container's environment, it authenticates with AWS services like S3 automatically.</p>"},{"location":"MLOps/mlflow/#13-production-best-practice-environment-files","title":"\ud83d\udd12 1.3. Production Best Practice: Environment Files","text":"<p>The most secure and standard method for providing credentials to a Docker container is using an environment file (e.g., <code>prediction_app.env</code>).</p> <p>Warning: This file contains sensitive credentials and must never be committed to version control. Add it to your <code>.gitignore</code> file.</p> <p>1. Create the <code>.env</code> file:</p> <pre><code># ----------------------------------\n# MLflow Production Configuration\n# ----------------------------------\n# The public IP or domain of your EC2 instance running the MLflow server.\nMLFLOW_TRACKING_URI=\"http://&lt;YOUR_EC2_IP_ADDRESS&gt;:5000\"\n\n# ----------------------------------\n# AWS Credentials for MLflow Artifacts\n# ----------------------------------\n# Credentials for an IAM user with read-only access to your S3 artifact bucket.\nAWS_ACCESS_KEY_ID=\"&lt;YOUR_AWS_ACCESS_KEY_ID&gt;\"\nAWS_SECRET_ACCESS_KEY=\"&lt;YOUR_AWS_SECRET_ACCESS_KEY&gt;\"\nAWS_DEFAULT_REGION=\"&lt;YOUR_S3_BUCKET_REGION&gt;\"\n</code></pre> <p>2. Run the container with the <code>--env-file</code> flag:</p> <p>This command injects the variables into the container at runtime.</p> <pre><code>docker run --env-file ./src/prediction_server/prediction_app.env -p 8000:8000 your-image-name\n</code></pre>"},{"location":"MLOps/mlflow/#2-deploying-an-mlflow-tracking-server-on-aws","title":"\u2601\ufe0f 2. Deploying an MLflow Tracking Server on AWS","text":"<p>This section provides a step-by-step guide to setting up a robust MLflow Tracking Server using EC2 (server), S3 (artifact storage), and RDS (backend database).</p>"},{"location":"MLOps/mlflow/#21-step-1-create-an-s3-bucket-for-artifacts","title":"\ud83e\udea3 2.1. Step 1: Create an S3 Bucket for Artifacts","text":"<ol> <li>Navigate to S3: Log into your AWS account and go to the S3 service.</li> <li>Create Bucket: Click \"Create bucket\" and configure the following:<ul> <li>Bucket name: Must be globally unique (e.g., <code>yourname-mlflow-artifacts-2025</code>).</li> <li>AWS Region: Choose a region (e.g., <code>ap-south-1</code>). Important: Launch all other services (EC2, RDS) in the same region.</li> <li>Block Public Access: Keep \"Block all public access\" checked for security.</li> <li>Bucket Versioning: Enable to protect against accidental data loss.</li> <li>Tags (Recommended): Add a tag for cost tracking (e.g., <code>Key: Project</code>, <code>Value: mlflow-server</code>).</li> <li>Default encryption: Keep the default (<code>SSE-S3</code>).</li> </ul> </li> <li>Finalize: Review your settings and click \"Create bucket\".</li> </ol>"},{"location":"MLOps/mlflow/#22-step-2-create-a-postgresql-database-with-rds","title":"\ud83d\udc18 2.2. Step 2: Create a PostgreSQL Database with RDS","text":"<ol> <li>Navigate to RDS: In the AWS Console, go to the RDS service.</li> <li>Create Database: Click \"Create database\" and follow the wizard:<ul> <li>Creation method: Select \"Standard Create\".</li> <li>Engine: Choose \"PostgreSQL\".</li> <li>Templates: Select the \"Free tier\" template.</li> <li>Settings:<ul> <li>DB instance identifier: <code>mlflow-db</code>.</li> <li>Master username: <code>mlflow_user</code>.</li> <li>Master password: Create a strong password and store it securely.</li> </ul> </li> <li>Connectivity:<ul> <li>Public access: Select \"No\".</li> <li>VPC security group: Choose \"Create new\" and name it <code>mlflow-db-security-group</code>.</li> </ul> </li> <li>Additional configuration:<ul> <li>Initial database name: Enter <code>mlflow_db</code>. This is crucial.</li> </ul> </li> </ul> </li> <li>Finalize: Review the settings and click \"Create database\".</li> </ol> <p>Note: Database creation can take 10-15 minutes.</p>"},{"location":"MLOps/mlflow/#23-step-3-launch-an-ec2-virtual-server","title":"\ud83d\udda5\ufe0f 2.3. Step 3: Launch an EC2 Virtual Server","text":"<ol> <li>Navigate to EC2: Go to the EC2 service in the AWS Console.</li> <li>Launch Instance: Click \"Launch instance\" and configure:<ul> <li>Name: <code>mlflow-server</code>.</li> <li>AMI: Select Ubuntu (Free tier eligible).</li> <li>Instance type: Choose <code>t2.micro</code> (Free Tier eligible).</li> <li>Key pair (login):<ul> <li>Click \"Create new key pair\", name it <code>mlflow-key</code>, and keep the defaults.</li> <li>The <code>.pem</code> file will download. Store this file securely.</li> </ul> </li> <li>Network settings:<ul> <li>Click \"Edit\".</li> <li>Create a new security group (<code>mlflow-server-sg</code>) with these inbound rules:<ol> <li>SSH: <code>Type: SSH</code>, <code>Source: My IP</code> (for better security).</li> <li>HTTP: <code>Type: HTTP</code>, <code>Source: Anywhere</code>.</li> <li>Custom TCP: <code>Type: Custom TCP</code>, <code>Port: 5000</code>, <code>Source: Anywhere</code>.</li> </ol> </li> </ul> </li> </ul> </li> <li>Launch: Review the summary and click \"Launch instance\".</li> </ol>"},{"location":"MLOps/mlflow/#24-step-4-connecting-the-components","title":"\ud83e\udd1d 2.4. Step 4: Connecting the Components","text":""},{"location":"MLOps/mlflow/#241-connect-ec2-and-rds-security-groups","title":"2.4.1. Connect EC2 and RDS Security Groups","text":"<p>Create a firewall rule to allow the EC2 instance to communicate with the RDS database.</p> <ol> <li>Navigate to the RDS dashboard, select your <code>mlflow-db</code>, and go to the \"Connectivity &amp; security\" tab.</li> <li>Click on the active VPC security group (<code>mlflow-db-security-group</code>).</li> <li>Go to the \"Inbound rules\" tab and click \"Edit inbound rules\".</li> <li>Add a new rule:<ul> <li>Type: <code>PostgreSQL</code>.</li> <li>Source: Select your EC2 security group (<code>mlflow-server-sg</code>).</li> </ul> </li> <li>Click \"Save rules\".</li> </ol>"},{"location":"MLOps/mlflow/#242-connect-to-your-ec2-instance","title":"2.4.2. Connect to Your EC2 Instance","text":"<ol> <li>Go to the EC2 dashboard, select your <code>mlflow-server</code>, and copy the \"Public IPv4 address\".</li> <li>Open a terminal and make your key file private:</li> </ol> <pre><code>chmod 400 /path/to/your/mlflow-key.pem\n</code></pre> <ol> <li>Connect via SSH:</li> </ol> <pre><code>ssh -i /path/to/your/mlflow-key.pem ubuntu@&lt;YOUR_PUBLIC_IP_ADDRESS&gt;\n</code></pre>"},{"location":"MLOps/mlflow/#243-create-and-attach-an-iam-role-for-s3-access","title":"2.4.3. Create and Attach an IAM Role for S3 Access","text":"<p>Grant your EC2 instance permissions to access the S3 bucket.</p> <ol> <li> <p>Create an IAM Policy:</p> <ul> <li>Go to IAM &gt; Policies &gt; \"Create policy\".</li> <li>Use the visual editor:<ul> <li>Service: <code>S3</code>.</li> <li>Actions: <code>ListBucket</code>, <code>GetObject</code>, <code>PutObject</code>, <code>DeleteObject</code>.</li> <li>Resources: Specify the ARN for your bucket (<code>arn:aws:s3:::your-bucket-name</code>) and the objects within it (<code>arn:aws:s3:::your-bucket-name/*</code>).</li> </ul> </li> <li>Name the policy <code>MLflowS3AccessPolicy</code>.</li> </ul> </li> <li> <p>Create an IAM Role:</p> <ul> <li>Go to IAM &gt; Roles &gt; \"Create role\".</li> <li>Trusted entity: <code>AWS service</code>.</li> <li>Use case: <code>EC2</code>.</li> <li>Attach the <code>MLflowS3AccessPolicy</code> you just created.</li> <li>Name the role <code>MLflowEC2Role</code>.</li> </ul> </li> <li> <p>Attach the Role to EC2:</p> <ul> <li>In the EC2 dashboard, select your <code>mlflow-server</code>.</li> <li>Go to \"Actions\" &gt; \"Security\" &gt; \"Modify IAM role\".</li> <li>Select <code>MLflowEC2Role</code> and save.</li> </ul> </li> </ol>"},{"location":"MLOps/mlflow/#244-ubuntu-server-setup-best-practices","title":"2.4.4. Ubuntu Server Setup Best Practices","text":"<ol> <li>Update Your System:</li> </ol> <pre><code>sudo apt update &amp;&amp; sudo apt upgrade -y\n</code></pre> <ol> <li>Create a New User:</li> </ol> <pre><code># Replace 'your_username' with a chosen name\nsudo adduser your_username\nsudo usermod -aG sudo your_username\n</code></pre> <pre><code>&gt; Log out and log back in as the new, non-root user for daily work.\n</code></pre> <ol> <li>Set Up a Basic Firewall:</li> </ol> <pre><code>sudo ufw allow OpenSSH\nsudo ufw allow 80/tcp\nsudo ufw allow 5000/tcp\nsudo ufw enable\n</code></pre>"},{"location":"MLOps/mlflow/#25-step-5-install-mlflow-software","title":"\ud83d\udee0\ufe0f 2.5. Step 5: Install MLflow Software","text":"<ol> <li>Install Tools:</li> </ol> <pre><code>sudo apt update\nsudo apt install python3-pip python3-venv -y\n</code></pre> <ol> <li>Create a Virtual Environment and Install Packages:</li> </ol> <pre><code>python3 -m venv mlflow-env\nsource mlflow-env/bin/activate\npip install mlflow boto3 psycopg2-binary\n</code></pre>"},{"location":"MLOps/mlflow/#26-step-6-launch-the-mlflow-server","title":"\u25b6\ufe0f 2.6. Step 6: Launch the MLflow Server","text":"<p>This command connects all the components. You will need your RDS Endpoint, RDS Password, and S3 Bucket Name.</p> <p>SQLAlchemy Connection String</p> <p>The required format is <code>postgresql://&lt;user&gt;:&lt;password&gt;@&lt;host&gt;:&lt;port&gt;/&lt;database&gt;</code>.</p> <p>Execute the following, replacing all placeholders:</p> <pre><code>mlflow server \\\n    --backend-store-uri postgresql://mlflow_user:&lt;YOUR_RDS_PASSWORD&gt;@&lt;YOUR_RDS_ENDPOINT&gt;/mlflow_db \\\n    --default-artifact-root s3://&lt;your-s3-bucket-name&gt;/\n    --host 0.0.0.0 \\\n    --port 5000\n</code></pre> <p><code>--host 0.0.0.0</code> Explained: This tells the server to listen on all available network interfaces, making the UI accessible via the instance's public IP address.</p> <p>To keep the server running after you disconnect, use a <code>screen</code> session:</p> <pre><code># Start a new named session\nscreen -S mlflow\n\n# Activate environment and run the mlflow server command\nsource mlflow-env/bin/activate\nmlflow server ... # (paste the full command from above)\n\n# Detach from the session by pressing Ctrl+A, then D.\n</code></pre>"},{"location":"MLOps/mlflow/#27-step-7-local-machine-setup","title":"\ud83d\udcbb 2.7. Step 7: Local Machine Setup","text":""},{"location":"MLOps/mlflow/#271-connect-your-local-project","title":"2.7.1. Connect Your Local Project","text":"<p>Configure your local machine to log experiments to the remote server.</p> <ul> <li>Set the Tracking URI:</li> </ul> <pre><code># In your local terminal (macOS/Linux)\nexport MLFLOW_TRACKING_URI=\"http://&lt;YOUR_EC2_PUBLIC_IP&gt;:5000\"\n</code></pre> <ul> <li>Configure AWS Credentials: Grant your local machine S3 upload permissions.</li> </ul>"},{"location":"MLOps/mlflow/#272-aws-cli-configuration-guide","title":"2.7.2. AWS CLI Configuration Guide","text":"<ol> <li>Install the AWS CLI:</li> </ol> <pre><code>pip install awscli\n</code></pre> <ol> <li>Run Configure:</li> </ol> <pre><code>aws configure\n</code></pre> <ol> <li>Enter Your Credentials:<ul> <li>AWS Access Key ID: Paste your key.</li> <li>AWS Secret Access Key: Paste your secret key.</li> <li>Default region name: Enter your S3 bucket's region (e.g., <code>ap-south-1</code>).</li> <li>Default output format: Press Enter for <code>json</code>.</li> </ul> </li> </ol> <p>The CLI securely stores these credentials, and MLflow will automatically use them.</p>"},{"location":"MLOps/mlflow/#28-step-8-persistent-server-operation-with-systemd","title":"\ud83d\udd04 2.8. Step 8: Persistent Server Operation with <code>systemd</code>","text":"<p>To run the MLflow server as a background service that starts on boot, use <code>systemd</code>.</p> <ol> <li>SSH into your EC2 server.</li> <li>Create a <code>systemd</code> service file:</li> </ol> <pre><code>sudo nano /etc/systemd/system/mlflow-server.service\n</code></pre> <ol> <li>Paste the following configuration, replacing all placeholders.</li> </ol> <pre><code>[Unit]\nDescription=MLflow Tracking Server\nAfter=network.target\n\n[Service]\nUser=&lt;your_user&gt;\nRestart=on-failure\n# Note: Use the absolute path to the mlflow executable in your venv\nExecStart=/home/&lt;your_user&gt;/mlflow-env/bin/mlflow server \\\n    --backend-store-uri postgresql://mlflow_user:&lt;YOUR_RDS_PASSWORD&gt;@&lt;YOUR_RDS_ENDPOINT&gt;/mlflow_db \\\n    --default-artifact-root s3://&lt;your-s3-bucket-name&gt;/\n    --host 127.0.0.1 \\\n    --port 5000\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <pre><code>&gt; **Security Note:** `--host` is set to `127.0.0.1`, meaning the server only accepts connections from the machine itself. A reverse proxy like Nginx should be used to handle public traffic securely.\n</code></pre> <ol> <li>Enable and Start the Service:</li> </ol> <pre><code># Reload systemd to recognize the new file\nsudo systemctl daemon-reload\n\n# Enable the service to start on boot\nsudo systemctl enable mlflow-server.service\n\n# Start the service now\nsudo systemctl start mlflow-server.service\n\n# Check its status\nsudo systemctl status mlflow-server.service\n</code></pre>"},{"location":"MLOps/mlflow/#3-using-a-static-ip-with-aws-elastic-ip","title":"\ud83d\udccd 3. Using a Static IP with AWS Elastic IP","text":"<p>An EC2 instance's public IP changes on every restart, which breaks the <code>MLFLOW_TRACKING_URI</code>. An Elastic IP (EIP) provides a permanent, static IP address to solve this problem.</p>"},{"location":"MLOps/mlflow/#31-eip-intuition-a-permanent-address","title":"\ud83c\udfe0 3.1. EIP Intuition: A Permanent Address","text":"<p>An EIP is a static public IPv4 address you allocate to your AWS account. You can attach it to your EC2 instance, and it will persist across all stop/start cycles, ensuring permanent connectivity.</p>"},{"location":"MLOps/mlflow/#32-step-by-step-eip-implementation","title":"\ud83d\uddfa\ufe0f 3.2. Step-by-Step EIP Implementation","text":"<ol> <li> <p>Allocate an Elastic IP:</p> <ul> <li>Go to the EC2 Dashboard &gt; Elastic IPs.</li> <li>Click \"Allocate Elastic IP address\" and confirm by clicking \"Allocate\".</li> </ul> </li> <li> <p>Associate the EIP with Your EC2 Instance:</p> <ul> <li>On the Elastic IPs screen, select the new IP.</li> <li>Click \"Actions\" &gt; \"Associate Elastic IP address\".</li> <li>Choose Instance as the resource type and select your <code>mlflow-server</code> instance.</li> <li>Click \"Associate\".</li> </ul> </li> <li> <p>Update the <code>MLFLOW_TRACKING_URI</code>:</p> <ul> <li>Replace the old dynamic IP in your <code>prediction_app.env</code> file and any other configurations with the new Elastic IP.</li> <li>Example: <code>MLFLOW_TRACKING_URI=\"http://&lt;Your_Elastic_IP&gt;:5000\"</code></li> </ul> </li> </ol>"},{"location":"MLOps/mlflow/#33-eip-cost-and-security-considerations","title":"\u26a0\ufe0f 3.3. EIP Cost and Security Considerations","text":"Consideration Detail Best Practice/Tip Cost \ud83d\udcb0 EIPs are free only when associated with a running EC2 instance. AWS charges a small hourly fee for EIPs that are allocated but unassociated or on a stopped instance. Since your server may stop/start, you will incur a minimal charge during the stopped period. This is usually worth the operational stability. Security \ud83d\udee1\ufe0f The EIP is just an address. Your EC2 Security Group (<code>mlflow-server-sg</code>) must still allow inbound traffic on port <code>5000</code>. No change is needed if you configured the security group correctly in Step 2.3. Advanced DNS \ud83c\udff7\ufe0f For maximum flexibility, use Route 53 to create a friendly domain name (e.g., <code>mlflow.yourproject.com</code>) that points to the EIP. If you ever change the EIP, you only need to update the DNS record, not every client configuration. This decouples your clients from the specific IP address."},{"location":"MLOps/tools/","title":"MLOps Tooling","text":"<p>This project leverages a modern stack of MLOps tools to ensure reproducibility, scalability, and maintainability. Each tool plays a specific and crucial role in the project lifecycle.</p> Tool Category Role in Project GitHub Source Code Management Manages the source code repository, facilitates collaboration, and hosts the CI/CD workflows via GitHub Actions. DVC (Data Version Control) Data &amp; Model Versioning Versions large data files, models, and intermediate artifacts. It works alongside Git to ensure every experiment is fully reproducible. Conda Environment Management Creates isolated Python environments to ensure consistency across development and execution stages. uv Dependency Management A fast Python package installer and resolver, used for installing and managing dependencies defined in <code>requirements.txt</code>. DVC Pipelines Pipeline Orchestration The primary tool for orchestrating the multi-stage data pipeline (<code>dvc.yaml</code>). It automatically tracks dependencies and manages execution. Great Expectations Data Quality &amp; Validation Acts as the primary data quality gate. It defines and runs \"expectation suites\" to validate data at the Bronze, Silver, and Gold stages. MLflow Experiment Tracking &amp; Model Registry Serves as the central hub for MLOps. It tracks experiments, logs parameters and metrics, and manages the lifecycle of trained models in the Model Registry. SHAP Model Explainability Provides deep insights into model behavior by explaining the output of machine learning models, ensuring transparency and trust. FastAPI API Framework Used to build the high-performance, production-ready API for serving the champion model. Docker Containerization Packages the FastAPI prediction server and its dependencies into a portable container image for deployment. GitHub Actions CI/CD Automates the testing, validation, and deployment pipelines, ensuring code quality and enabling seamless releases. Google Artifact Registry Deployment A private Docker registry used to securely store and manage the prediction server's container images. Google Cloud Run Deployment A serverless platform used to deploy and scale the containerized FastAPI prediction server. Backblaze B2 Cloud Infrastructure Provides S3-compatible object storage that serves as the remote backend for DVC, storing all large data and model files. AWS (EC2, RDS, S3) Cloud Infrastructure A suite of AWS services used to host the remote MLflow tracking server: EC2 for the virtual server, RDS for the PostgreSQL metadata database, and S3 for the artifact store."},{"location":"Modeling/model_explainability_lgbm_champ/","title":"Explaining the LightGBM Champion Model","text":"<p>This report provides a deep dive into our champion model, LightGBM, which was chosen after a rigorous process of evaluation and refinement, as detailed in the Model Selection Report. Here, we use SHAP to understand why the model makes its decisions.</p>"},{"location":"Modeling/model_explainability_lgbm_champ/#1-executive-summary","title":"1. Executive Summary","text":"<p>After identifying and correcting an overfitting issue caused by a leaky feature, the final LightGBM model was validated as the champion. It achieves a Test Set RMSE of $7.60 and demonstrates stable, reliable, and interpretable behavior.</p> <p>This SHAP analysis confirms that the model has learned logical and robust patterns from the data. Key drivers of price predictions include <code>time</code>, <code>flight_type</code>, and now, critically, temporal features like <code>day_of_week</code> and <code>day</code>. The model's behavior is consistent, and its predictions can be trusted.</p>"},{"location":"Modeling/model_explainability_lgbm_champ/#2-global-model-explainability","title":"2. Global Model Explainability","text":""},{"location":"Modeling/model_explainability_lgbm_champ/#a-shap-summary-plot","title":"A. SHAP Summary Plot","text":"<p>The summary plot provides a global overview of the model's feature importance and the impact of each feature on the predictions.</p> <p></p> <p>Insights:</p> <ul> <li><code>time</code> remains the most influential feature. Higher values (longer flights) strongly push the price prediction higher.</li> <li><code>flight_type</code> is the second most important feature, with a clear categorical impact.</li> <li>Temporal Features Matter: Unlike previous iterations, <code>day_of_week</code> and <code>day</code> are now contributing which confirms that removing the complex feature engineering process has allowed the model to learn these more subtle patterns. This gets more clear in local shap analysis.</li> </ul>"},{"location":"Modeling/model_explainability_lgbm_champ/#b-shap-feature-importance-bar-plot","title":"B. SHAP Feature Importance (Bar Plot)","text":"<p>This plot shows the mean absolute SHAP value for each feature, quantifying its average impact on the model's output.</p> <p></p> <p>Insights:</p> <ul> <li>This plot confirms the findings from the summary plot, with <code>time</code>, <code>flight_type</code>, <code>from_location</code>, <code>to_location</code>, and  being the top features.</li> <li>The feature importance is now more distributed and logical, without a single feature dominating the model's decisions.</li> <li>Temporal Features also have some level of importance which closely aligns with the exploratory data analysis findings.</li> </ul>"},{"location":"Modeling/model_explainability_lgbm_champ/#3-the-case-of-the-reappearing-temporal-features","title":"3. The Case of the Reappearing Temporal Features","text":"<p>In our initial, overfit models, temporal features like <code>month</code>, <code>year</code>, <code>day</code> and <code>day_of_week</code> were assigned zero importance. This was a major red flag, as EDA showed clear seasonal patterns. The final, stable model corrects this.</p> <p>Why did this happen?</p> <p>The engineered features and preprocessing like scaling, OHE were causing model to focus more on some of the features that hold most of the predictive power . It was so powerful and specific that the model could essentially memorize the price for a given route, ignoring all other features. By overfitting to <code>route</code>, the model never needed to learn the more subtle (but more generalizable) patterns related to seasonality or the day of the week.</p> <p>By removing the unnecessary <code>preprocessing</code> and features including <code>route</code> feature(removed in second interation), we forced the model to look for other signals. As a result, it correctly identified the importance of the cyclical <code>day_of_week</code> and <code>day</code> features, which now play a significant role in its predictions. This is a strong indicator that our final model is more robust and has learned a more accurate representation of the real-world factors driving flight prices.</p>"},{"location":"Modeling/model_explainability_lgbm_champ/#4-local-model-explainability","title":"4. Local Model Explainability","text":""},{"location":"Modeling/model_explainability_lgbm_champ/#a-shap-force-plot","title":"A. SHAP Force Plot","text":"<p>The force plot visualizes the SHAP values for individual predictions. The interactive plot linked below allows for exploring the forces driving the price for thousands of different flights.</p> <p>View the interactive force plot</p> <ul> <li>It further shows the combinational power of the feature like lower <code>time</code> duration for flight and <code>economy</code> flight type reduced the price significantly.</li> <li>Longer duration flights costs more when combined with expensive flight type like <code>firstclass</code> and expensive <code>agency</code> but also show medium price when Longer duration is combined with <code>economy</code>.</li> </ul>"},{"location":"Modeling/model_explainability_lgbm_champ/#b-shap-waterfall-plots","title":"B. SHAP Waterfall Plots","text":"<p>These plots show how the model arrived at its final prediction for specific instances. The <code>f(x)</code> value at the top is the model's predicted output, and <code>E[f(x)]</code> at the bottom is the base value (the average prediction).</p> <p>The data for these instances can be found in the accompanying CSV file: Final_model_shap_local_instances.csv.</p>"},{"location":"Modeling/model_explainability_lgbm_champ/#instance-0","title":"Instance 0","text":"<ul> <li>Insights: The model correctly predicts a high price. The primary drivers are the long flight <code>time</code> and the <code>flight_type</code> (first class). The specific <code>day_of_week</code> also contributes positively to the price, demonstrating the model's use of temporal features.</li> <li><code>agency</code> also contributes here some agencies are more expensive then others which aligns with the findings during EDA.</li> </ul>"},{"location":"Modeling/model_explainability_lgbm_champ/#instance-1","title":"Instance 1","text":"<ul> <li>Insight: The model predicts a low price significantly lower than the average, driven down by the <code>flight_type</code> (economy) and a short <code>time</code>. The <code>from_location</code>, <code>to_location</code> including other features all contribute to lowering the price except for day_of_week which is increasing the price a little.</li> </ul>"},{"location":"Modeling/model_explainability_lgbm_champ/#instance-2","title":"Instance 2","text":"<ul> <li>Insight: Here, the model balances competing factors. A long <code>time</code> pushes the price up, but this is counteracted by the <code>flight_type</code> (economy), cheaper <code>agency</code> and <code>from_location</code>, a low-impact <code>day_of_week</code>, resulting in a prediction close to the average.</li> </ul>"},{"location":"Modeling/model_explainability_lgbm_champ/#5-feature-dependence-plots","title":"5. Feature Dependence Plots","text":"<p>Dependence plots show how a single feature's value affects its SHAP value, revealing the relationship it has learned.</p>"},{"location":"Modeling/model_explainability_lgbm_champ/#a-time","title":"A. Time","text":"<p>Insight: A clear, positive linear relationship. As flight duration increases, its impact on the price increases. The vertical coloring shows interactions with <code>flight_type</code>.</p>"},{"location":"Modeling/model_explainability_lgbm_champ/#b-day-of-week","title":"B. Day of Week","text":"<p>Insight: This plot is crucial. It shows the model has learned a distinct, non-linear pattern for the day of the week, confirming that the cyclical features are working as intended.</p>"},{"location":"Modeling/model_explainability_lgbm_champ/#c-flight-type","title":"C. Flight Type","text":"<p>Insight: Shows the clear categorical impact of <code>flight_type</code>, with each class having a distinct and separate impact on price.</p>"},{"location":"Modeling/model_explainability_lgbm_champ/#6-conclusion","title":"6. Conclusion","text":"<p>The SHAP analysis confirms that our final, stable LightGBM model is not a \"black box\". It has learned intuitive and explainable patterns from the data. Its predictions are driven by a logical hierarchy of features, and its behavior is consistent and trustworthy. This transparency is crucial for deploying the model in a production environment.</p>"},{"location":"Modeling/model_explainability_lgbm_vs_xgb/","title":"Model Comparison: LightGBM vs. XGBoost (Archived)","text":"<p>Note: This document is archived and no longer relevant to the final project outcome.</p> <p>This document was used for a preliminary comparison between LightGBM and XGBoost. However, the final model selection process, detailed in the Model Selection Report, revealed that the XGBoost model suffered from significant overfitting and was dropped from consideration.</p> <p>The key takeaway is that while both models are powerful, LightGBM proved to be more stable and reliable for this specific dataset and feature set. The final analysis confirmed that LightGBM was the superior choice, not just in performance, but in its ability to generalize without overfitting.</p> <p>Please refer to the main Model Selection Report for the complete and accurate story of how the champion model was chosen.</p>"},{"location":"Modeling/model_selection_report/","title":"Flight Price Prediction: Model Selection Report","text":""},{"location":"Modeling/model_selection_report/#1-executive-summary","title":"1. Executive Summary","text":"<p>This report details the systematic evaluation and iterative refinement of machine learning models for the flight price prediction task. The process involved establishing a baseline, tuning multiple advanced models, and conducting a final bake-off.</p> <p>Initial results pointed to a LightGBM model with suspiciously high accuracy. This prompted a deep-dive investigation that uncovered and corrected a subtle overfitting issue caused by a leaky feature. After this refinement, a final, robust LightGBM model was confirmed as the champion.</p> <p>The final model demonstrates an excellent balance of high performance and stability, with a Cross-Validation RMSE of $9.57 and a Final Test Set RMSE of $7.60. This documentation tells the complete story, from a wide-ranging bake-off to the crucial investigative work that produced a truly reliable and production-ready model.</p>"},{"location":"Modeling/model_selection_report/#2-initial-bake-off-the-too-good-to-be-true-result","title":"2. Initial Bake-Off &amp; The \"Too Good to Be True\" Result","text":"<p>The first step was to compare our tuned tree-based models against a Linear Regression baseline.</p> Model CV R\u00b2 Score CV RMSE CV RMSE Std Dev (Stability) CV MAE Duration LGBMRegressor (Tuned v1) ~1.000 $1.02 $0.38 $0.61 2.5 min RandomForestRegressor (Tuned) 0.999 $10.50 $0.11 $5.38 6.9 min XGBoostRegressor (Tuned) 0.999 $11.95 $3.32 $9.48 1.9 min LinearRegression (Base) 0.986 $42.64 $0.18 $34.32 2.2 min"},{"location":"Modeling/model_selection_report/#analysis-and-red-flags","title":"Analysis and Red Flags","text":"<p>While the initial results were impressive across the board, the LightGBM model's performance was almost perfect (CV RMSE ~$1.02). Such high accuracy on a real-world dataset is a significant red flag for potential overfitting or data leakage. Furthermore, this initial model completely ignored temporal features, which contradicted our findings from the EDA. This warranted a deeper investigation.</p>"},{"location":"Modeling/model_selection_report/#3-iteration-1-the-overfitting-trap","title":"3. Iteration 1: The Overfitting Trap","text":"<p>To simplify and optimize, a new iteration was run with two key changes:</p> <ol> <li>A new <code>is_tree_model</code> parameter was introduced to create a more efficient pipeline for tree-based models, bypassing unnecessary steps like one-hot encoding and scaling.</li> <li>Interaction features that showed zero importance in the initial SHAP analysis were removed.</li> </ol> <p>This left a feature set that included <code>route</code> (a combination of origin and destination) and newly added cyclical temporal features.</p>"},{"location":"Modeling/model_selection_report/#31-iteration-1-results","title":"3.1. Iteration 1: Results","text":"Model CV RMSE Final Model RMSE (Train+Val) Overfitting Gap LightGBM $7.25 $0.59 ~92% XGBoost $6.47 $0.72 ~89% <ul> <li>The scores for LightGBM dropped to a reasonable range but the performance on the combined model is very overfitting for both.</li> <li>However, this time there was some level of importance given to <code>temporal</code> features which didnt happen before.</li> </ul>"},{"location":"Modeling/model_selection_report/#32-diagnosis-severe-overfitting","title":"3.2. Diagnosis: Severe Overfitting","text":"<p>The results were clear: both models were severely overfitting. The error on the combined training and validation data (<code>Final Model RMSE</code>) was an order of magnitude lower than the average error during cross-validation (<code>CV RMSE</code>). This indicates the models were memorizing the training data and failing to generalize.</p> <p>SHAP analysis of this run revealed that the engineered <code>route</code> feature had an overwhelmingly dominant contribution, dwarfing even <code>time</code> and <code>flight_type</code>. This pointed to <code>route</code> as the primary source of data leakage and overfitting.</p>"},{"location":"Modeling/model_selection_report/#4-iteration-2-taming-the-model-finding-the-true-champion","title":"4. Iteration 2: Taming the Model &amp; Finding the True Champion","text":"<p>The clear next step was to remove the leaky <code>route</code> feature, forcing the models to learn from the more fundamental <code>from_location</code> and <code>to_location</code> features.</p>"},{"location":"Modeling/model_selection_report/#41-iteration-2-results","title":"4.1. Iteration 2: Results","text":"Model CV RMSE Final Model RMSE (Train+Val) Overfitting Gap Verdict LightGBM $9.57 $7.66 ~20% Stable &amp; Reliable XGBoost $12.08 $0.90 ~92% Still Overfitting"},{"location":"Modeling/model_selection_report/#42-analysis-and-final-decision","title":"4.2. Analysis and Final Decision","text":"<p>This iteration was the breakthrough:</p> <ol> <li>LightGBM is the Champion: By removing the <code>route</code> feature, the LightGBM model's performance stabilized. The CV RMSE and the final model RMSE are now closely aligned, indicating it generalizes well. The slight remaining gap is expected and healthy.</li> <li>XGBoost is Dropped: The XGBoost model, even without the <code>route</code> feature, continued to overfit severely. This made it an unreliable candidate for production.</li> </ol> <p>Conclusion: The stabilized LightGBM model from Iteration 2 was declared the definitive champion.</p>"},{"location":"Modeling/model_selection_report/#5-final-champion-model-performance-on-unseen-test-data","title":"5. Final Champion Model: Performance on Unseen Test Data","text":"<p>The final step was to evaluate the champion LightGBM model on the hold-out test set to confirm its real-world performance.</p> Metric Value R\u00b2 Score 0.99956 Root Mean Squared Error (RMSE) $7.60 Mean Absolute Error (MAE) $5.50 Median Absolute Error $4.21 Max Error $39.94 <p>The test set RMSE of $7.60 is perfectly in line with the final model's training RMSE of $7.66 and the cross-validation RMSE of $9.57. This consistency is the ultimate proof that the model is robust, reliable, and not overfit.</p>"},{"location":"Modeling/model_selection_report/#next-steps-understanding-the-champion","title":"Next Steps: Understanding the Champion","text":"<p>The metrics clearly show that our refined LightGBM is the champion model. The next stage of our analysis is to dive deep into its behavior to ensure it has learned logical and robust patterns from the data.</p> <ul> <li>Deep Dive into LightGBM's Behavior \u00bb</li> </ul>"},{"location":"Modeling/training_pipeline/","title":"Training Pipeline","text":"<p>The Training Pipeline is responsible for training, evaluating, and logging machine learning models using the processed data from the Gold layer. It is highly configurable and deeply integrated with MLflow for experiment tracking and model management.</p> <ul> <li>Source Code: <code>src/pipelines/training_pipeline.py</code></li> </ul>"},{"location":"Modeling/training_pipeline/#purpose","title":"Purpose","text":"<ul> <li>To train machine learning models on the clean, feature-rich Gold dataset.</li> <li>To systematically evaluate model performance using different strategies (simple validation or cross-validation).</li> <li>To log all relevant information to MLflow, including parameters, metrics, artifacts (like plots and predictions), and the final model itself.</li> <li>To provide a flexible framework for experimenting with different models and hyperparameters.</li> </ul>"},{"location":"Modeling/training_pipeline/#pipeline-workflow","title":"Pipeline Workflow","text":"<p>The pipeline executes a series of steps, from data loading to model registration, all tracked within a single MLflow run.</p> <pre><code>%%{init: {'theme': 'dark'}}%%\ngraph TD\n    subgraph \"Setup\"\n        A[Load Gold Data]\n        B[Load params.yaml]\n        C[Set MLflow Experiment]\n    end\n\n    subgraph \"MLflow Run\"\n        D[Start Run] --&gt; E{Log Params &amp; Tags}\n        E --&gt; F{Instantiate Model}\n        F --&gt; G{Select Training Strategy}\n        G -- Simple Train/Validate --&gt; H(Train &amp; Evaluate)\n        G -- Cross-Validation --&gt; I(Perform CV &amp; Retrain)\n        G -- Train Final Model --&gt; J(Train on All Data)\n        H --&gt; K{Evaluate on Test Set?}\n        I --&gt; K\n        J --&gt; K\n        K -- Yes --&gt; L[Log Test Metrics &amp; Plots]\n        K -- No --&gt; M[Log Model to MLflow]\n        L --&gt; M\n        M --&gt; N{Register Model?}\n        N -- Yes --&gt; O[Register in Model Registry]\n        N -- No --&gt; P[End Run]\n        O --&gt; P\n    end\n\n    A --&gt; D\n    B --&gt; D\n    C --&gt; D\n</code></pre>"},{"location":"Modeling/training_pipeline/#key-stages","title":"Key Stages","text":"<p>The pipeline is divided into five main stages, executed sequentially:</p> <ol> <li> <p>Configuration &amp; Setup:</p> <ul> <li>Loads the main <code>params.yaml</code> file.</li> <li>Selects the active model configuration based on the <code>model_config_to_run</code> key.</li> <li>Sets the MLflow experiment name, creating it if it doesn't exist.</li> </ul> </li> <li> <p>Data Preparation:</p> <ul> <li>Loads the training, validation, and (optional) test datasets from the <code>data/gold_data/processed/</code> directory.</li> <li>Separates features (X) from the target variable (y).</li> <li>Drops any specified multicollinear columns to prevent issues with certain models.</li> <li>Loads the fitted <code>Scaler</code> and <code>PowerTransformer</code> objects, which are essential for unscaling predictions to their original magnitude for evaluation.</li> </ul> </li> <li> <p>Model Training &amp; Validation:</p> <ul> <li>This is the core of the pipeline, executed within an MLflow run context.</li> <li>MLflow Setup: Autologging is enabled based on the model's library (e.g., <code>sklearn</code>, <code>xgboost</code>), and all parameters from <code>params.yaml</code> are logged.</li> <li>Model Instantiation: The model is created using the class and hyperparameters defined in the active configuration.</li> <li>Training Strategy: Based on the configuration, one of three paths is taken:<ul> <li>Simple Validation: The model is trained on the training set and evaluated on both the training and validation sets.</li> <li>Cross-Validation: Time-based cross-validation is performed on the combined training and validation data, results are logged, and a final model is retrained on the full combined dataset.</li> <li>Train Final Model: The model is trained on the combined training and validation data without any intermediate evaluation. This is typically used for producing the final production artifact.</li> </ul> </li> </ul> </li> <li> <p>Final Evaluation on Test Set:</p> <ul> <li>If <code>evaluate_on_test_set</code> is enabled in the configuration, the final trained model is evaluated against the hold-out test set.</li> <li>All metrics and plots for the test set are logged to MLflow, prefixed with <code>test/</code>.</li> </ul> </li> <li> <p>Model Logging &amp; Registration:</p> <ul> <li>If <code>log_model_artifact</code> is true, the final trained model object is logged to the MLflow run.</li> <li>If <code>register_model</code> is also true, the logged model is registered in the MLflow Model Registry with the specified model name, creating a new version.</li> </ul> </li> </ol>"},{"location":"Modeling/training_pipeline/#configuration","title":"Configuration","text":"<p>The pipeline's behavior is almost entirely controlled by <code>params.yaml</code>, providing a centralized place for all settings.</p> <ul> <li><code>training_pipeline.model_config_to_run</code>: This is the master key that determines which model configuration to use from the <code>models</code> dictionary.</li> <li><code>training_pipeline.models.&lt;YourModel&gt;</code>: Each entry in this dictionary defines a complete training run:<ul> <li><code>model_class</code>: The Python class name of the model (e.g., <code>LGBMRegressor</code>).</li> <li><code>training_params</code>: A dictionary of hyperparameters passed directly to the model.</li> <li><code>log_...</code>: A series of boolean flags (<code>log_model_artifact</code>, <code>log_plots</code>, <code>register_model</code>, etc.) that control what gets saved to MLflow.</li> <li><code>cross_validation.enabled</code>: A boolean to toggle between simple validation and cross-validation.</li> <li><code>train_model_only</code>: A boolean to enable the final training mode.</li> <li><code>evaluate_on_test_set</code>: A boolean to control the final evaluation on the hold-out test set.</li> </ul> </li> </ul> <p>Static configuration, such as the target column name and mappings for MLflow functions, is stored in <code>src/shared/config/config_training.py</code>.</p>"},{"location":"Modeling/training_pipeline/#how-to-run","title":"How to Run","text":"<p>The pipeline can be executed using the CLI shortcut defined in <code>pyproject.toml</code>.</p> <p>With Simple Validation or Cross-Validation:</p> <pre><code>run-training-pipeline &lt;train_file.parquet&gt; &lt;validation_file.parquet&gt; --test_file_name &lt;test_file.parquet&gt;\n</code></pre> <p>For Final Model Training (<code>train_model_only: true</code>): The pipeline automatically combines the train and validation sets.</p> <pre><code>run-training-pipeline &lt;train_file.parquet&gt; &lt;validation_file.parquet&gt;\n</code></pre> <p>Example:</p> <pre><code>run-training-pipeline train.parquet validation.parquet --test_file_name test.parquet\n</code></pre>"},{"location":"Modeling/tuning_pipeline/","title":"Hyperparameter Tuning Pipeline","text":"<p>The Hyperparameter Tuning Pipeline is a systematic process for finding the optimal hyperparameters for a given model. It automates the search process using various strategies and leverages MLflow to log and compare the results of different trials.</p> <ul> <li>Source Code: <code>src/pipelines/tuning_pipeline.py</code></li> <li>Configuration: <code>tuning.yaml</code></li> </ul>"},{"location":"Modeling/tuning_pipeline/#purpose","title":"Purpose","text":"<ul> <li>To automate the search for the best model hyperparameters, saving significant manual effort.</li> <li>To support multiple advanced tuning strategies, including Grid Search, Random Search, and Optuna.</li> <li>To provide a flexible configuration-driven approach where new tuning experiments can be defined entirely in YAML.</li> <li>To log every trial and the final best results to MLflow for full traceability and analysis.</li> </ul>"},{"location":"Modeling/tuning_pipeline/#pipeline-workflow","title":"Pipeline Workflow","text":"<p>The pipeline is designed to execute a full tuning experiment within a single, organized MLflow run.</p> <pre><code>%%{init: {'theme': 'dark'}}%%\ngraph TD\n    subgraph \"Setup\"\n        A[Load Gold Data]\n        B[Load tuning.yaml]\n        C[Select Active Tuning Config]\n    end\n\n    subgraph \"MLflow Run\"\n        D[Start Run] --&gt; E{Log Tuning Parameters}\n        E --&gt; F{Select Tuner Type}\n        F -- Optuna --&gt; G[Run Optuna Search]\n        F -- Scikit-learn Tuner --&gt; H[Run Grid/Random Search]\n        G --&gt; I{Find Best Hyperparameters &amp; Score}\n        H --&gt; I\n        I --&gt; J{Log Best Results to MLflow}\n        J --&gt; K{Log Best Model?}\n        K -- Yes --&gt; L[Log Model Artifact &amp; Register]\n        K -- No --&gt; M[End Run]\n        L --&gt; M\n    end\n\n    A --&gt; D\n    B --&gt; D\n    C --&gt; D\n</code></pre>"},{"location":"Modeling/tuning_pipeline/#key-stages","title":"Key Stages","text":"<ol> <li> <p>Setup &amp; Configuration:</p> <ul> <li>Loads the training data specified in the CLI command.</li> <li>Loads the <code>tuning.yaml</code> configuration file.</li> <li>Selects the active tuning experiment to run based on the <code>model_to_tune</code> key.</li> <li>Checks if the selected configuration is <code>enabled: true</code>.</li> </ul> </li> <li> <p>MLflow Run Execution:</p> <ul> <li>An MLflow run is started with the <code>run_name</code> defined in the configuration.</li> <li>All parameters from the active tuning configuration are logged to MLflow for reproducibility.</li> <li>Tags are set to identify the model class and tuner type.</li> </ul> </li> <li> <p>Tuner Selection &amp; Execution:</p> <ul> <li>The pipeline checks the <code>tuner_type</code> and executes the corresponding search strategy.</li> <li>For Optuna: It dynamically constructs a parameter-definer function from the <code>param_space</code> dictionary. This allows for defining complex search spaces (with ranges, steps, and distributions) directly in YAML.</li> <li>For Scikit-learn Tuners (<code>grid</code>, <code>random</code>, etc.): It uses the <code>param_grid</code> dictionary, where each hyperparameter is mapped to a list of discrete values to test.</li> <li>The chosen tuner runs the search using cross-validation.</li> </ul> </li> <li> <p>Logging Results:</p> <ul> <li>Once the search is complete, the pipeline logs the following to the active MLflow run:<ul> <li>The best hyperparameters found (<code>best_...</code> params).</li> <li>The best cross-validation score achieved.</li> </ul> </li> <li>If <code>log_model_artifact: true</code>, the best-performing model estimator is saved as an artifact.</li> <li>If <code>register_model: true</code>, the saved model is also registered in the MLflow Model Registry.</li> </ul> </li> </ol>"},{"location":"Modeling/tuning_pipeline/#configuration-tuningyaml","title":"Configuration (<code>tuning.yaml</code>)","text":"<p>This file is the single source of truth for all tuning experiments. To run a new experiment, you only need to add a new configuration to the <code>tuning_configs</code> dictionary and point the top-level <code>model_to_tune</code> key to it.</p> <ul> <li><code>model_to_tune</code>: The key of the configuration to run from <code>tuning_configs</code>.</li> <li><code>tuning_configs.&lt;YourTuningConfig&gt;</code>:<ul> <li><code>enabled</code>: Set to <code>true</code> to run this experiment.</li> <li><code>model_class</code>: The model to tune (e.g., <code>XGBRegressor</code>).</li> <li><code>run_name</code>: The name for the MLflow run.</li> <li><code>tuner_type</code>: The core search strategy. See supported tuners below.</li> <li><code>tuner_params</code>: Parameters for the tuner itself, like <code>n_trials</code> for Optuna or <code>cv</code> for the cross-validation folds.</li> <li><code>param_space</code> (for Optuna): A dictionary defining the search space for each hyperparameter.</li> <li><code>param_grid</code> (for sklearn tuners): A dictionary with lists of exact values to try.</li> </ul> </li> </ul>"},{"location":"Modeling/tuning_pipeline/#supported-tuners","title":"Supported Tuners","text":"<p>The pipeline supports the following <code>tuner_type</code> values:</p> <ul> <li><code>grid</code>: Exhaustive Grid Search (<code>GridSearchCV</code>).</li> <li><code>random</code>: Randomized Search (<code>RandomizedSearchCV</code>).</li> <li><code>halving_grid</code>: Halving Grid Search (<code>HalvingGridSearchCV</code>).</li> <li><code>halving_random</code>: Halving Random Search (<code>HalvingRandomSearchCV</code>).</li> <li><code>optuna</code>: Bayesian optimization using the Optuna framework.</li> </ul>"},{"location":"Modeling/tuning_pipeline/#how-to-run","title":"How to Run","text":"<p>The pipeline is run from the command line, specifying the training data to use.</p> <p>Using CLI Shortcut:</p> <pre><code>run-tuning-pipeline &lt;train_file.parquet&gt;\n</code></pre> <p>Example:</p> <pre><code>run-tuning-pipeline train.parquet\n</code></pre>"},{"location":"Testing/testing_strategy/","title":"\ud83e\uddea Testing Strategy","text":"<p>This document provides an in-depth analysis of the comprehensive testing strategy for this MLOps project. The strategy is designed to ensure reliability, correctness, and reproducibility across the entire machine learning lifecycle, from data ingestion to model deployment.</p>"},{"location":"Testing/testing_strategy/#philosophy-and-frameworks","title":"\ud83c\udfaf Philosophy and Frameworks","text":"<p>The testing philosophy is built on catching errors as early as possible. It ensures:</p> <ul> <li>Data Quality: Preventing bad data from corrupting pipelines or models.</li> <li>Code Correctness: Verifying that all components behave as expected.</li> <li>Model Performance: Confirming that models meet performance benchmarks.</li> <li>Reproducibility: Ensuring that results can be consistently replicated.</li> </ul> <p>All tests are located in the <code>tests/</code> directory and are executed using the <code>pytest</code> framework. The strategy makes extensive use of <code>pytest</code> features like fixtures for reusable setup (<code>conftest.py</code>), parametrization for efficient testing, and mocking for component isolation.</p>"},{"location":"Testing/testing_strategy/#the-testing-pyramid-in-practice","title":"\ud83d\udd2c The Testing Pyramid in Practice","text":"<p>The project's testing strategy is structured like the classic testing pyramid, with a broad base of fast unit tests, a smaller layer of integration tests, and a peak of end-to-end validation.</p> <pre><code>graph TD\n    subgraph \"Testing Pyramid\"\n        direction TB\n        E2E[\"End-to-End Validation&lt;br/&gt;(Great Expectations &amp; MLflow)\"]\n        Integration[\"Integration Tests&lt;br/&gt;(Pipelines &amp; Component Interactions)\"]\n        Unit[\"Unit Tests&lt;br/&gt;(Functions &amp; Classes)\"]\n    end\n    Unit --&gt; Integration --&gt; E2E\n</code></pre>"},{"location":"Testing/testing_strategy/#level-1-unit-tests-the-foundation","title":"\ud83e\udd47 Level 1: Unit Tests (The Foundation)","text":"<ul> <li>Purpose: To verify the correctness of the smallest units of code\u2014individual functions and classes\u2014in complete isolation.</li> <li>Location: <code>tests/test_data_ingestion/</code>, <code>tests/test_data_preprocessing/</code>, <code>tests/test_gold_data_preprocessing/</code></li> </ul>"},{"location":"Testing/testing_strategy/#key-implementations","title":"Key Implementations:","text":"<ol> <li> <p>Testing Data Loaders (<code>test_data_loader.py</code>):</p> <ul> <li>Uses <code>@pytest.mark.parametrize</code> to efficiently test that the <code>load_data</code> function can correctly handle multiple file formats (<code>csv</code>, <code>json</code>, <code>parquet</code>, etc.).</li> <li>Leverages a custom fixture (<code>create_test_file</code>) to generate temporary files on the fly for each test run.</li> </ul> </li> <li> <p>Testing Preprocessing Functions (<code>test_silver_preprocessing.py</code>):</p> <ul> <li>Each data cleaning and feature engineering function (e.g., <code>standardize_column_format</code>, <code>create_date_features</code>) is tested individually.</li> <li>A shared <code>pytest</code> fixture provides a messy, realistic DataFrame to ensure the functions are robust.</li> </ul> </li> <li> <p>Testing Custom Transformers (<code>test_gold_data_preprocessing/</code>):</p> <ul> <li>This is a cornerstone of the unit testing strategy. Each custom <code>scikit-learn</code>-compatible transformer (e.g., <code>CategoricalEncoder</code>, <code>OutlierTransformer</code>, <code>Scaler</code>) has its own dedicated test file.</li> <li>Tests are exhaustive and cover:<ul> <li>Correct initialization and configuration.</li> <li>The mathematical correctness of the <code>fit</code> and <code>transform</code> methods.</li> <li>Graceful handling of edge cases, such as unseen categories during transformation.</li> <li>State persistence via <code>save()</code> and <code>load()</code> methods.</li> <li>Error handling for incorrect usage (e.g., calling <code>transform</code> before <code>fit</code>).</li> </ul> </li> </ul> </li> </ol>"},{"location":"Testing/testing_strategy/#level-2-integration-tests","title":"\ud83e\udd48 Level 2: Integration Tests","text":"<ul> <li>Purpose: To verify the interaction and data flow between multiple components or entire sub-systems.</li> <li>Location: <code>tests/test_pipelines/</code>, <code>tests/test_data_split/</code>, <code>tests/test_prediction_server/</code></li> </ul>"},{"location":"Testing/testing_strategy/#key-implementations_1","title":"Key Implementations:","text":"<ol> <li> <p>Pipeline Orchestration Tests (<code>test_pipelines/</code>):</p> <ul> <li>These tests verify the control flow of the data pipelines without performing slow computations.</li> <li>They use extensive mocking (<code>@patch</code>) to replace heavy functions (like data loading or GE validation) with lightweight spies.</li> <li>The tests assert that the pipeline script calls the correct functions in the correct order and correctly handles success or failure from the mocked components.</li> </ul> </li> <li> <p>Data Splitting Logic (<code>test_data_split.py</code>):</p> <ul> <li>This is a critical integration test that verifies the chronological data split.</li> <li>It uses <code>tmp_path</code> and <code>monkeypatch</code> to create a fully isolated, temporary file system. This ensures the test is hermetic and does not touch the actual project data.</li> <li>Assertions confirm not only that the files are created with the correct number of rows but also that the chronological order is maintained between splits, preventing data leakage.</li> </ul> </li> <li> <p>Prediction Server Logic (<code>test_predict.py</code>):</p> <ul> <li>This tests the integration of all preprocessing steps within the context of a prediction request.</li> <li>It verifies that the <code>preprocessing_for_prediction</code> function correctly orchestrates calls to all the fitted transformers in the right sequence, ensuring the API can successfully transform raw input into a model-ready feature vector.</li> </ul> </li> </ol>"},{"location":"Testing/testing_strategy/#level-3-end-to-end-validation","title":"\ud83e\udd49 Level 3: End-to-End Validation","text":"<ul> <li>Purpose: To validate the entire workflow, from raw data to model performance, in a production-like manner.</li> <li>Implementation: These are not traditional <code>pytest</code> tests but are integrated directly into the MLOps workflow.</li> </ul>"},{"location":"Testing/testing_strategy/#key-implementations_2","title":"Key Implementations:","text":"<ol> <li> <p>Data Validation with Great Expectations:</p> <ul> <li>What it is: Live data quality gates embedded within each DVC pipeline stage (<code>bronze</code>, <code>silver</code>, <code>gold</code>).</li> <li>How it works: After data is transformed in a pipeline stage, a Great Expectations checkpoint is run against it. If the data does not meet the predefined expectations (e.g., schema adherence, value ranges, distribution checks), the data is automatically quarantined, and the pipeline fails.</li> <li>Reports: Detailed HTML reports are generated, allowing for quick debugging of data quality issues.</li> </ul> </li> <li> <p>Model Performance Validation in MLflow:</p> <ul> <li>What it is: The final validation of the trained model's performance and generalization capabilities.</li> <li>How it works: The <code>training_pipeline.py</code> script runs evaluations on the hold-out test set and performs time-based cross-validation. All performance metrics (RMSE, MAE, R\u00b2), evaluation plots, and model artifacts are logged to MLflow.</li> <li>The \"Test\": The test is the manual or automated review of the MLflow run. A successful run is one where the model's performance on the validation and test sets meets the project's business objectives.</li> </ul> </li> </ol>"},{"location":"Testing/testing_strategy/#how-to-run-tests","title":"\u25b6\ufe0f How to Run Tests","text":"<ul> <li>Run all tests:</li> </ul> <pre><code>pytest\n</code></pre> <ul> <li>Run tests for a specific directory:</li> </ul> <pre><code>pytest tests/test_gold_data_preprocessing/\n</code></pre> <ul> <li>Run a specific test file:</li> </ul> <pre><code>pytest tests/test_data_split/test_split_data.py\n</code></pre>"}]}